#!/usr/bin/python
#
# reposurgeon - a repository surgeon.
#
# By ESR, October 2010.  BSD terms apply.
#
# Requires Python 2.7.2 or newer.
#
import sys, os, cmd, tempfile, subprocess, glob, hashlib, cProfile, cPickle
import readline, time, calendar, signal, shutil, copy, shlex, collections
import email.message, email.parser, email.utils
import re, sre_constants
import xml.etree.cElementTree as ET

version="1.9"

#
# This code is intended to be hackable to support for special-purpose or
# custom operations, though it's even better if you can come up with a new
# surgical primitive general enough to ship with the stock version.  For
# either case, here's a guide to the architecture.
#
# The core classes are largely about deserializing and reserializing import
# streams.  In between these two operations the repo state lives in a
# fairly simple Python object, Repository. The main part of Repository
# is just a list of events - Commits, Blobs, Tags, Resets, and Passthroughs.
# These are straightforward representations of the command types in an
# import stream, with Passthrough as a way of losslessly conveying lines
# the parser does not recognize.
#
#  +-------------+    +---------+    +-------------+
#  | Deserialize |--->| Operate |--->| Reserialize |
#  +-------------+    +---------+    +-------------+
#
# The general theory of reposurgeon is: you deserialize, you do stuff
# to the event list that preserves correctness invariants, you
# reserialize.  The "do stuff" is mostly not in the core classes, but
# there is one major exception.  The primitive to delete a commit and
# shuffle its fileops forwards or backwards is seriously intertwined
# with the core classes and actually makes up almost 50% of Repository
# by line count.
#
# The rest of the surgical code lives outside the core classes. Most
# of it lives in the RepoSurgeon class (the command interpreter) or
# the RepositoryList class (which encapsulated name access to a list
# of repositories and also hosts surgical operations involving
# multiple repositories). A few bits, like the repository reader and
# builder, have enough logic that's independent of these
# classes to be factored out of it.
#
# In designing new commands for the interpreter, try hard to keep them
# orthogonal to the selection-set code. As often as possible, commands
# should all have a similar form with a (single) selection set argument.
#
# VCS is not a core class.  The code for manipulating actual repos is bolted
# on the the ends of the pipeline, like this:
#
#  +--------+    +-------------+    +---------+    +-----------+    +--------+
#  | Import |--->| Deserialize |--->| Operate |--->| Serialize |--->| Export |
#  +--------+    +-------------+ A  +---------+    +-----------+    +--------+
#       +-----------+            |
#       | Extractor |------------+
#       +-----------+
#
# The Import and Export boxes call methods in VCS.
#
# Extractor classes build the deserialized internal representation directly.
# Each extractor class is a set of VCS-specific methods to be used by the
# RepoStreamer driver class.
#

class VCS:
    "Class representing a version-control system."
    def __init__(self, name,
                 subdirectory,
                 exporter,
                 styleflags,
                 properties,
                 initializer,
                 lister,
                 importer,
                 checkout,
                 preserve,
                 authormap,
                 notes):
        self.name = name
        self.subdirectory = subdirectory
        self.exporter = exporter
        self.styleflags = styleflags
        self.properties = properties
        self.initializer = initializer
        self.lister = lister
        self.importer = importer
        self.checkout = checkout
        self.preserve = preserve
        self.authormap = authormap
        self.notes = notes
    def __repr__(self):
        return "         Name: " + str(self.name) + "\n" \
             + " Subdirectory: " + str(self.subdirectory) + "\n" \
             + "     Exporter: " + str(self.exporter) + "\n" \
             + " Export-Style: " + repr(self.styleflags) + "\n" \
             + "   Properties: " + repr(self.properties) + "\n" \
             + "  Initializer: " + str(self.initializer) + "\n" \
             + "       Lister: " + str(self.lister) + "\n" \
             + "     Importer: " + str(self.importer) + "\n" \
             + "     Checkout: " + str(self.checkout) + "\n" \
             + "     Preserve: " + str(self.preserve) + "\n" \
             + "    Authormap: " + str(self.authormap) + "\n" \
             + "        Notes: " + str(self.notes) + "\n"

# Most knowledge about specific version-control systems lives in the
# following class list. Exception; there's a git-specific hook in the
# repo reader; also see the extractor classes.
# The members are, respectively:
#
# * Name of its characteristic subdirectory.
# * Command to export from the VCS to the interchange format
# * Export-style flags.
#     "nl-after-commit" = inserts an extra NL after each commit
#     "nl-after-comment" = inserts an extra NL after each comment
#     "export-progress" = exporter generates its own progress messages,
#                         no need for baton prompt.
# * Flag specifying whether it handles per-commit properties on import
# * Command to initialize a new repo
# * Command to import from the interchange format
# * Command to check out working copies of the repo files.
# * Default preserve set (typically config and hook files).
# * Likely location for an importer to drop an authrmaps file
# * Command to list files under repository control.
#
# Note that some of the commands used here are plugins or extensions
# that are not part of the basic VCS. Thus these may fail when called;
# we need to be prepared to cope with that.
#
# %(tempfile)s in a command gets substituted with the name of a
# tempile that the calling code will know to read or write from as
# appropriate after the command is done.  If your exporter can simply
# dump to stdout, or your importer read from stdin, leave out the
# %(tempfile)s; reposurgeon will popen(3) the command, and it will
# actually be slightly faster (especially on large repos) because it
# won't have to wait for the tempfile I/O to complete.
#
# %(basename) is replaced with the basename of the repo directory.
#
vcstypes = [
    VCS(name="git",
        subdirectory=".git",
        exporter="git fast-export -M -C --signed-tags=verbatim --tag-of-filtered-object=drop --all",
        styleflags=set(["nl-after-commit"]),
        properties=False,
        initializer="git init",
        importer="git fast-import",
        checkout="git checkout",
        lister="git ls-files",
        preserve=set(['.git/config', '.git/hooks']),
        authormap=".git/cvs-authors",
        notes="The authormap is not required, but will be used if present."),
    # 
    VCS(name="bzr",
        subdirectory=".bzr",
        exporter="bzr fast-export --no-plain %(basename)s",
        styleflags=set(["export-progress", "nl-after-comment"]),
        properties=True,
        initializer=None,
        lister=None,
        importer="bzr fast-import -",
        checkout="bzr checkout",
        preserve=set([]),
        authormap=None,
        notes="Requires the bzr-fast-import plugin."),
    # Export is tested and works; import is flaky.
    VCS(name="hg",
        subdirectory=".hg",
        exporter="hg-fast-export.py --marks /dev/null --mapping /dev/null --heads /dev/null --status /dev/null --repo .",
        styleflags=set(["nl-after-comment",
                        "nl-after-commit",
                        "export-progress"]),
        properties=False,
        initializer="hg init",
        lister="hg locate",
        importer="hg fastimport %(tempfile)s",
        checkout="hg checkout",
        preserve=set([".hg/hgrc"]),
        authormap=None,
        notes="The hg export-import methods are not part of stock Mercurial."),
    # If we ever implement an exporter, don't forget to preserve hooks/.  
    VCS(name="svn",
        subdirectory="locks",
        exporter="svnadmin dump .",
        styleflags=set(["export-progress"]),
        properties=False,
        initializer=None,
        importer=None,
        checkout=None,
        lister=None,
        preserve=set([]),
        authormap=None,
        notes="Read-only. Run from the repository, not a checkout directory."),
    ]

# How to write extractor classes:
#
# Clone one of the existing ones and mutate.  
#
# Significant fact: None of the get_* methods for extracting information about
# a revision is called until after checkout has been called on that revision.
#
# Most methods take a native revision ID as argument. The value and type of the
# ID don't matter to any of the code that will call the extractor, except that
# IDs must be hashable so they can be dictionary keys.
#
# The 'name'. 'subdirectory', and 'visible' members must be set. The
# subdirectory member is how an extractor recognizes what repositories
# it can consume.  If the visible member is false, the 'read' command
# will ignore the existence of the extractor.
#
# The strings returned by get_committer() and get_authors() should look like
#
# J. Random User <random@foobar> 2011-11-29T10:13:32Z
#
# that is, a free text name followed by an email ID followed by a date.
# The date specification can be anything Attribution() can parse; in
# particular, RFC3339/ISO8601 dates are good, so are RFC822 (email) dates,
# and so is git's native integer-Unix-timetamp/timezone pairs.

class GitExtractor:
    "Repository extractor for the git version-control system."
    # Regardless of what revision and branch was current at start,
    # after the git extractor runs the head revision on the master branch
    # will be checked out.
    #
    # The git extractor does not attempt to recover N ops,
    # symbolic links, gitlinks, or directory fileops.
    #
    # To be streamed, a git repo must have <emphasis>local</emphasis>
    # refs to all branches - in particular, local tracking branches
    # corresponding to all remotes.
    #
    # Some of these limitations could be fixed, but the git extractor
    # is not intended to replace git-fast-export; it only exists as a
    # test for the generic RepoStreamer code and a model for future
    # extractors.
    def __init__(self):
        # These must be set for every extractor class
        self.name = "git-extractor"
        self.subdirectory = ".git"
        self.visible = False
        # These are internal
        self.revlist = []
        self.parents = {}
        self.header = {}
        self.meta = {}
        self.tags = []
        self.refs = {}
        self.baton = None
    def analyze(self, baton):
        "Analyze a git repository for streaming."
        self.baton = baton
        # Get the topologically-ordered list of revisions and parent hashes
        with popen_or_die("git log --all --topo-order --reverse --format='%H %P'") as fp:
            for line in fp:
                fields = line.strip().split()
                self.revlist.append(fields[0])
                self.parents[fields[0]] = fields[1:]
        self.baton.twirl()
        # Next, all other per-commit data except branch IDs
        with popen_or_die("git log --all --reverse --date=raw --format='%H|%cn <%ce> %cd|%an <%ae> %ad'") as fp:
            for line in fp:
                (h, ci, ai) = line.strip().split('|')
                self.meta[h] = {'ci':ci, 'ai':ai}
        # Next, find all refs
        for root, dirs, files in os.walk(".git/refs"):
            for leaf in files:
                assert dirs is not None  # Pacify pylint
                ref = os.path.join(root, leaf)
                with file(ref) as fp:
                    self.refs[ref[5:]] = fp.read().strip()
        self.baton.twirl()
        # Next, grab all tag objects.
        with popen_or_die("git tag -l") as fp:
            for line in fp:
                tag = line.strip()
                with popen_or_die("git rev-parse %s" % tag) as fp:
                    taghash = fp.read().strip()
                # Annotated tags are first-class objects with their
                # own hashes.  The hash of a lightweight tag is just
                # the commit it points to. Handle both cases.
                objecthash = taghash
                with popen_or_die("git cat-file -p %s" % tag) as fp:
                    comment = None
                    tagger = None
                    for line in fp:
                        line = line.strip()
                        if line.startswith("tagger "):
                            tagger = line[len("tagger "):]
                        elif line.startswith("object"):
                            objecthash = line.split()[1]
                        elif comment is None and not line:
                            comment = ""
                        elif type(comment) == type(""):
                            comment += line + "\n"
                            if objecthash != taghash:
                                # committish isn't a mark; we'll fix that later
                                self.tags.append(Tag(name=tag,
                                                     tagger=Attribution(tagger),
                                                     comment=comment,
                                                     committish=objecthash))
                    self.refs["refs/tags/" + tag] = objecthash
        self.baton.twirl()
        # Color branches in the order the tips occur.  Emulate the
        # git-export order.
        refnames = self.refs.keys()
        refnames.sort(key=lambda name: self.revlist.index(self.refs[name]))
        for ref in refnames:
            self.__branch_color(self.refs[ref], ref)
        uncolored = [revision for revision in self.revlist if 'branch' not in self.meta[revision]]
        if uncolored:
            if verbose >= 1:
                raise Fatal("missing branch attribute for: %s" % uncolored)
            else:
                raise Fatal("some branches do not have local ref names.")
        self.baton.twirl()
    def __metadata(self, rev, fmt):
        with popen_or_die("git log -1 --format='%s' %s" % (fmt, rev)) as fp:
            return fp.read()[:-1]
    def __branch_color(self, rev, color):
        if rev.startswith("ref"):
            return
        while not 'branch' in self.meta[rev]:
            self.meta[rev]['branch'] = color
            parents = self.get_parents(rev)
            if not parents:
                break
            elif len(parents) == 1:
                # This case avoids blowing Python's stack by recursing
                # too deep on large repos.
                rev = parents[0]
            else:
                for parent in parents:
                    self.__branch_color(parent, color)
                break
    def pre_extract(self, repo):
        "Hook for any setup actions required before streaming."
        assert repo is not None  # Pacify pylint
    def post_extract(self, repo):
        for event in repo.commits():
            event.properties = OrderedMap()
        os.system("git checkout --quiet master")
    def isclean(self):
        "Return True if repo has no unsaved changes."
        return not capture("git ls-files --modified")
    def get_revlist(self):
        "Return a list of commit ID strings in commit timestamp order."
        return self.revlist
    def get_taglist(self):
        "Return a list of tag name strings."
        return self.tags
    def get_resetlist(self):
        "Return a dictionary of reset names with revisions as values."
        return [item for item in self.refs.items() if "/tags/" not in item[0]]
    def checkout(self, rev, filemap):
        "Check the directory out to a specified revision."
        assert filemap is not None # pacify pylint
        os.system("git checkout --quiet %s" % rev)
        manifest = capture("git ls-files").split()
        return manifest
    def cleanup(self, rev, issued):
        "Cleanup after checkout."
        assert rev and (issued is not None) # Pacify pylint
    def get_parents(self, rev):
        "Return the list of commit IDs of a commit's parents."
        return self.parents[rev]
    def get_branch(self, rev):
        return self.meta[rev]['branch']
    def get_comment(self, rev):
        "Return a commit's change comment as a string."
        return self.__metadata(rev, "%B")
    def get_committer(self, rev):
        "Return the committer's ID/date as a string."
        return self.meta[rev]['ci']
    def get_authors(self, rev):
        "Return the author's name and email address as a string."
        return [self.meta[rev]['ai']]
    def get_properties(self, rev):
        "Return a list of properties for the commit."
        assert rev is not None # Pacify pylint
        return collections.OrderedDict()

# WARNING: The following extractor is not used and has bitrotted!
#
# It is included only as a coding example for people who want to write
# extractors for other version-control systems. It was replaced by the
# dumpfile stream parser because the Subversion client tools are hideously
# slow.

class SvnExtractor():
    "Repository extractor for the Subversion version-control system."
    # Native Subversion properties that we don't suppress:
    # svn:externals, svn:special, svn:mergeinfo.
    # The reason for these suppressions is to avoid a huge volume
    # of junk properties.  We want to let through other properties
    # that might carry useful information.
    IgnoreProperties = (
        "svn:executable",
        "svn:mime-type",
        "svn:keywords",
        "svn:eol-style",
        "svn:needs-lock",
        )
    def __init__(self):
        # These must be set for every extractor class
        self.name = "svn"
        self.subdirectory = ".svn"
        self.visible = True
        # These are internal
        self.commits = None
        self.revlist = []
        self.branchtips = {}
        self.topdir = None
        self.svnroot = None
        self.trunk = ""
        self.tags = []
        self.manifest = []
        self.baton = None
    def analyze(self, baton):
        "Analyze a Subversion repository for streaming."
        self.baton = baton
        class SvnCommit:
            "Represent a Subversion commit."
            def __init__(self, **cargs):
                self.__dict__ = cargs
                self.branch = ""
                self.parents = []
                self.ignores = None
                self.properties = []
        self.topdir = os.getcwd()
        # Havoc can ensue if the checkout directory is not up to date
        # Warning! Can result in unexpectedly huge downloads
        # when called on a partial checkout!
        baton.twirl("update")
        with CriticalRegion():
            toprev = capture("svn up --quiet")
        self.baton.twirl('+')
        # Fetch repo metadata
        baton.twirl("info")
        with popen_or_die("svn info --xml") as fp:
            try:
                info = ET.parse(fp).getroot()
            except:
                raise Fatal("error while parsing Subversion info.")
        toprev = info.find("entry/commit").get("revision")
        self.svnroot = info.find("entry/repository/root").text
        # Prepare to read the log in chunks so that the log analysis
        # phase doesn't just silently grind forever on large repositories.
        if toprev is None or self.svnroot is None:
            raise Fatal("couldn't retrieve top revision number or repo root.")
        else:
            try:
                countfmt = " %%%dd of %s" % (len(toprev), toprev)
                toprev = int(toprev)
            except ValueError:
                raise Fatal("couldn't parse top revision, check svninfo.")
        if verbose >= DEBUG_EXTRACT:
            announce("root is %s, tip rev is %s" % (self.svnroot, toprev))
        CHUNK = 100
        intervals = [[str(i + 1), str((i + CHUNK))]
                     for i in range(0, toprev, CHUNK)]
        intervals[-1][1] = "HEAD"
        self.baton.twirl('+')
        # Collect and parse all log information
        # This code includes no assumptions about branch structure.
        baton.twirl("log")
        self.commits = {}
        for (lo, hi) in intervals:
            with popen_or_die("svn log --xml -v -r%s:%s" % (lo, hi)) as fp:
                try:
                    log = ET.parse(fp).getroot()
                except:
                    raise Fatal("error while parsing Subversion log.")
                for logentry in log.findall("logentry"):
                    parsed = {}
                    parsed["pathops"] = {}
                    rev = parsed["revision"] = logentry.get("revision").encode("ascii")
                    if rev not in self.commits:
                        for attr in ("author", "date", "msg"):
                            sub = logentry.find(attr)
                            if sub is None:
                                sub = ""  # Author can be None for generated commits
                            else:
                                sub = sub.text
                            parsed[attr] = sub
                            paths = logentry.find("paths")
                            if paths is not None:
                                for path in list(paths):
                                    filepath = path.text
                                    action = path.get("action")
                                    parsed['pathops'][filepath[1:]] = action
                        if not parsed["author"]:
                            parsed["author"] = "no-author"
                        if verbose >= DEBUG_EXTRACT:
                            announce("r%s: registering %s" \
				     % (rev, parsed))
                        self.commits[rev] = SvnCommit(**parsed)
                    baton.twirl()
            baton.twirl()
        baton.twirl('+')
        # Compute the set of keys (revision numbers).
        self.revlist = self.commits.keys()
        self.revlist.sort(key=int)
        # Because Subversion doesn't expose properties in XML, we have
        # to make a special pass through to get them.  A full
        # recursive check on all revisions is hideously expensive,
        # so on all commits but the last we try to catch only
        # directory properties.
        baton.twirl("properties")
        for rev in self.revlist:
            commit = self.commits[rev]
            if rev == self.revlist[-1]:
                depth = "infinity"
            else:
                depth = "immediates"
            commit.ignores = None
            with popen_or_die("svn proplist --xml -v --depth=%s -r%s %s" \
                              % (depth, rev, commit.branch)) as fp:
                try:
                    dom = ET.parse(fp)
                except:
                    raise Fatal("error while parsing Subversion properties.")
                for target in dom.findall("target"):
                    path = target.get("path")[len(self.svnroot)+1:]
                    for prop in target.findall("property"):
                        name = prop.get("name")
                        value = prop.text
                        if name == "svn:ignore":
                            commit.ignores = value
                        elif name not in SvnExtractor.IgnoreProperties:
                            commit.properties.append((path + "#"+ name, value))
                self.baton.twirl()
        self.baton.twirl("+")
        if verbose >= DEBUG_EXTRACT:
            for rev in self.revlist:
                commit = self.commits[rev]
                if commit.ignores:
                    decoration = '+'
                else:
                    decoration = '-'
                msg = commit.msg
                if msg == None:
                    msg = ""
                announce("%s %s %2d '%s'" % \
                         (commit.revision, decoration,
                          len(commit.pathops), msg.strip()))
        self.baton.startcounter(countfmt)
        self.baton.twirl("revisions:")
    def pre_extract(self, repo):
        "Hook for any setup actions required before streaming."
        assert repo is not None # pacify pylint
    def post_extract(self, repo):
        assert repo is not None # pacify pylint
        self.baton.endcounter()        
        os.chdir(self.topdir)
        os.system("svn up --quiet")
        # The copy commits used to create initial directories, tags,
        # and branches show up as commits with no fileops.  We
        # turn them into tags to preserve the metadata while making
        # easy for the user to delete them if required.
        if verbose >= DEBUG_EXTRACT:
            announce("at post-extraction time:")
            for commit in repo.commits():
                msg = commit.comment
                if msg == None:
                    msg = ""
                announce("%4s %2d %2d '%s'" % \
                         (commit.mark,
                          len(commit.fileops),
                          len(commit.properties),
                          msg.strip()))
        initial = repo.commits()[0]
        # There will always be one property, the fossil revision
        if not initial.fileops and len(initial.properties) <= 1:
            assert repo.events[0].__class__ == Reset
            if len(repo.commits()) >= 2:
                repo.events.pop(0)
                repo.tagify(initial, "root", repo.commits()[1].mark)
            else:
                complain("could not tagify root commit.")
        if self.trunk == "trunk":
            for commit in repo.commits():
                # There will always be one property, the fossil revision
                if not commit.fileops and len(commit.properties) <= 1:
                    if commit.parent_marks:
                        repo.tagify(commit,
                                    os.path.basename(commit.branch),
                                    commit.parent_marks[0],
                                    "\n[[Tag from directory creation or copy commit at Subversion r%s]]\n" % commit.get_fossil())
                    else:
                        complain("revision %s, mark %s: could not tagify parentless commit." % (commit.get_fossil(), commit.mark))
        repo.renumber()
    def isclean(self):
        "Return True if repo has no unsaved changes."
        return len(capture("svn status")) == 0
    def get_revlist(self):
        "Return a list of commit ID strings in commit timestamp order."
        return self.revlist
    def get_taglist(self):
        "Return a list of tag objects."
        return []
    def get_resetlist(self):
        "Return a dictionary of reset names with revisions as values."
        return []
    def checkout(self, rev, filemap):
        "Check the directory out to a specified revision, return its file list."
        os.chdir(self.topdir)
        # All the tricky stuff gets done here.  What we need to do is
        # compute the branch name and parent revisions for this
        # commit.  This requires updating self.branchtips, which is a
        # dictionary mapping branch names to branch tip revisions.
        #
        # There are two kinds of repos: branchy and flat. The self.trunk
        # member is "trunk" for a branch repo and "" for a flat one.
        #
        # A branchy repo has a "trunk" top-level directory, and its
        # branches live in 'branches' at the top level, and its
        # branches live in 'tags' at the top level (but neither of
        # these latter two directories are required to exist). Any
        # other directory at the top level is presumed to be an
        # exiguous branch (it's not uncommon, for example, for
        # there to be a web content directory as an exiguous branch).
        # A branchy directory with no exiguous branches is the
        # recommended standard layout.
        #
        # A flat repo has no branches and no tags; it's just one big
        # tree with a linear history.
        #
        # Is it flat or branchy?  Initially we assume flat. This code
        # relies on the assumption that trunk gets created in a
        # revision earlier than any in which we have to generate any
        # fileops that need a nonempty branch.
        #
        commit = self.commits[rev]
        if verbose >= DEBUG_EXTRACT:
            announce("r%s: pathops '%s'" % (rev, commit.pathops))
        if 'trunk' in commit.pathops:
            self.trunk = "trunk"
        if verbose >= DEBUG_COMMANDS:
            announce("checking out %s" % rev)
        capture("svn up -r%s" % rev)
        # The common prefix of the names could be the branch.
        # But it could also be some directory beneath the branch.
        # It could even be a truncated filename if all the files
        # on the directory happen to have a common name prefix.
        commit.branch = ""
        if self.trunk == "trunk":
            common = os.path.commonprefix(commit.pathops.keys())
            if os.sep in common:
                while common.endswith(os.sep):
                    common = common[:-1]
            if common != "":
                common = common.split(os.sep)
                if common[0] in ("tags", "branches"):
                    commit.branch = os.path.join(*common[0:2])
                elif os.path.isdir(common[0]):
                    commit.branch = common[0]
        self.baton.twirl()
        if verbose >= DEBUG_EXTRACT:
            announce("r%s: branch is '%s'" % (rev, commit.branch))
        # Now that we have the branch, we can get the directory contents
        # and the filelist.  This had to be done first because the branch
        # arguments to svnup and svn ls actually matter,
        branchdir = os.path.join(self.topdir, commit.branch)
        self.manifest = []
        if os.path.exists(branchdir):
            self.manifest += capture("svn ls -R %s" % branchdir).split()
            os.chdir(branchdir)
        self.baton.twirl()
        ignores = self.commits[rev].ignores
        if ignores:
            if verbose >= DEBUG_EXTRACT:
                announce("generating .gitignore")
            if not os.path.exists(branchdir):
                os.mkdir(branchdir)
                os.chdir(branchdir)
            with open(".gitignore", "w") as ofp:
                ofp.write(ignores)
            self.manifest.append(".gitignore")
        if verbose >= DEBUG_EXTRACT:
            announce("r%s: files %s" % (rev, self.manifest))
        # Now compute the parent pointer. Someday, we may use mergeprops here.
        if commit.branch in self.branchtips:
            # The easy case is that we've already seen a commit on this branch.
            commit.parents = [self.branchtips[commit.branch]]
            #if verbose >= DEBUG_EXTRACT:
            #    announce("r%s: parent from branch tip" % (rev,))
        else:
            #if verbose >= DEBUG_EXTRACT:
            #    announce("r%s: looking backward" % (rev,))
            # This is the tricky bit.  We see a commit on a new branch.
            # We use the fact that new branches have to be created by
            # copy operations.  The first commit on a branch is always
            # a no-fileop commit representing the copy, which means that
            # somewhere before it there's a commit with an *identical
            # file map*. We have to find that one.
            lookfor = signature_map(self.manifest)
            for earlier in self.revlist:
                if earlier == rev:
                    break
                # THIS NO LONGER WORKS!
                # We should do something like an MD5 content hash check here.
                elif filemap.revision(earlier) == lookfor:
                    commit.parents = [earlier]
                    break
        self.baton.progress()
        if verbose >= DEBUG_EXTRACT:
            announce("r%s: parents %s" % (rev, commit.parents,))
        # Finally, this commit becomes its branch tip, and we're done.
        self.branchtips[commit.branch] = rev
        return self.manifest
    def cleanup(self, rev, issued):
        "Cleanup after checkout."
        assert rev and (issued is not None) # Pacify pylint
        ignores = self.commits[rev].ignores
        if ignores:
            if verbose >= DEBUG_EXTRACT:
                announce("generating .gitignore")
            os.remove(".gitignore")
            if self.manifest == [".gitignore"]:
                os.rmdir(os.path.join(self.topdir, self.commits[rev].branch))
    def get_parents(self, rev):
        "Return the list of commit IDs of a commit's parents."
        return self.commits[rev].parents
    def get_branch(self, rev):
        "Return the branch to which this commit belongs."
        branch = self.commits[rev].branch
        if not branch:
            tip = "root"
        elif branch.startswith("tags" + os.sep):
            return branch
        elif branch == self.trunk:
            tip = "master"
        else:
            tip = os.path.basename(branch)
        return os.path.join("refs", "heads", tip)
    def get_comment(self, rev):
        "Return a commit or tag's change comment as a string."
        return self.commits[rev].msg
    def get_committer(self, rev):
        "Return the committer's ID/date as a string."
        return "%s <%s>" % (self.commits[rev].author,self.commits[rev].author) \
               + " " \
               + self.commits[rev].date
    def get_authors(self, rev):
        "Return a list of ID/date strings for authors."
        assert rev is not None # Pacify pylint
        return []
    def get_properties(self, rev):
        "Return a list of properties for the commit."
        return self.commits[rev].properties

# More extractors go here

extractors = [GitExtractor()]

verbose         = 0
DEBUG_EXTRACT   = 2    # Debug repo-extractor logic
DEBUG_SHUFFLE   = 3    # Debug file and directory handling
DEBUG_DELETE    = 4    # Debug canonicalization after deletes
DEBUG_COMMANDS  = 4    # Show commands as they are executed
DEBUG_EMAILIN   = 4    # Debug event round-tripping through mailbox_{out|in} 
DEBUG_MERGE     = 4    # Debug mark assignments in merging
DEBUG_LEXER     = 5    # Debug selection-language parsing

global_options = {}

def screenwidth():
    "Return the current width of the terminal window."
    with popen_or_die('stty size', 'r') as tp:
        return int(tp.read().split()[1])

def nuke(directory, legend):
    "Remove a (large) directory, with a progress indicator."
    with Baton(legend, enable=verbose>=DEBUG_SHUFFLE) as baton:
        for root, dirs, files in os.walk(directory, topdown=False):
            for name in files:
                os.remove(os.path.join(root, name))
                baton.twirl()
            for name in dirs:
                os.rmdir(os.path.join(root, name))
                baton.twirl()
    try:
        os.rmdir(directory)
    except OSError:
        pass

def rfc3339(t):
    "RFC3339 string from Unix time."
    return time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime(t))

def complain(msg):
    sys.stdout.flush()
    sys.stderr.write("reposurgeon: %s\n" % msg)

def announce(msg):
    sys.stdout.write("reposurgeon: %s\n" % msg)

class Baton:
    "Ship progress indications to stdout."
    def __init__(self, prompt, endmsg='done', enable=False):
        self.prompt = prompt
        self.endmsg = endmsg
        self.countfmt = None
        self.counter = 0
        if enable:
            self.stream = sys.stdout
        else:
            self.stream = None
        self.count = 0
        self.time = 0
    def __enter__(self):
        if self.stream:
            self.stream.write(self.prompt + "...")
            if os.isatty(self.stream.fileno()):
                self.stream.write(" \010")
            self.stream.flush()
        self.count = 0
        self.time = time.time()
        return self
    def startcounter(self, countfmt):
        self.countfmt = countfmt
        self.counter = 1
    def bumpcounter(self):
        if self.stream is None:
            return
        if os.isatty(self.stream.fileno()):
            if self.countfmt:
                update = self.countfmt % self.counter
                self.stream.write(update + ("\010" * len(update)))
                self.stream.flush()
            else:
                self.twirl()
        self.counter = self.counter + 1
    def endcounter(self):
        if self.stream:
            w = len(self.countfmt % self.count)
            self.stream.write((" " * w) + ("\010" * w))
            self.stream.flush()
        self.countfmt = None
    def twirl(self, ch=None):
        "One twirl of the baton."
        if self.stream is None:
            return
        if os.isatty(self.stream.fileno()):
            if ch:
                self.stream.write(ch)
                self.stream.flush()
                return
            else:
                update = "-/|\\"[self.count % 4]
                self.stream.write(update + ("\010" * len(update)))
                self.stream.flush()
        self.count = self.count + 1
    def __exit__(self, extype, value_unused, traceback_unused):
        if extype == KeyboardInterrupt:
            self.endmsg = "interrupted"
        if extype == Fatal:
            self.endmsg = "aborted by error"
        if self.stream:
            self.stream.write("...(%2.2f sec) %s.\n" \
                              % (time.time() - self.time, self.endmsg))
        return False

class OrderedMap(collections.OrderedDict):
    "Override the base class's odd and slightly broken __str__ method."
    def __repr__(self):
        if not self:
            return '{}'
        else:
            return "{" + collections.OrderedDict.__repr__(self)[12:-2] + "}"
    __str__ = __repr__

class RepoSurgeonEmail(email.message.Message):
    "Specialized email message with a distinguishing starter."
    def __init__(self, **kwargs):
        email.message.Message.__init__(self, **kwargs)        
        self.set_unixfrom(78 * "-")
    @staticmethod
    def readmsg(fp):
        msg = ''
        firstline = fp.readline()
        if not firstline:
            return None
        elif not firstline.startswith(78 * "-"):
            msg = firstline
        while True:
            line = fp.readline()
            if not line:
                break
            if line.startswith(78 * "-"):
                break
            msg += line
        return msg
    def __str__(self):
        return email.message.Message.__str__(self).replace("\n--", "\n.--")

class Date:
    "A time/date in local time. Preserves TZ information but doesn't use it."
    def __init__(self, text):
        "Recognize date formats that exporters or email programs might emit."
        # First, look for git's preferred format.
        text = text.strip() 
        if re.match(r"[0-9]+\s*[+-][0-9]+$", text):
            (self.timestamp, self.timezone) = text.split()
            self.timestamp = int(self.timestamp)
            return
        # If that didn't work, look for an RFC822 date, which git also
        # accepts. Note, there could be edge cases that Python's parser
        # handles but git doesn't.
        try:
            self.timestamp = int(time.mktime(email.utils.parsedate(text)))
            self.timezone = text.split()[5]
            return
        except TypeError:
            # time.mktime throws this when it gets None:
            # TypeError: argument must be 9-item sequence, not None
            pass
        # Also accept IS8601 dates in Zulu time, just because I like them.
        try:
            # Discard subsecond precision, import-stream format can't use it.
            text = re.sub(r"\.[0-9]+Z", "Z", text)
            iso860date = time.strptime(text, "%Y-%m-%dT%H:%M:%SZ")
            self.timestamp = calendar.timegm(iso860date)
            self.timezone = "+0000"
            return
        except ValueError:
            # time.strptime() throws this
            # "time data 'xxxxxx' does not match format '%Y-%m-%dT%H:%M:%S'" 
            pass
        # Date format not recognized
        raise Fatal("'%s' is not a valid timestamp" % text)
    def iso8601(self):
        "Format as an ISO8601 timestamp."
        return time.strftime("%Y-%m-%dT%H:%M:%S", time.localtime(self.timestamp)) + self.timezone
    def rfc3339(self):
        return rfc3339(self.timestamp)
    def rfc822(self):
        "Format as an RFC822 timestamp."
        return time.strftime("%a %d %b %Y %H:%M:%S", time.localtime(self.timestamp)) + " " + self.timezone
    def delta(self, other):
        return other.timestamp - self.timestamp
    def __str__(self):
        "Format as a git timestamp."
        return str(self.timestamp) + " " + self.timezone
    def __cmp__(self, other):
        return cmp(self.timestamp, other.timestamp)

class Attribution:
    "Represents an attribution of a repo action to a person and time."
    def __init__(self, person=None):
        self.name = self.email = self.date = None
        if person:
            # Deal with a cvs2svn artifact
            person = person.replace("(no author)", "no-author")
            # First, validity-check the email address
            (self.name, self.email) = email.utils.parseaddr(person)
            if not self.email:
                raise Fatal("can't recognize address in attribution %s" % person)
            # Attribution format is actually stricter than RFC822;
            # needs to have a following date in the right place.
            person = person.replace(" <", "|").replace("> ", "|")
            try:
                self.date = Date(person.strip().split("|")[2])
            except (ValueError, IndexError):
                raise Fatal("malformed attribution %s" % person)
    def email_out(self, msg, hdr):
        "Update an RC822 message object with a representation of this."
        msg[hdr] = self.name + " <" + self.email + ">"
        msg[hdr + "-Date"] = self.date.rfc822()
    def remap(self, authors):
        "Remap the attribution name."
        for (local, (name, mail)) in authors.items():
            if self.email.startswith(local + "@") or self.email == local:
                self.name = name
                self.email = mail
                break
    def action_stamp(self):
        return self.date.rfc3339() + "!" + self.email
    def __eq__(self, other):
        "Compare attributions after canonicalization."
        return (self.name == other.name
                and self.email == other.email
                and self.date == other.date)
    def who(self):
        return self.name + " <" + self.email + ">"
    def __str__(self):
        return self.name + " <" + self.email + "> " + str(self.date)

class Blob:
    "Represent a detached blob of data referenced by a mark."
    def __init__(self, repo=None):
        self.repo = repo
        self.mark = None
        self.path = None      # First in-repo path associated with this blob
        self.colors = []
        self.cookie = None
    def blobfile(self):
        "File where the content lives."
        return self.repo.subdir() + "/blob-" + repr(id(self))
    def moveto(self, repo):
        "Change the repo this blob is associated with."
        oldloc = self.blobfile()
        self.repo = repo
        newloc = self.blobfile()
        if verbose >= DEBUG_SHUFFLE:
            announce("blob rename calls os.rename(%s, %s)" % (oldloc, newloc))
        os.rename(oldloc, newloc)
        return self
    def clone(self, repo):
        "Clone a copy of this blob, pointing at the same file."
        c = copy.copy(self)
        if verbose >= DEBUG_SHUFFLE:
            announce("blob clone for %s (%s) calls os.link()" % (self.mark, self.path))
        os.link(self.blobfile(), c.blobfile())
        c.moveto(repo)
        return c
    def __str__(self):
        if not os.path.exists(self.blobfile()):
            return ''
        else:
            with open(self.blobfile()) as dp:
                content = dp.read()
            return "blob\nmark %s\ndata %d\n%s\n" % (self.mark, len(content), content)

class Tag:
    "Represents an annotated tag."
    def __init__(self, name, committish, tagger, comment):
        self.name = name
        self.committish = committish
        self.tagger = tagger
        self.comment = comment
        self.color = None
    def email_out(self, eventnum):
        "Enable do_mailbox_out() to report these."
        msg = RepoSurgeonEmail()
        msg["Event-Number"] = str(eventnum+1)
        msg["Tag-Name"] = self.name
        if self.tagger:
            self.tagger.email_out(msg, "Tagger")
        msg.set_payload(self.comment)
        return str(msg)
    def email_in(self, msg):
        "Update this Tag from a parsed email message."
        if "Tag-Name" not in msg or "Tagger" not in msg:
            raise Fatal("update to tag %s is malformed" % self.name)
        modified = False
        newname = msg["Tag-Name"]
        if self.name != newname:
            if verbose >= DEBUG_EMAILIN:
                announce("in tag %d, Tag-Name is modified %s -> %s" \
                      % (int(msg["Event-Number"]), repr(self.name), repr(newname)))
            self.name = newname
            modified = True
        if "Tagger" in msg:
            (newname, newemail) = email.utils.parseaddr(msg["Tagger"])
            if not newname or not newemail:
                raise Fatal("can't recognize address in Tagger: %s" % msg['Tagger'])
            else:
                if self.tagger.name != newname or self.tagger.email != newemail:
                    (self.tagger.name, self.tagger.email) = (newname, newemail)
                    if verbose >= DEBUG_EMAILIN:
                        announce("in tag %d, Tagger is modified" \
                              % (int(msg["Event-Number"])))
                    modified = True
            date = Date(msg["Tagger-Date"])
            if date != self.tagger.date:
                if verbose >= DEBUG_EMAILIN:
                    announce("in event %d, Tagger-Date is modified '%s' -> '%s' (delta %d)" \
                          % (int(msg["Event-Number"]),
                             self.tagger.date, date,
                             self.tagger.date.delta(date)))
                self.tagger.date = date
                modified = True
        newcomment = msg.get_payload().strip() + "\n"
        if newcomment != self.comment:
            if verbose >= DEBUG_EMAILIN:
                announce("in tag %d, comment is modified '%s' -> '%s'" \
                      % (int(msg["Event-Number"]), repr(self.comment), repr(newcomment)))
            modified = True
            self.comment = newcomment
        return modified
    def __str__(self):
        "Dump this tag in import-stream format."
        st = "tag %s\nfrom %s\n" % (self.name, self.committish)
        if self.tagger:
            st += "tagger %s\n" % self.tagger
        st += "data %d\n%s\n" % (len(self.comment), self.comment,)
        return st

class Reset:
    "Represents a branch creation."
    def __init__(self):
        self.ref = None
        self.committish = None
    def __str__(self):
        "Dump this reset in import-stream format."
        st = "reset %s\n" % self.ref
        if self.committish:
            st += "from %s\n\n" % self.committish
        return st

class FileOp:
    "Represent a primitive operation on a file."
    modify_re = re.compile(r"(M) ([0-9]+) (\S+) (.*)")
    def __init__(self, commit=None):
        self.op = None
        self.commit = commit                 # Only used for debugging.
        self.committish = None
        self.source = None
        self.target = None
        self.mode = None
        self.path = None
        self.ref = None
    # Following two functions emulate the FileOp sort used by git
    # fast_export As it says, 'Handle files below a directory first,
    # in case they are all deleted and the directory changes to a file
    # or symlink.'
    def __pathname__(self):
        if hasattr(self, 'path'):
            return self.path
        elif hasattr(self, 'source'):
            return self.source
        elif self.op == 'deleteall':
            return ""
        else:
            raise Fatal("internal error while extracting pathname")
    def construct(self, *opargs):
        if opargs[0] == "M":
            (self.op, self.mode, self.ref, self.path) = opargs
            if type(self.mode) == type(0):
                self.mode = "%06o" % self.mode
        elif opargs[0] == "D":
            (self.op, self.path) = opargs
        elif opargs[0] in ("R", "C"):
            (self.op, self.source, self.target) = opargs
        elif opargs[0] == "deleteall":
            self.op = "deleteall"
        else:
            raise Fatal("unexpected fileop %s" % opargs[0])
    def parse(self, opline):
        if opline.startswith("M"):
            m = FileOp.modify_re.match(opline)
            if not m:
                raise Fatal("bad format of M line: %s" % repr(opline))
            (self.op, self.mode, self.ref, self.path) = m.groups()
        elif opline[0] == "N":
            (self.op, self.ref, self.committish) = shlex.split(opline)
        elif opline[0] == "D":
            (self.op, self.path) = ("D", opline[2:].strip())
        elif opline[0] in ("R", "C"):
            (self.op, self.source, self.target) = shlex.split(opline)
        elif opline == "deleteall":
            self.op = "deleteall"
        else:
            raise Fatal("unexpected fileop %s while parsing" % opline)
        return self
    def inlinefile(self):
        return self.commit.repo.subdir() + "/inline-" + repr(id(self))
    def paths(self):
        "Return the set of all paths touched by this file op."
        if self.op in ("M", "D"):
            return set([self.path])
        elif self.op in ("R", "C"):
            return set([self.source, self.target])
        elif self.op == "N":
            return set([])
    def relevant(self, other):
        "Do two fileops touch the same file(s)?"
        if self.op == "deleteall" or other.op == "deleteall":
            return True
        else:
            return self.paths() & other.paths()
    @staticmethod
    def compare(a, b):
        "Emulates the sort that git fast-export uses."
        name_a = FileOp.__pathname__(a)
        name_b = FileOp.__pathname__(b)
        len_a = len(name_a)
        len_b = len(name_b)
        slen = min(len_a, len_b)
        # strcmp will sort 'd' before 'd/e', we want 'd/e' before 'd'
        c = cmp(name_a[:slen], name_b[:slen])
        if c:
            return c
        c = len_b - len_a
        if c:
            return c
        # renames go last
        return (a.op == 'R') - (b.op == 'R') 
    def __str__(self):
        "Dump this fileop in import-stream format."
        if self.op == "M":
            showmode = self.mode
            if type(self.mode) == type(0):
                showmode = "%06o" % self.mode
            st = " ".join((self.op, showmode, self.ref)) + " "
            if len(self.path.split()) > 1:
                st += '"' + self.path + '"'
            else:
                st += self.path
            if self.ref == 'inline':
                with open(self.inlinefile()) as fp:
                    content = fp.read()
                st += "\ndata %d\n%s" % (len(content), content)
        elif self.op == "N":
            st = " ".join((self.op, self.ref, self.committish)) + "\n"
            if self.ref == 'inline':
                with open(self.inlinefile()) as fp:
                    content = fp.read()
                st += "data %d\n%s" % (len(content), content)
        elif self.op in "D":
            st = "D "
            if len(self.path.split()) > 1:
                st += '"' + self.path + '"'
            else:
                st += self.path
        elif self.op in ("R", "C"):
            st = '%s "%s" "%s"' %  (self.op, self.source, self.target)
        elif self.op == "deleteall":
            st = self.op
        else:
            raise Fatal("unexpected fileop %s while writing" % self.op)
        return st

class Commit:
    "Generic commit object."
    def __init__(self, repo):
        self.repo = repo
        self.mark = None             # Mark name of commit (may be None)
        self.authors = []            # Authors of commit
        self.committer = None        # Person responsible for committing it.
        self.comment = None          # Commit comment
        self.parent_marks = []       # List of parent nodes
        self.branch = None           # branch name
        self.fileops = []            # blob and file operation list
        self.properties = OrderedMap()         # commit properties (extension)
        self.pushed_to = False       # Flagged for resolution after delete
        self.color = None
        self.common = None           # Used only by the Subversion parser
    def when(self):
        "Imputed timestamp for sorting after merges."
        return self.committer.date.timestamp
    def moveto(self, repo):
        "Change the repo this commit is associated with."
        oldinlines = []
        for fileop in self.fileops:
            if fileop.op == 'M' and fileop.ref == 'inline': 
                oldinlines.append(fileop.inlinefile()) 
        self.repo = repo
        newinlines = []
        for fileop in self.fileops:
            if fileop.op == 'M' and fileop.ref == 'inline': 
                newinlines.append(fileop.inlinefile())
        for (old, new) in zip(oldinlines, newinlines):
            os.rename(old, new)
    def clone(self, repo=None):
        "Clone this commit, without its fileops."
        c = copy.copy(self)
        c.committer = copy.deepcopy(self.committer)
        c.authors = copy.deepcopy(self.authors)
        c.fileops = []
        if repo is not None:
            c.moveto(repo)
        return c
    def lister(self, eventnum, cols):
        "Enable do_list() to report commits."
        topline = self.comment.split("\n")[0]
        return "%6d %s %-*s" % \
                      (eventnum+1, self.committer.date.iso8601()[:-5], cols-27, topline[:cols-27]) 
    def tags(self, eventnum, cols):
        "Enable do_list() to report lightweight tags."
        assert cols > -1    # pacify pylint
        return self.branch and "/tags/" in self.branch and "%6d %s" % (eventnum+1, self.branch) 
    def email_out(self, eventnum):
        "Enable do_mailbox_out() to report these."
        msg = RepoSurgeonEmail()
        msg["Event-Number"] = str(eventnum+1)
        msg["Branch"] = self.branch
        if self.authors:
            self.authors[0].email_out(msg, "Author")
            for (i, coauthor) in enumerate(self.authors[1:]):
                coauthor.email_out(msg, "Author" + repr(2+i))
        self.committer.email_out(msg, "Committer")
        for (name, value) in self.properties.items():
            hdr = "-".join([s.capitalize() for s in name.split("-")])
            value = value.replace("\n", r"\n")
            value = value.replace("\t", r"\t")
            msg["Property-" + hdr] = value
        msg.set_payload(self.comment)
        return str(msg)
    def email_in(self, msg):
        "Update this commit from a parsed email message."
        idme = self.mark or "unmarked"
        if "Branch" not in msg:
            raise Fatal("update to commit %s is malformed" % idme)
        if "Committer" not in msg or "Committer-Date" not in msg:
            raise Fatal("update to commit %s is malformed" % idme)
        modified = False
        if self.branch != msg["Branch"]:
            modified = True
        self.branch = msg["Branch"]
        (newname, newemail) = email.utils.parseaddr(msg["Committer"])
        if not newemail:
            raise Fatal("can't recognize address in Committer: %s" % msg["Committer"])
        else:
            if self.committer.name != newname or self.committer.email != newemail:
                (self.committer.name, self.committer.email) = (newname, newemail)
                if verbose >= DEBUG_EMAILIN:
                    announce("in commit %d, Committer is modified" \
                          % (int(msg["Event-Number"])))
                modified = True
        date = Date(msg["Committer-Date"])
        if date != self.committer.date:
            if verbose >= DEBUG_EMAILIN:
                announce("in event %d, Committer-Date is modified '%s' -> '%s' (delta %d)" \
                      % (int(msg["Event-Number"]),
                         self.committer.date, date,
                         self.committer.date.delta(date)))
            self.committer.date = date
            modified = True
        if "Author" in msg:
            author_re = re.compile("Author[0-9]*$")
            authorkeys = list(filter(author_re.match, list(msg.keys())))
            # Potential minor bug here if > 10 authors;
            # lexicographic sort order doesn't match numeric
            authorkeys.sort()
            for i in range(len(authorkeys) - len(self.authors)):
                self.authors.append(Attribution())
            # Another potential minor bug: permuting the set of authors
            # will look like a modification, as old and new authors are
            # compaired pairwise rather than set equality being checked.
            # Possibly a feature if one thinks order is significant, but
            # I just did it this way because it was easier.
            for (i, hdr) in enumerate(authorkeys):
                (newname, newemail) = email.utils.parseaddr(msg[hdr])
                if not newemail:
                    raise Fatal("can't recognize address in %s: %s" % (hdr, msg[hdr]))
                else:
                    if self.authors[i].name != newname or self.authors[i].email != newemail:
                        (self.authors[i].name, self.authors[i].email) = (newname, newemail)
                        if verbose >= DEBUG_EMAILIN:
                            announce("in commit %d, Author is modified" \
                                  % (int(msg["Event-Number"])))
                        modified = True
                if hdr + "-Date" in msg:
                    date = Date(msg[hdr + "-Date"])
                    if date != self.authors[i].date:
                        if verbose >= DEBUG_EMAILIN:
                            announce("in event %d, %s-Date is modified" \
                                  % (int(msg["Event-Number"]), hdr))
                        self.authors[i].date = date
                        modified = True
        newprops = OrderedMap()
        for prophdr in [s for s in list(msg.keys()) if s.startswith("Property-")]:
            propkey = prophdr[9:].lower()
            propval = msg[prophdr]
            if propval == "True":
                propval = True
            elif propval == "False":
                propval = False
            else:
                propval = propval.replace(r"\n", "\n")
                propval = propval.replace(r"\t", "\t")
            newprops[propkey] = propval
        modified |= (newprops != self.properties)
        self.properties = newprops
        newcomment = msg.get_payload()
        if newcomment != self.comment:
            if verbose >= DEBUG_EMAILIN:
                announce("in commit %d, comment is modified '%s' -> '%s'" \
                      % (int(msg["Event-Number"]), repr(self.comment), repr(newcomment)))
            modified = True
            self.comment = newcomment
        return modified
    def children(self):
        "Get a list of this commit's children."
        return [e for e in self.repo.commits() if self.mark in e.parent_marks] 
    def parents(self):
        "Get a list of this commit's parents."
        return [e for e in self.repo.commits() if e.mark in self.parent_marks] 
    def cliques(self):
        "Return a dictionary mapping filenames to associated M cliques."
        cliques = {}
        for (i, fileop) in enumerate(self.fileops):
            if fileop.op == "M":
                if fileop.path not in cliques:
                    cliques[fileop.path] = []
                cliques[fileop.path].append(i)
        return cliques
    def fileop_dump(self, i):
        "Dump file ops without data or inlines; used for debugging only."
        print("commit %d, mark %s:" % (i+1, self.mark))
        for (i, op) in enumerate(self.fileops):
            if op is not None:
                print("%d: %-20s" % (i, str(op)))
    def paths(self):
        "Return the set of all paths touched by this commit."
        pathset = set([])
        for fileop in self.fileops:
            pathset |= fileop.paths()
        return pathset
    def manifest(self):
        "Return a map from paths to marks for files existing at this commit."
        ancestors = {}
        for commit in self.parents():
            ancestors.update(commit.manifest())
        for fileop in self.fileops:
            if fileop.op == 'M':
                ancestors[fileop.path] = fileop.ref
            elif fileop.op == 'D':
                if fileop.path in ancestors:
                    del ancestors[fileop.path]
            elif fileop.op == 'C':
                ancestors[fileop.target] = ancestors[fileop.source]
            elif fileop.op == 'R':
                ancestors[fileop.target] = ancestors[fileop.source]
                if fileop.source in ancestors:
                    del ancestors[fileop.source]
        return ancestors
    def checkout(self, directory=None):
        "Make a directory with links to files in a specified checkout."
        if not directory:
            directory = os.path.join(self.repo.subdir(), self.mark)
        try:
            sys.setrecursionlimit(len(self.repo.events) * 2)
            os.mkdir(directory)
            for (path, mark) in self.manifest().items():
                fullpath = os.path.join(directory, path)
                fulldir = os.path.dirname(fullpath)
                if not os.path.exists(fulldir):
                    os.makedirs(fulldir)
                os.link(self.repo.objfind(mark).blobfile(), fullpath)
        except OSError:
            raise Recoverable("could not create checkout directory or files.")
        return directory
    def set_fossil(self, value):
        "Set the commit's fossil property (for reference-lifting)."
        oldval = self.properties.get("fossil")
        if oldval is not None and oldval != value:
            complain("fossil property of %s overwritten, %s to %s" \
                     % (self.mark, oldval, value))
        self.properties["fossil"] = value
    def get_fossil(self):
        return self.properties.get("fossil")
    def dump(self, write_properties=True):
        "Dump this commit in import-stream format."
        st = "commit %s\n" % self.branch
        if self.mark:
            st += "mark %s\n" % self.mark
        if self.authors:
            for author in self.authors:
                st += "author %s\n" % author
        if self.committer:
            st += "committer %s\n" % self.committer
        if self.comment is not None:
            st += "data %d\n" % len(self.comment)
            st += self.comment
        if "nl-after-comment" in self.repo.export_style():
            st += "\n"
        if self.parent_marks:
            st += "from %s\n" % self.parent_marks[0]
        for ancestor in self.parent_marks[1:]:
            st += "merge %s\n" % ancestor
        if write_properties:
            for (name, value) in self.properties.items():
                if value in (True, False):
                    if value:
                        st += "property %s\n" % name
                else:
                    st += "property %s %d %s\n" % (name, len(value), value)
        for op in self.fileops:
            st += str(op) + "\n"
        if "nl-after-commit" in self.repo.export_style():
            st += "\n"
        return st
    def __str__(self):
        return self.dump(write_properties=not self.repo or not self.repo.vcs or self.repo.vcs.properties)

class Passthrough:
    "Represents a passthrough line."
    def __init__(self, line):
        self.text = line
    def email_out(self, eventnum):
        "Enable do_mailbox_out() to report these."
        msg = RepoSurgeonEmail()
        msg["Event-Number"] = str(eventnum+1)
        msg.set_payload(self.text)
        return str(msg)
    def email_in(self, msg):
        self.text = msg.get_payload()
    def __str__(self):
        "Dump this passthrough in import-stream format."
        return self.text

class Fatal(Exception):
    "Unrecoverable error."
    def __init__(self, msg):
        Exception.__init__(self)
        self.msg = msg

# Generic extractor code begins here

class signature:
    "A file signature - file path, hash value of content and permissions."
    def __init__(self, path):
        self.path = path
        self.hashval = None
        self.perms = None
        if not os.path.isdir(path):
            with file(path) as fp:
                self.hashval = hashlib.sha1(fp.read()).hexdigest()
            self.perms = os.stat(path).st_mode
            # Map to the restricted set of modes that are allowed in
            # the stream format.
            if self.perms & 0o100700 == 0o100700:
                self.perms = 0o100755
            elif self.perms & 0o100600 == 0o100600:
                self.perms = 0o100644
    def __eq__(self, other):
        #if verbose >= DEBUG_EXTRACT:
        #    announce("%s == %s -> %s" % (str(self),
        #                                 str(other),
        #                                 self.__dict__ == other.__dict__))
        return self.__dict__ == other.__dict__
    def __ne__(self, other):
        return not signature.__eq__(self, other)
    def __str__(self):
        return "<%s:%s:%s>" % (self.path, "%6o" % self.perms, self.hashval[:4])
    __repr__ = __str__

def signature_map(paths):
    return dict([(path, signature(path)) for path in paths])

def capture(command):
    "Run a specified command, capturing the output."
    if verbose >= DEBUG_COMMANDS:
        announce("%s: capturing %s" % (rfc3339(time.time()), command))
    try:
        content = subprocess.check_output(command, shell=True)
    except (subprocess.CalledProcessError, OSError) as oe:
        raise Fatal("execution of '%s' failed: %s" % (command, oe))
    if verbose >= DEBUG_COMMANDS:
        sys.stderr.write(content)
    return content

class FileMap:
    """Object store indexed by file path and version. Used because
extractors for changeset-oriented systems like Subversion need to
track which files have existed at given revisions. And they need to do
it space-efficiently - in repos with thousands of commits the data
set gets huge."""
    FileTag = "metadata"
    def __init__(self, basedir):
        self.basedir = basedir[len(os.getcwd())+1:]
    def __revname(self, revision):
        "Return the top level of a revision's metadata directory."
        return os.path.join(self.basedir, revision)
    def __stashname(self, revision, path):
        "Return the metadata file for any given path in the history."
        return os.path.join(self.__revname(revision), path, FileMap.FileTag)
    def register(self, revision, path, payload):
        "Register metadata for a specified path and revision."
        stashname = self.__stashname(revision, path)
        if verbose >= DEBUG_EXTRACT+1:
            announce("register(r%s, %s, %s)" % (revision, path, payload))
        if not os.path.exists(os.path.dirname(stashname)):
            os.makedirs(os.path.dirname(stashname))
        with open(stashname, "w") as ofp:
            cPickle.dump(payload, ofp)
    def retrieve(self, revision, path):
        "Retrieve metadata for a specified path and revision."
        stashname = self.__stashname(revision, path)
        with open(stashname, "r") as ifp:
            payload = cPickle.load(ifp)
        if verbose >= DEBUG_EXTRACT+1:
            announce("retrieve(r%s, %s) -> %s" % (revision, path, payload))
        return payload
    def remove(self, revision, path):
        "Remove a file path from the map."
        # Does not scavenge directories that might have had
        # this as its only file - that's an unusual case in
        # a source-code repo, and directory entries are cheap.
        stashname = self.__stashname(revision, path)
        if verbose >= DEBUG_EXTRACT+1:
            announce("remove(r%s, %s)" % (revision, path))
        os.remove(stashname)
        os.rmdir(os.path.dirname(stashname))
    def derive(self, revision, parents):
        "Create a new revision filemap that is the union of the parent maps."
        # TODO: Presently only handles single-parent case.
        if verbose >= DEBUG_EXTRACT+1:
            announce("derive(r%s, %s)" % (revision, parents))
        if os.path.exists(self.__revname(parents[0])):
            shutil.copytree(self.__revname(parents[0]),
                            self.__revname(revision))
        return self.children(revision, "")
    def member_of(self, revision, path):
        "Is a specified path in the specified revision?"
        ismember = os.path.exists(self.__stashname(revision, path))
        if verbose >= DEBUG_EXTRACT+1:
            announce("member_of(r%s, %s) = %s" % (revision, path, ismember))
        return ismember
    def __clipper(self, rev, cdir):
        "Trim our housekeeping stuff off a path in the filestore."
        return cdir[len(self.basedir+os.sep+rev+os.sep):-len(os.sep+FileMap.FileTag)]
    @staticmethod
    def dirwalk(cdir):
        "Walk a directory tree, using a generator"
        for f in os.listdir(cdir):
            fullpath = os.path.join(cdir,f)
            if os.path.isdir(fullpath) and not os.path.islink(fullpath):
                for x in FileMap.dirwalk(fullpath):  # recurse into subdir
                    yield x
            else:
                yield fullpath
    def children(self, revision, parent):
        "Return a list of the children of a specified node."
        # TODO: turn this into an iterator generator
        subtree = []
        stashname = os.path.dirname(self.__stashname(revision, parent))
        if os.path.isdir(stashname):
            for cdir in FileMap.dirwalk(stashname):
                subtree.append(self.__clipper(revision, cdir))
        if verbose >= DEBUG_EXTRACT+1:
            announce("children(r%s, %s) = %s" % (revision, parent, subtree))
        return subtree

class RepoStreamer:
    "Repository factory driver class for all repo analyzers."
    def __init__(self, extractor):
        self.markseq = 0
        self.tagseq = 0
        self.commits = {}
        self.markmap = {}
        self.filemap = None
        self.hash_to_mark = {}
        self.baton = None
        self.extractor = extractor
    def __newmark(self, revision=None):
        self.markseq += 1
        mark = ":" + str(self.markseq)
        if revision:
            self.markmap[revision] = mark
        return mark
    def extract(self, repo, progress=True):
        if not self.extractor.isclean():
            raise Recoverable("directory %s has unsaved changes." % os.getcwd())
        repo.makedir()
        self.filemap = FileMap(repo.subdir())
        with Baton(prompt="Extracting", enable=progress) as self.baton:
            self.extractor.analyze(self.baton)
            self.extractor.pre_extract(repo)
            #saved_umask = os.umask(0)
            consume = copy.copy(self.extractor.get_revlist())
            while consume:
                revision = consume.pop(0)
                commit = Commit(repo)
                self.baton.twirl()
                present = self.extractor.checkout(revision, self.filemap)
                parents = self.extractor.get_parents(revision)
                commit.committer = Attribution(self.extractor.get_committer(revision))
                commit.authors = [Attribution(a) \
                                  for a in self.extractor.get_authors(revision)]
                commit.parent_marks = [self.markmap[rev] for rev in parents]
                commit.branch = self.extractor.get_branch(revision)
                commit.comment = self.extractor.get_comment(revision)
                if verbose >= DEBUG_EXTRACT:
                    msg = commit.comment
                    if msg == None:
                        msg = ""
                    announce("r%s: comment '%s'" % (revision, msg.strip()))
                expected = self.filemap.derive(revision, parents)
                if present:
                    removed = set(expected.keys()) - set(present)
                    for path in present:
                        if os.path.isdir(path):
                            continue
                        if not os.path.exists(path):
                            announce("r%s: expected path %s does not exist!" % \
                                     (revision, path))
                            continue
                        newsig = signature(path)
                        if newsig.hashval in self.hash_to_mark:
                            #if verbose >= DEBUG_EXTRACT:
                            #    announce("r%s: %s has old hash" \
                            #             % (revision, path))
                            # The file's hash corresponds to an existing
                            # blob; generate modify, copy, or rename as
                            # appropriate.
                            if self.filemap.retrieve(revision, path) != newsig:
                                if verbose >= DEBUG_EXTRACT:
                                    announce("r%s: update for %s" % (revision, path))
                                for (oldpath, oldsig) in self.filemap.items(revision):
                                    if oldsig == newsig:
                                        if oldpath in removed:
                                            op = FileOp(commit)
                                            op.construct('R', oldpath, path)
                                            commit.fileops.append(op)
                                            self.filemap.remove(revision,
                                                                oldpath)
                                            self.filemap.register(revision,
                                                                  path,
                                                                  oldsig)
                                        elif oldpath != path:
                                            op = FileOp(commit)
                                            op.construct('C', oldpath, path)
                                            commit.fileops.append(op)
                                            self.filemap.record(revision,
                                                                path,
                                                                oldsig)
                                        break
                                else:
                                    op = FileOp(commit)
                                    op.construct('M',
                                                 newsig.perms,
                                                 self.hash_to_mark[newsig.hashval],
                                                 path)
                                    commit.fileops.append(op)
                        else:
                            # Content hash doesn't match any existing blobs
                            if verbose >= DEBUG_EXTRACT:
                                announce("r%s: %s has new hash" \
                                         % (revision, path))
                            blobmark = self.__newmark()
                            self.hash_to_mark[newsig.hashval] = blobmark
                            # Actual content enters the representation
                            blob = Blob(repo)
                            blob.mark = blobmark
                            shutil.copyfile(path, blob.blobfile())
                            blob.path = path
                            repo.events.append(blob)
                            # Its new fileop is added to the commit
                            op = FileOp(commit)
                            op.construct('M', newsig.perms, blobmark, path)
                            commit.fileops.append(op)
                        self.filemap.register(revision, path, newsig)
                    for tbd in removed:
                        op = FileOp(commit)
                        op.construct('D', tbd)
                        commit.fileops.append(op)
                        self.filemap.remove(revision, tbd)
                self.extractor.cleanup(revision, True)
                if not parents and commit.branch != "refs/heads/master":
                    reset = Reset()
                    reset.ref = commit.branch
                    repo.events.append(reset)
                commit.fileops.sort(cmp=FileOp.compare)
                commit.set_fossil(revision)
                commit.properties.update(self.extractor.get_properties(revision)) 
                commit.mark = self.__newmark(revision)
                if verbose >= DEBUG_EXTRACT:
                    announce("r%s: gets mark %s (%d ops)" % (revision, commit.mark, len(commit.fileops)))
                repo.events.append(commit)
            # Now append reset objects
            resets = self.extractor.get_resetlist()
            resets.sort(key=lambda (k, v): v)
            for (resetname, revision) in resets:
                reset = Reset()
                reset.ref = resetname
                reset.committish = self.markmap[revision]
                repo.events.append(reset)
            # Last, append tag objects.
            tags = self.extractor.get_taglist()
            tags.sort(key=lambda t: t.tagger.date)
            for tag in tags:
                tag.committish = self.markmap.get(tag.committish)
                repo.events.append(tag)
            self.extractor.post_extract(repo)
        return repo

# Stream parsing
#
# The Subversion dumpfile format is documented at
#
# https://svn.apache.org/repos/asf/subversion/trunk/notes/dump-load-format.txt
#
# See also dumpfile.txt in this directory

# Use numeric codes rather than (un-interned) strings
# to reduce working-set size.
SD_NONE = 0
SD_FILE = 1
SD_DIR = 2
SD_ADD = 0
SD_DELETE = 1
SD_CHANGE = 2
SD_REPLACE = 3

class Trace:
    "All the metadata we need to retain across revisions about a file."
    def __init__(self):
        self.mark = None
        self.perms = 0o100644
    def set_perms(self, newperms):
        self.perms = newperms
    def set_mark(self, newmark):
        self.mark = newmark
    def clone(self):
        return copy.copy(self)
    def __eq__(self, other):
        return self.__dict__ == other.__dict__
    def __ne__(self, other):
        return self.__dict__ != other.__dict__
    def __str__(self):
        return "<Trace(%s, %06o)>" % (self.mark, self.perms)
    __repr__ = __str__

class StreamParser:
    "Parse a fast-import stream or Subversion dump to populate a Repository."
    # If these don't match the constants above, havoc will ensue
    class NodeAction:
        ActionValues = ("add", "delete", "change", "replace")
        PathTypeValues = ("none", "file", "dir")
        def __init__(self):
            self.revision = None
            self.path = None
            self.kind = SD_NONE
            self.action = None
            self.from_rev = None
            self.from_path = None
            self.blob = None
            self.props = None
            self.history = None
            self.from_set = None
        def __str__(self):
            tell = "<NodeAction: r%s %s %s '%s' " \
                   % (self.revision,
                      StreamParser.NodeAction.ActionValues[self.action],
                      StreamParser.NodeAction.PathTypeValues[self.kind],
                      self.path)
            if self.from_rev:
                tell += "from=%s:%s %s " % (self.from_rev, self.from_path, self.from_set)
            if self.props is not None:
                # Trim off the OrderedDict wrapper
                tell += "%s " % self.props
            return tell[:-1] + ">"
        __repr__ = __str__
    class RevisionRecord:
        def __init__(self, nodes, props):
            self.nodes = nodes
            self.props = props
    # Native Subversion properties that we don't suppress:
    # svn:externals, svn:mergeinfo.  The reason for these suppressions
    # is to avoid a huge volume of junk file properties - cvs2svn
    # in particular generates them like mad.  We want to let through other
    # properties that might carry useful information.
    IgnoreProperties = (
        "svn:executable",  # We special-case this one elsewhere
        "svn:ignore",      # We special-case this one elsewhere
        "svn:special",     # We special-case this one elsewhere
        "svn:mime-type",
        "svn:keywords",
        "svn:eol-style",
        "svn:needs-lock",
        )
    def __init__(self, repo):
        self.repo = repo
        self.fp = None
        self.import_line = 0
        self.markseq = 0
        self.ccount = 0
        self.linebuffers = []
        # Everying below here is Subversion-specific
        self.markmap = {}       # Revision-to-mark map
        self.parent = None
        self.branches = {}
        self.branchlink = {}
        self.uuid = None
        self.revisions = OrderedMap()
        self.filemap = None
        self.histories = {}
    def error(self, msg, atline=True):
        "Throw fatal error during parsing."
        if atline:
            raise Fatal(msg + " at line " + repr(self.import_line))
        else:
            raise Fatal(msg)
    def warn(self, msg, atline=True):
        "Display a parse warning."
        if atline and self.import_line:
            print("reposurgeon: " + msg + " at line " + repr(self.import_line))
        else:
            print("reposurgeon: " + msg)
    def __newmark(self, revision=None):
        self.markseq += 1
        mark = ":" + str(self.markseq)
        if revision:
            self.markmap[revision] = mark
        return mark
    def readline(self):
        if self.linebuffers:
            line = self.linebuffers.pop()
        else:
            line = self.fp.readline()
        self.ccount += len(line)
        self.import_line += 1
        return line
    def pushback(self, line):
        self.ccount -= len(line)
        self.import_line -= 1
        self.linebuffers.append(line)
    # Helpers for import-stream files
    def fi_read_data(self, line=None):
        "Read a fast-import data section."
        if not line:
            line = self.readline()
        if line.startswith("data <<"):
            delim = line[7:]
            while True:
                dataline = self.readline()
                if dataline == delim:
                    break
                elif not dataline:
                    raise Fatal("EOF while reading blob")
        elif line.startswith("data"):
            try:
                count = int(line[5:])
                data = self.fp.read(count)
            except ValueError:
                self.error("bad count in data")
        else:
            self.error("malformed data header %s" % repr(line))
        line = self.readline()
        if line != '\n':
            self.pushback(line) # Data commands optionally end with LF
        return data
    def fi_parse_fileop(self, fileop):
        # Read a fast-import fileop
        if fileop.ref[0] == ':':
            pass
        elif fileop.ref == 'inline':
            wfp = open(fileop.inlinefile(), "w")
            wfp.write(self.fi_read_data())
            wfp.close()
        else:
            self.error("unknown content type in filemodify")
    # Helpers for Subversion dumpfiles
    @staticmethod
    def sd_body(line):
        # Parse the body from a Subversion header line
        return line.split(":")[1].strip()
    def sd_require_header(self, hdr):
        # Consume a required header line
        line = self.readline()
        self.ccount += len(line)
        if not line.startswith(hdr):
            self.error('required %s header missing')
        return StreamParser.sd_body(line)
    def sd_require_spacer(self):
        line = self.readline()
        if line.strip():
            self.error('found %s expecting blank line' % repr(line))
    def sd_read_blob(self, length):
        content = self.fp.read(length)
        assert self.fp.read(1) == '\n'
        self.import_line += content.count('\n') + 1
        self.ccount += len(content) + 1
        return content
    def sd_read_props(self, target, checklength):
        # Parse a Subversion properties section, return as an OrderedDict.
        props = OrderedMap()
        self.ccount = 0
        while self.ccount < checklength:
            line = self.readline()
            if verbose >= DEBUG_EXTRACT+1:
                announce("readprops, line %d: %s" % \
                         (self.import_line, repr(line)))
            if line.startswith("PROPS-END"):
                # This test should be !=, but I get random off-by-ones from
                # real dumpfiles - I don't know why.
                if self.ccount < checklength:
                    self.error("expected %d property chars, got %d"\
                               % (checklength, self.ccount))
                break
            elif not line.strip():
                continue
            elif line[0] == "K":
                key = self.sd_read_blob(int(line.split()[1]))
                line = self.readline()
                if line[0] != 'V':
                    raise self.error("property value garbled")
                value = self.sd_read_blob(int(line.split()[1]))
                props[key] = value
                if verbose >= DEBUG_EXTRACT+1:
                    announce("readprops: on %s, setting %s = %s"\
                             % (target, key, repr(value)))
        return props
    #
    # The main event
    #
    def fast_import(self, fp, progress=False):
        "Initialize the repo from a fast-import stream.."
        self.repo.makedir()
        try:
            self.fp = fp
            with Baton("reposurgeon: from %s" % os.path.relpath(fp.name), enable=progress) as baton:
                self.import_line = 0
                self.linebuffers = []
                # First, determine the input type
                line = self.readline()
                if line.startswith("SVN-fs-dump-format-version: "):
                    if StreamParser.sd_body(line) not in ("1", "2"):
                        raise Fatal("unsupported dump format version %s" \
                                    % version)
                    self.filemap = FileMap(self.repo.subdir())
                    # Beginning of Subversion dump parsing
                    while True:
                        line = self.readline()
                        if not line:
                            break
                        elif not line.strip():
                            continue
                        elif line.startswith("UUID:"):
                            self.uuid = StreamParser.sd_body(line)
                        elif line.startswith("Revision-number: "):
                            # Begin Revision processing
                            if verbose >= DEBUG_EXTRACT+1:
                                announce("revision parsing, line %d: begins" % \
                                     (self.import_line))
                            revision = StreamParser.sd_body(line)
                            plen = int(self.sd_require_header("Prop-content-length"))
                            self.sd_require_header("Content-length")
                            self.sd_require_spacer()
                            props = self.sd_read_props("commit", plen)
                            # Parsing of the revision header is done
                            node = None # pacify pylint
                            nodes = []
                            in_header = False
                            plen = tlen = -1
                            # Node list parsing begins
                            while True:
                                line = self.readline()
                                if verbose >= DEBUG_EXTRACT+1:
                                    announce("node list parsing, line %d: %s" % \
                                             (self.import_line, repr(line)))
                                if not line:
                                    break
                                elif not line.strip():
                                    if not in_header:
                                        continue
                                    else:
                                        if plen > -1:
                                            node.props = self.sd_read_props(node.path, plen)
                                        if tlen > -1:
                                            text = self.sd_read_blob(tlen)
                                            node.blob = Blob(self.repo)
                                            with open(node.blob.blobfile(), "w") as wfp:
                                                wfp.write(text)
                                        node.revision = revision
                                        nodes.append(node)
                                        in_header = False
                                elif line.startswith("Revision-number: "):
                                    self.pushback(line)
                                    break
                                # Node processing begins
                                elif line.startswith("Node-path: "):
                                    node = StreamParser.NodeAction()
                                    node.path = StreamParser.sd_body(line)
                                    plen = tlen = -1
                                    in_header = True
                                elif line.startswith("Node-kind: "):
                                    node.kind = StreamParser.sd_body(line)
                                    node.kind = StreamParser.NodeAction.PathTypeValues.index(node.kind)
                                    if node.kind is None:
                                        self.error("unknown kind %s"%node.kind)
                                elif line.startswith("Node-action: "):
                                    node.action = StreamParser.sd_body(line)
                                    node.action = StreamParser.NodeAction.ActionValues.index(node.action)
                                    
                                    if node.action is None:
                                        self.error("unknown action %s" \
                                                   % node.action)
                                elif line.startswith("Node-copyfrom-rev: "):
                                    node.from_rev = StreamParser.sd_body(line)
                                elif line.startswith("Node-copyfrom-path: "):
                                    node.from_path = StreamParser.sd_body(line)
                                elif line.startswith("Text-copy-source-md5: "):
                                    continue
                                elif line.startswith("Text-content-md5: "):
                                    continue
                                elif line.startswith("Text-content-sha1: "):
                                    continue
                                elif line.startswith("Text-content-length: "):
                                    tlen = int(StreamParser.sd_body(line))
                                elif line.startswith("Prop-content-length: "):
                                    plen = int(StreamParser.sd_body(line))
                                elif line.startswith("Content-length: "):
                                    continue
                                else:
                                    if verbose >= DEBUG_EXTRACT+1:
                                        announce("node list parsing, line %d: uninterpreted line %s" % \
                                             (self.import_line, repr(line)))
                                    continue
                                # Node processing ends
                            # Node list parsing ends
                            self.revisions[revision] = StreamParser.RevisionRecord(nodes, props)
                            if verbose >= DEBUG_EXTRACT+1:
                                announce("revision parsing, line %d: ends" % \
                                         (self.import_line))
                            # End Revision processing
                    # End of Subversion dump parsing
                    self.svn_postprocess(baton)
                else:
                    self.pushback(line)
                    # Beginning of fast-import stream parsing
                    while True:
                        line = self.readline()
                        if not line:
                            break
                        elif not line.strip():
                            continue
                        elif line.startswith("blob"):
                            blob = Blob(self.repo)
                            line = self.readline()
                            if line.startswith("mark"):
                                blob.mark = line[5:].strip()
                                wfp = open(blob.blobfile(), "w")
                                blobcontent = self.fi_read_data()
                                # Parse CVS and Subversion $-headers
                                # There'd better not be more than one of these.
                                for m in re.finditer(r"\$Id *:[^$]*\$",
                                                     blobcontent):
                                    fields = m.group(0).split()
                                    # Save file basename and CVS version
                                    if fields[1].endswith(",v"):
                                        # CVS revision
                                        blob.cookie = (fields[1][:-2], fields[2])
                                    else:
                                        # Subversion revision
                                        blob.cookie = fields[1]
                                    self.repo.write_fossils = True
                                for m in re.finditer(r"\$Revision *: *([^$]*)\$",
                                                     blobcontent):
                                    rev = m.group(0).strip()
                                    if '.' not in rev:
                                        # Subversion revision
                                        blob.cookie = rev
                                        self.repo.write_fossils = True
                                wfp.write(blobcontent)
                                wfp.close()
                            else:
                                self.error("missing mark after blob")
                            self.repo.events.append(blob)
                            baton.twirl()
                        elif line.startswith("data"):
                            self.error("unexpected data object")
                        elif line.startswith("commit"):
                            commitbegin = self.import_line
                            commit = Commit(self.repo)
                            commit.branch = line.split()[1]
                            while True:
                                line = self.readline()
                                if not line:
                                    break
                                elif line.startswith("mark"):
                                    commit.mark = line[5:].strip()
                                elif line.startswith("author"):
                                    try:
                                        commit.authors.append(Attribution(line[7:]))
                                    except ValueError:
                                        self.error("malformed author line")
                                elif line.startswith("committer"):
                                    try:
                                        commit.committer = Attribution(line[10:])
                                    except ValueError:
                                        self.error("malformed committer line")
                                elif line.startswith("property"):
                                    fields = line.split(" ")
                                    if len(fields) < 3:
                                        self.error("malformed property line")
                                    elif len(fields) == 3:
                                        commit.properties[fields[1]] = True
                                    else:
                                        name = fields[1]
                                        length = int(fields[2])
                                        value = " ".join(fields[3:])
                                        if len(value) < length:
                                            value += fp.read(length-len(value))
                                            if fp.read(1) != '\n':
                                                self.error("trailing junk on property value")
                                        elif len(value) == length + 1:
                                            value = value[:-1] # Trim '\n'
                                        else:
                                            self.error("garbage length field on property line")
                                        commit.properties[name] = value
                                elif line.startswith("data"):
                                    commit.comment = self.fi_read_data(line)
                                elif line.startswith("from") or line.startswith("merge"):
                                    commit.parent_marks.append(line.split()[1])
                                # Handling of file ops begins.
                                elif line[0] in ("C", "D", "R"):
                                    commit.fileops.append(FileOp(commit).parse(line))
                                elif line == "deleteall\n":
                                    commit.fileops.append(FileOp(commit).parse("deleteall"))
                                elif line[0] == "M":
                                    fileop = FileOp(commit).parse(line)
                                    commit.fileops.append(fileop)
                                    if fileop.mode == "160000":
                                        # This is a submodule link.  The ref
                                        # field is a SHA1 hash and the path
                                        # is an external reference name.
                                        # Don't try to collect data, just pass
                                        # it through.
                                        self.warn("submodule link")
                                    elif fileop.mode == "120000":
                                        # This is a submodule link.  Don't
                                        # try to collect data, just pass
                                        # it through.
                                        self.warn("symbolic link")
                                    else:
                                        self.fi_parse_fileop(fileop)
                                elif line[0] == "N":
                                    fileop = FileOp(commit).parse(line)
                                    commit.fileops.append(fileop)
                                    self.fi_parse_fileop(fileop)
                                # Handling of file ops ends.
                                elif line.isspace():
                                    # This handles slightly broken
                                    # exporters like the bzr-fast-export
                                    # one that may tack an extra LF onto
                                    # the end of data objects.  With it,
                                    # we don't drop out of the
                                    # commit-processing loop until we see
                                    # a *nonblank* line that doesn't match
                                    # a commit subpart.
                                    continue
                                else:
                                    # Dodgy bzr autodetection hook...
                                    if not self.repo.vcs:
                                        if "branch-nick" in commit.properties:
                                            for vcs in vcstypes:
                                                if vcs.name == "bzr":
                                                    self.repo.vcs = vcs
                                                    break
                                    self.pushback(line)
                                    break
                            if not (commit.mark and commit.committer):
                                self.import_line = commitbegin
                                self.error("missing required fields in commit")
                            if commit.mark is None:
                                self.warn("unmarked commit")
                            self.repo.events.append(commit)
                            baton.twirl()
                        elif line.startswith("reset"):
                            reset = Reset()
                            reset.ref = line[6:].strip()
                            line = self.readline()
                            if line.startswith("from"):
                                reset.committish = line[5:].strip()
                            else:
                                self.pushback(line)
                            self.repo.events.append(reset)
                            baton.twirl()
                        elif line.startswith("tag"):
                            tagger = None
                            tagname = line[4:].strip()
                            line = self.readline()
                            if line.startswith("from"):
                                referent = line[5:].strip()
                            else:
                                self.error("missing from after tag")
                            line = self.readline()
                            if line.startswith("tagger"):
                                try:
                                    tagger = Attribution(line[7:])
                                except ValueError:
                                    self.error("malformed tagger line")
                            else:
                                self.warn("missing tagger after from in tag")
                                self.pushback(line)
                            self.repo.events.append(Tag(tagname,
                                                        referent, tagger,
                                                        self.fi_read_data()))
                            baton.twirl()
                        else:
                            # Simply pass through any line we don't understand.
                            self.repo.events.append(Passthrough(line))
                    # End of fast-import parsing
                self.import_line = 0
        except KeyboardInterrupt:
            nuke(self.repo.subdir(), "reposurgeon: import interrupted, removing %s" % self.repo.subdir())
            raise KeyboardInterrupt
    #
    # The rendezvous between parsing and object building for import
    # streams is pretty trivial and best done inline in the parser
    # because reposurgeon's internal structures are designed to match
    # those entities. For Subversion dumpfiles, on the other hand,
    # there's a fair bit of impedance-matching required.  That happens
    # in the following functions.
    #
    def svn_commit(self, revision):
        "Translate Subversion actions into import-stream file operations."
        record = self.revisions[revision]
        commit = Commit(self.repo)
        # Build a linear history for the whole repo.  We'll branchify
        # it later.
        if self.parent:
            commit.parent_marks = [self.markmap[self.parent]]
            self.filemap.derive(revision, [self.parent])
        commit.set_fossil(revision)
        try:
            ad = record.props.pop("svn:date")
        except KeyError, key:
            self.error("missing required %s" % key)
        if "svn:author" in record.props:
            au = record.props.pop("svn:author")
        else:
            au = "no-author"
        if "svn:log" in record.props:
            commit.comment = record.props.pop("svn:log")
        if global_options["use_uuid"]:
            attribution = "%s <%s@%s> %s" % (au, au, self.uuid, ad)
        else:
            attribution = "%s <%s> %s" % (au, au, ad)
        commit.committer = Attribution(attribution)
        commit.properties.update(record.props)
        # Zero revision is never interesting - no operations, no comment, no
        # author, it's just a start marker for a non-incremental dump.
        if revision == "0": 
            return
        for node in record.nodes:
            if verbose >= DEBUG_EXTRACT:
                announce("r%s: %s" % (revision, node))
            # Handle per-file and per-directory properties
            perms = None
            if node.props is not None:
                # Empty properties section on a path clears its properties
                for pathprop in node.props:
                    if pathprop.startswith(node.path + "#"):
                        del node.props[pathprop]
                perms = 0o100644
                for (prop, val) in node.props.items():
                    if prop == "svn:executable":
                        perms = 0o100755
                    elif prop == "svn:special":
                        # Map to git symlink and hope it flies
                        perms = 0o120000
                    elif prop not in StreamParser.IgnoreProperties:
                        commit.properties[node.path + "#" + prop] = val
            # Actions on directories
            if node.kind == SD_DIR:
                if node.path == 'trunk' + os.sep:
                    self.branches['trunk' + os.sep] = None
                if node.action == SD_ADD:
                    # No-op, since stream format doesn't track
                    # empty directories.
                    pass
                elif node.action in (SD_DELETE, SD_REPLACE):
                    if node.action == SD_DELETE and \
                           node.path in self.branches:
                        if verbose >= DEBUG_EXTRACT:
                            announce("r%s: deleteall %s" % (revision,
                                                            node.path))
                        # It's arguable that the right thing to do with a
                        # Subversion branch tip delete is to remove it,
                        # leaving the branch tip live but inactive. But
                        # that's a policy decision we shouldn't force
                        # at this low a level.
                        fileop = FileOp(commit)
                        fileop.construct("deleteall", node.path[:-1])
                        commit.fileops.append(fileop)
                        continue
                    for child in self.filemap.children(revision, node.path):
                        if verbose >= DEBUG_EXTRACT:
                            announce("r%s: deleting %s" % (revision, child))
                        self.filemap.remove(revision, child)
                        fileop = FileOp(commit)
                        fileop.construct("D", child)
                        commit.fileops.append(fileop)
                    continue
                elif node.action == SD_CHANGE:
                    if node.props is not None:
                        if verbose >= DEBUG_EXTRACT:
                            announce("r%s: setting properties %s on %s" \
                                     % (revision, node.props, node.path))
                        # svn.ignore gets handled here,
                        if node.path == os.sep:
                            gitignore_path = ".gitignore"
                        else:
                            gitignore_path = os.path.join(node.path,
                                                          ".gitignore")
                        # There are no other directory properties that can
                        # turn into fileops.
                        if "svn:ignore" in node.props:
                            blob = Blob(self.repo)
                            blob.mark = self.__newmark() 
                            with open(blob.blobfile(), "w") as wfp:
                                wfp.write(node.props["svn:ignore"])
                            self.repo.events.append(blob)
                            newtrace = Trace()
                            newtrace.set_mark(blob.mark)
                            self.filemap.register(revision,
                                                  gitignore_path,
                                                  newtrace)
                            fileop = FileOp(commit)
                            fileop.construct("M",
                                             newtrace.perms,
                                             newtrace.mark,
                                             gitignore_path)
                            commit.fileops.append(fileop)
                        elif self.filemap.member_of(revision,
                                                    gitignore_path):
                            self.filemap.remove(revision, gitignore_path)
                            fileop = FileOp(commit)
                            fileop.construct("D", gitignore_path)
                            commit.fileops.append(fileop)
                # Handle directory copies used for branching and tags
                if node.from_path:
                    if self.branches:
                        if node.path.count(os.sep) == 2:
                            if node.path.startswith("tags" + os.sep):
                                self.branches[node.path] = None
                            elif node.path.startswith("branches" + os.sep):
                                self.branches[node.path] = None
                        elif node.path.count(os.sep) == 1:
                            # Exiguous top-level branch
                            self.branches[node.path] = None
                    for source in self.filemap.children(node.from_rev, node.from_path + os.sep):
                        subtrace = self.filemap.retrieve(node.from_rev, source)
                        assert subtrace.mark
                        fileop = FileOp(commit)
                        dest = os.path.join(node.path,
                                            source[len(node.from_path+os.sep):])
                        fileop.construct("M",
                                             subtrace.perms,
                                             subtrace.mark,
                                             dest)
                        self.filemap.register(revision, dest, subtrace)
            # Actions on files
            elif node.kind == SD_FILE:
                if node.action == SD_DELETE:
                    assert node.blob is None
                    fileop = FileOp(commit)
                    fileop.construct("D", node.path)
                    commit.fileops.append(fileop)
                    if not self.filemap.member_of(revision, node.path):
                        raise Fatal("r%s: delete of nonexistent %s %s." \
                                    % (revision, node.kind, node.path))
                    else:
                        self.filemap.remove(revision, node.path)
                elif node.action in (SD_ADD, SD_CHANGE, SD_REPLACE):
                    if node.action == SD_ADD:
                        if self.filemap.member_of(revision, node.path):
                            raise Fatal("r%s: add of existing path %s" \
                                        % (revision, node.path))
                        orig = Trace()
                    else:
                        orig = self.filemap.retrieve(revision, node.path)
                    trace = orig.clone()
                    if perms:
                        trace.set_perms(perms)
                    if node.from_path:
                        if not self.filemap.member_of(node.from_rev,node.from_path):
                            raise Fatal("%s not in source commit %s" % \
                                        (node.from_path, node.from_rev))
                        else:
                            oldtrace = self.filemap.retrieve(node.from_rev,
                                                             node.from_path)
                            trace.set_mark(oldtrace.mark)
                    if node.blob is not None:
                        node.blob.mark = self.__newmark() 
                        self.repo.events.append(node.blob)
                        trace.set_mark(node.blob.mark)
                    if trace != orig:
                        if verbose >= DEBUG_EXTRACT:
                            announce("r%s %s: modified %s -> %s" % (revision, node.path, orig, trace))
                        fileop = FileOp(commit)
                        fileop.construct("M",
                                         trace.perms,
                                         trace.mark,
                                         node.path)
                        commit.fileops.append(fileop)
                    self.filemap.register(revision, node.path, trace)
            # Node loop ends
        commit.common = os.path.commonprefix([node.path for node in record.nodes])
        commit.branch = os.path.join("refs", "heads", "master") + os.sep
        # It's important that this happens after blob creation
        commit.mark = self.__newmark(revision)
        self.parent = revision
        self.repo.events.append(commit)
        # Deduce links between what will later become branches on the
        # basis of copies. This is tricky because a revision can be
        # the target of multiple copies.  Humans don't abuse this
        # because tracking multiple copies is too hard to do in a slow
        # organic brain, but tools like cvs2svn can generate large
        # sets of them. cvs2svn seems to try to copy each file and
        # directory from the commit corresponding to the CVS revision
        # where the file was last changed before the copy, which may
        # be substantially earlier than the CVS revision corresponding
        # to the copy). Fortunately, we can resolve such sets by the
        # simple expedient of picking the *latest* revision in them!
        if revision not in self.branchlink:
            copies = [node for node in record.nodes \
                      if node.from_rev is not None]
            if copies:
                copies.sort(key=lambda node: int(node.from_rev))
                self.branchlink[revision] = copies[-1].from_rev
    def svn_postprocess(self, baton):
        "Perform topological analysis, define branches, lift tags."
        toprev = len(self.revisions)
        countfmt = " %%%dd of %s" % (len(str(toprev)), toprev)
        # First pass: Find all directory copies
        if verbose >= DEBUG_EXTRACT:
            announce("Pass 1")
        baton.startcounter(countfmt)
        baton.twirl("copynodes:")
        copynodes = []
        for revision in self.revisions:
            record = self.revisions[revision]
            for node in record.nodes:
                if node.from_path is not None and node.kind == SD_DIR:
                    copynodes.append(node)
                    if verbose >= DEBUG_EXTRACT:
                        announce("copynode at %s" % node)
            baton.bumpcounter()
            copynodes.sort(key=lambda n: n.from_rev)
        baton.endcounter()
        # Second pass: build filemaps, part 1.
        # Keeping a filemap for each revision gets prohibitively
        # expensive.  So we keep around only those that are identified
        # as directory copy sources.
        if verbose >= DEBUG_EXTRACT:
            announce("Pass 2")
        baton.startcounter(countfmt)
        baton.twirl("filemaps1:")
        filemaps = {}
        source_revisions = [node.from_rev for node in copynodes]
        filemap = set([])
        for revision in self.revisions:
            record = self.revisions[revision]
            for node in record.nodes:
                if node.action == SD_ADD and node.kind == SD_FILE:
                    filemap.add(node.path)
                elif node.action == SD_DELETE:
                    filemap.discard(node.path)
                    node.from_set = set([])
                    for path in copy.copy(filemap):
                        if path.startswith(node.path + os.sep):
                           filemap.add(path)
                    filemap -= node.from_set
            if revision in source_revisions:
                filemaps[revision] = copy.copy(filemap)
            baton.bumpcounter()
        baton.endcounter()
        if verbose >= DEBUG_EXTRACT:
            announce("filemaps %s" % filemaps)
        # Third pass: Build filemaps, part 2.
        # Update each source filemap by incorporating all the
        # copies that have it as a target.
        if verbose >= DEBUG_EXTRACT:
            announce("Pass 3")
        baton.twirl("filemaps2")
        for copynode in copynodes:
            if copynode.revision in source_revisions:
                for path in filemaps[copynode.from_rev]:
                    if path.startswith(copynode.from_path + os.sep):
                        suffix = path[len(copynode.from_path+os.sep):]
                        filemaps[copynode.revision].add(os.path.join(copynode.path, suffix))
            baton.twirl()
        baton.twirl('+')
        # Fourth pass: build from sets in each directory copy record.
        if verbose >= DEBUG_EXTRACT:
            announce("Pass 4")
        baton.twirl("filemaps2")
        for copynode in copynodes:
            copynode.from_set = set([])
            for path in filemaps[copynode.from_rev]:
                if path.startswith(copynode.from_path + os.sep):
                    suffix = path[len(copynode.from_path+os.sep):]
                    copynode.from_set.add(os.path.join(copynode.path, suffix))
            if not copynode.from_set:
                raise Fatal("empty from set for %s" % copynode)
        baton.twirl('+')
        # Fifth pass: build history links
        if verbose >= DEBUG_EXTRACT:
            announce("Pass 5")
        baton.startcounter(countfmt)
        baton.twirl("histories:")
        for revision in self.revisions:
            record = self.revisions[revision]
            for node in record.nodes:
                # if node.props is None, no property section.
                # if node.blob is None, no text section.
                try:
                    assert node.action in (SD_CHANGE, SD_ADD, SD_DELETE, SD_REPLACE)
                    assert node.blob is not None or \
                           node.props is not None or \
                           node.from_rev or \
                           node.action in (SD_ADD, SD_DELETE)
                    assert (node.from_rev is None) == (node.from_path is None)
                    assert node.kind in (SD_NONE, SD_FILE, SD_DIR)
                    assert node.kind != SD_NONE or node.action == SD_DELETE
                    assert node.action in (SD_ADD, SD_REPLACE) or not node.from_rev
                except AssertionError:
                    raise Fatal("forbidden operation in dump stream at r%s: %s" \
                                % (revision, node))
                # Thread this record onto the filetree map.
                # First, special case if the file was a copy target.
                # Note: this can fail if the source was created
                # by a directory copy.
                if node.kind == SD_FILE and node.from_path:
                    for pnode in self.revisions[node.from_rev].nodes:
                        if pnode.path == node.from_path:
                            node.history = pnode
                            break
                else:
                    # Refer it back to the longest prefix we can find
                    segments = node.path.split(os.sep) + ["dummy"]
                    for i in range(1, len(segments)):
                        prefix = os.sep.join(segments[:-i])
                        if prefix in self.histories:
                            node.history = self.histories[prefix]
                            if verbose >= DEBUG_EXTRACT:
                                announce("r%s: relating %s to %s (%s)" \
                                         % (revision, node.path,
                                            node.history.revision, prefix))
                            # Recover some information that may have been
                            # elided due to a header being optional.
                            if node.kind == SD_NONE:
                                node.kind = node.history.kind
                            break
                assert node.kind in (SD_FILE, SD_DIR)
                self.histories[node.path] = node
                # os.sep is appended to avoid collisions with path prefixes.
                if node.kind == SD_DIR:
                    node.path += os.sep
        baton.endcounter()
        # Third pass: build commits
        # This code can eat your processor, so we make it give up
        # its timeslice at reasonable intervals. Needed because
        # it doesn't hit the disk.
        baton.startcounter(countfmt)
        baton.twirl("commits:")
        for revision in self.revisions:
            self.svn_commit(revision)
            baton.bumpcounter()
            # Given up our scheduler slot
            time.sleep(0)
        baton.endcounter()
        if verbose >= DEBUG_EXTRACT:
            announce("at post-parsing time:")
            for commit in self.repo.commits():
                msg = commit.comment
                if msg == None:
                    msg = ""
                announce("r%-4s %4s %2d %2d '%s'" % \
                         (commit.get_fossil(), commit.mark,
                          len(commit.fileops),
                          len(commit.properties),
                          msg.strip()[:20]))
        baton.twirl("+")
        # First, turn the root commit into a tag
        initial = self.repo.commits()[0]
        # There will always be one property, the fossil revision
        if not initial.fileops and len(initial.properties) >= 1:
            if len(self.repo.commits()) >= 2:
                self.repo.tagify(initial,
                                 "root",
                                 self.repo.commits()[1].mark,
                                 "[[Tag from directory creation or copy commit at Subversion r%s]]\n" % initial.get_fossil())
                baton.twirl()
            else:
                complain("could not tagify root commit.")
        baton.twirl("branches")
        # Now, branch analysis.
        if verbose >= DEBUG_EXTRACT:
            announce("Branches: %s" % (self.branches,))
        if self.branches:
            # Don't leave the linear relationships in place
            for commit in self.repo.commits():
                commit.parent_marks = []
            # Instead, determine a branch for each commit...
            for commit in self.repo.commits():
                for branch in self.branches:
                    if commit.common.startswith(branch):
                        commit.branch = branch
                        for fileop in commit.fileops:
                            if fileop.op in ("M", "D"):
                                fileop.path = fileop.path[len(branch):]
                            elif fileop.op in ("R", "C"):
                                fileop.source = fileop.source[len(branch):]
                                fileop.target = fileop.target[len(branch):]
                        break
                else:
                    commit.branch = "root"
                    self.branches["root"] = None
                baton.twirl()
            baton.twirl("+")
            # ...then rebuild parent links so they follow the branches
            branchroots = []
            for commit in self.repo.commits():
                if self.branches[commit.branch] is None:
                    branchroots.append(commit)
                    commit.parent_marks = []
                else:
                    commit.parent_marks = [self.branches[commit.branch]]
                self.branches[commit.branch] = commit.mark
                baton.twirl()
            baton.twirl("+")
            # The root branch is special. It wasn't made by a copy, so
            # we didn't get the information to connect it to trunk in the
            # last phase.
            if "root" in self.branches:
                for commit in self.repo.commits():
                    if commit.branch == "root":
                        break
                self.branchlink[commit.get_fossil()] = self.repo.commits()[0].get_fossil()
            baton.twirl("+")
            # Re-root branches here using the branchlink info
            if verbose >= DEBUG_EXTRACT:
                announce("branch roots: %s, links %s" % ([c.mark for c in branchroots], self.branchlink))
            baton.twirl("+")
            nonempty = set([c for c in self.repo.commits() if c.fileops]) 
            for root in branchroots:
                rootrev = root.get_fossil()
                if rootrev in self.branchlink:
                    root.parent_marks = [self.markmap[self.branchlink[rootrev]]]
                elif root.branch != ("trunk" + os.sep) and commit.branch in nonempty:
                    complain("r%s: can't connect nonempty %s to trunk" \
                             % (rootrev, root.branch))
                baton.twirl()
            baton.twirl("+")
            if verbose >= DEBUG_EXTRACT:
                announce("after branch analysis:")
                for commit in self.repo.commits():
                    if commit.parent_marks:
                        ancestor = commit.parent_marks[0]
                    else:
                        ancestor = '-'
                    announce("r%-4s %4s %4s %2d %2d '%s'" % \
                             (commit.get_fossil(),
                              commit.mark, ancestor,
                              len(commit.fileops),
                              len(commit.properties),
                              commit.branch))
            baton.twirl("tagifying")
            # Tagify normal branch-root commits, they don't carry any
            # information other than their metadata. The exceptions
            # are trunk and root (if the later exists); neither is
            # the result of a normal copy operation.
            for commit in self.repo.commits():
                if commit in branchroots \
                       and commit.branch != ("trunk"+os.sep) \
                       and commit.branch != "root" \
                       and commit.parent_marks \
                       and not commit.fileops:
                    self.repo.tagify(commit,
                                     os.path.basename(commit.branch[:-1]),
                                     commit.parent_marks[0],
                                     "[[Tag from directory creation or copy commit at Subversion r%s]]\n" % commit.get_fossil())
                    baton.twirl()
            baton.twirl("+")
        baton.twirl("polishing")
        # Now pretty up the branch names
        for commit in self.repo.commits():
            if commit.branch == "root":
                commit.branch = os.path.join("refs", "heads", "root")
            elif commit.branch.startswith("tags" + os.sep):
                pass
            elif commit.branch == "trunk" + os.sep:
                commit.branch = os.path.join("refs", "heads", "master")
            else:
                commit.branch = os.path.join("refs", "heads",
                                                 os.path.basename(commit.branch[:-1]))
            baton.twirl()
        baton.twirl("+")
        # Issue resets when required
        baton.twirl("resets")
        save_events = self.repo.events
        self.repo.events = []
        issued = set([])
        for event in save_events:
            if isinstance(event, Commit) \
                   and event.branch != os.path.join("refs","heads","master") \
                   and event.branch not in issued:
                reset = Reset()
                reset.ref = event.branch
                self.repo.events.append(reset)
                issued.add(event.branch)
            self.repo.events.append(event)
            baton.twirl()
        baton.twirl("+")
        baton.twirl("renumbering")
        self.repo.renumber()
        baton.twirl("+")

# Generic repository-manipulation code begins here

class Repository:
    "Generic repository object."
    def __init__(self, name=None):
        self.name = name
        self.readtime = time.time()
        self.vcs = None
        self.sourcedir = None
        self.events = []    # A list of the events encountered, in order
        self.preserve_set = set([])
        self.case_coverage = set([])
        self.basedir = os.getcwd()
        self.write_fossils = False
        self.dollar_map = {}
    def cleanup(self):
        "Release blob files associated with this repo."
        nuke(self.subdir(), "reposurgeon: cleaning up %s" % self.subdir())
    def subdir(self, name=None):
        if name is None:
            name = self.name
        if not name:
            return os.path.join(self.basedir, ".rs" + repr(os.getpid()))
        else:
            return os.path.join(self.basedir, ".rs" + repr(os.getpid())+ "-" + name) 
    def makedir(self):
        try:
            if verbose >= DEBUG_SHUFFLE:
                announce("repository fast import creates " + self.subdir())
            os.mkdir(self.subdir())
        except OSError:
            raise Fatal("can't create operating directory")
    def size(self):
        "Return the size of this import stream, for statistics display."
        return sum([len(str(e)) for e in self.events])
    def branchlist(self):
        "Return a list of branchnames in this repo."
        lst = []
        for commit in self.commits():
            if commit.branch not in lst:
                lst.append(commit.branch)
        return lst
    def find(self, mark):
        "Find an object index by mark"
        for (ind, event) in enumerate(self.events):
            if hasattr(event, "mark") and mark == event.mark:
                return ind
        return None
    def objfind(self, mark):
        "Find an object by mark"
        for (ind, event) in enumerate(self.events):
            if hasattr(event, "mark") and mark == event.mark:
                return self.events[ind]
        return None
    def read_authormap(self, selection, fp):
        "Read an author-mapping file and apply it to the repo."
        authormap = {}
        try:
            for line in fp:
                line = line.strip()
                if not line:
                    continue
                (local, netwide) = line.strip().split('=')
                (name, mail) = email.utils.parseaddr(netwide.strip())
                if not mail:
                    raise Fatal("can't recognize address in '%s'" % netwide)
                authormap[local.strip()] = (name, mail)
        except IOError:
            raise Recoverable("couldn't open author-map file")
        except ValueError:
            raise Recoverable("bad author map syntax")
        for ei in selection:
            event = self.events[ei]
            if isinstance(event, Commit):
                event.committer.remap(authormap)
                for author in event.authors:
                    author.remap(authormap)
            elif isinstance(event, Tag):
                event.tagger.remap(authormap)
    def write_authormap(self, selection, fp):
        "List the identifiers we need."
        contributors = set([])
        for ei in selection:
            event = self.events[ei]
            if isinstance(event, Commit):
                contributors.add(event.committer.who())
                for author in event.authors:
                    contributors.add(author.who())
            elif isinstance(event, Tag):
                contributors.add(event.tagger.who())
        for contributor in contributors:
            fp.write(contributor + "\n")
    def read_fossilmap(self, fp):
        "Read a fossil-references dump and initialize the repo's fossil map."
        mark_map = {}
        for event in self.commits():
            mark_map[(event.committer.date.timestamp, event.committer.email)] = event
        try:
            for line in fp:
                (fossil, timefield, person) = line.split()
                when_who = (Date(timefield).timestamp, person)
                if when_who in mark_map:
                    mark_map[when_who].set_fossil(fossil)
                else:
                    complain("no mark matches fossil %s" % fossil)
            del mark_map
        except ValueError:
            raise Recoverable("bad syntax in fossils file.")
    def write_fossilmap(self, fp):
        "Dump fossil references."
        for commit in self.commits():
            if "fossil" in commit.properties:
                fp.write("%s\t%s\t%s\n" % (commit.properties["fossil"],
                                           commit.committer.date.rfc3339(),
                                           commit.committer.email))
    def tagify(self, commit, name, committish, legend=""):
        "Turn a commit into a tag."
        if verbose >= DEBUG_EXTRACT:
            announce("tagifying: %s" % commit.mark)
        if commit.fileops:
            raise Fatal("Attempting to tagify a commit with fileops.")
        if not commit.comment:
            pref = ""
        else:
            pref = commit.comment + "\n"
        self.events.append(Tag(name=name,
                               committish=committish,
                               tagger=commit.committer,
                               comment=pref + legend))
        self.delete([self.events.index(commit)],
                         ["obliterate"],
                         quiet=True)
    def fast_import(self, fp, progress=False):
        "Read a stream file and use it to populate the repo."
        StreamParser(self).fast_import(fp, progress)
        self.readtime = time.time()
    def parse_dollar_cookies(self):
        "Extract info about fossil references from CVS/SVN header cookies."
        if self.dollar_map:
            return
        # The goal here is to throw away CVS and Subversion header
        # information still fossilized into $Id$ and $Subversion$
        # headers after conversion to a later version. For each
        # cookie, all but the earliest blob containing it has it
        # as a fossil which should be removed.  Then, the earliest
        # commit referencing that blob gets a fossil property set;
        # later references will be branching artifacts.
        seen = set([])
        for event in self.events:
            if isinstance(event, Blob) and event.cookie:
                if event.cookie in seen:
                    continue
                else:
                    # The first commit immediately after this blob
                    for ei in range(self.find(event.mark), len(self.events)):
                        if isinstance(self.events[ei], Commit):
                            commit = self.events[ei]
                            break
                    seen.add(event.cookie)
                    if "fossil" in commit.properties:
                        complain("fossil property of %s overwritten" \
                                 % commit.mark)
                    if type(event.cookie) == type(""):
                        svnkey = "SVN:" + event.cookie
                        self.dollar_map[svnkey] = commit
                    else:
                        (basename, cvsref) = event.cookie
                        for fileop in commit.fileops:
                            if fileop.op == 'M' and fileop.ref == event.mark:
                                if not os.path.basename(fileop.path).endswith(basename):
                                    # Usually the harmless result of a
                                    # file move or copy that cvs2svn or
                                    # git-svn didn't pick up on.
                                    complain("mismatched CVS header path '%s' in %s vs '%s' in %s"
                                             % (fileop.path, commit.mark, basename, event.mark))
                                cvskey = "CVS:%s:%s" % (fileop.path, cvsref)
                                self.dollar_map[cvskey] = commit
    def export_style(self):
        "How should we tune the export dump format?"
        if self.vcs:
            return self.vcs.styleflags
        else:
            # Default to git style
            return ("nl-after-commit",)
    def fast_export(self, selection, fp, progress=False):
        "Dump the repo object in fast-export format."
        with Baton("reposurgeon: exporting", enable=progress) as baton:
            try:
                for ei in selection:
                    baton.twirl()
                    event = self.events[ei]
                    if verbose >= DEBUG_MERGE:
                        if hasattr(event, "mark"):
                            announce("writing %d %s %s" % (ei, event.mark, event.__class__.__name__))
                    fp.write(str(event))
            except IOError as e:
                raise Fatal("export error: %s" % e)
    def preserve(self, filename):
        "Add a path to the preserve set, to be copied back on rebuild."
        if os.path.exists(filename):
            self.preserve_set.add(filename)
        else:
            self.error("%s doesn't exist" % filename, atline=False)
    def unpreserve(self, filename):
        "Remove a path from the preserve set."
        if filename in self.preserve_set:
            self.preserve_set.remove(filename)
        else:
            self.error("%s doesn't exist" % filename, atline=False)
    def preservable(self):
        "Return the repo's preserve set."
        return self.preserve_set
    def rename(self, newname):
        "Rename the repo."
        try:
            # Can fail if the target directory exists.
            if verbose >= DEBUG_SHUFFLE:
                announce("repository rename %s->%s calls os.rename(%s, %s)" % (self.name, newname, repr(self.subdir()), repr(self.subdir(newname))))
            os.rename(self.subdir(), self.subdir(newname))
            self.name = newname
        except OSError as e:
            raise Fatal("repo rename %s -> %s failed: %s"
                                       % (self.subdir(), self.subdir(newname), e))
    def commits(self):
        "Return a list of the repository commit objects."
        return [e for e in self.events if isinstance(e, Commit)]
    def earliest(self):
        "Return the date of earliest commit."
        return self.commits()[0].committer.date
    #
    # Delete machinery begins here
    #
    def __ancestor_count(self, event, path):
        "Count modifications of a path in this commit and its ancestry chain."
        count = 0
        for fileop in event.fileops:
            if fileop.op == "M" and fileop.path == path:
                count = 1
                break
        return count + sum([self.__ancestor_count(c, path) for c in event.parents()])
    def __compose(self, event, left, right):
        "Compose two relevant fileops."
        # Here's what the fields in the return value mean:
        # 0: Was this a modification
        # 1: Op to replace the first with (None means delete)
        # 2: Op to replace the second with (None means delete)
        # 3: If not None, a warning to emit
        # 4: Case number, for coverage analysis
        pair = (left.op, right.op)
        #
        # First op M
        #
        if pair == ("M", "M"):
            # Leave these in place, they get handled later.
            return (False, left, right, None, 0)
        # M a + D a -> D a
        # Or, could reduce to nothing if M a was the only modify..
        elif left.op == "M" and right.op in "D":
            if self.__ancestor_count(event, left.path) == 1:
                return (True, None, None, None, 1)
            else:
                return (True, right, None, None, 2)
        elif left.op == "M" and right.op == "R":
            # M a + R a b -> R a b M b, so R falls towards start of list
            if left.path == right.source:
                if self.__ancestor_count(event, left.path) == 1:
                    # M a has no ancestors, preceding R can be dropped
                    left.path = right.target
                    return (True, left, None, None, 3)
                else:
                    # M a has ancestors, R is still needed
                    left.path = right.target
                    return (True, right, left, None, 4)
            # M b + R a b can't happen.  If you try to generate this with
            # git mv it throws an error.  An ordinary mv results in D b M a.
            elif left.path == right.target:
                return(True, right, None, "M followed by R to the M operand?", -1)
        # Correct reduction for this would be M a + C a b -> C a b + M a + M b,
        # that is we'd have to duplicate the modify. We'll leave it in place
        # for now.
        elif left.op == "M" and right.op == "C":
            return (False, left, right, None, 5)
        #
        # First op D or deleteall
        #
        # Delete followed by modify undoes delete, since M carries whole files. 
        elif pair == ("D", "M"):
            return (True, right, None, None, 6)
        # But we have to leave deletealls in place, since they affect right ops
        elif pair == ("deleteall", "M"):
            return (False, left, right, None, 7)
        # These cases should be impossible
        elif left.op == "deleteall" and right.op != "M":
            return (False, left, right,
                    "Non-M operation after deleteall?", -1)
        if left.op == "D" and right.op == "D":
            return (False, left, right, "Two Ds of %s?" % left.path, -2)
        if left.op == "D" and right.op in ("R", "C"):
            if left.path == right.source:
                return (False, left, right,
                        "R or C of %s after deletion?" % left.path, -3)
            else:
                return (False, left, right, None, 8)
        #
        # First op R
        #
        elif pair == ("R", "D"):
            if left.target == right.path:
                # Rename followed by delete of target composes to nothing
                return (True, None, None, None, 9)
            else:
                # On followed by delete of source discard the delete
                # but user should be warned. 
                return (False, left, None,
                        "delete of %s after renaming to %s?" % (right.path, left.source), -4)
        # Rename followed by deleteall shouldn't be possible
        elif pair == ("R", "deleteall") and left.target == right.path:
            return (False, None, right,
                    "rename before deleteall not removed?", -5)
        # Leave rename or copy followed by modify alone
        elif pair == ("R", "M") or pair == ("C", "M"):
            return (False, left, right, None, 10)
        # Compose renames where possible
        elif left.op == "R" and right.op == "R":
            if left.target == right.source:
                left.target = right.target
                return (True, left, None, None, 11)
            else:
                return (False, left, right,
                        "R %s %s is inconsistent with following operation" \
                        % (left.source, left.target), -6)
        # We could do R a b + C b c -> C a c + R a b, but why?
        if left.op == "R" and right.op == "C":
            return (False, left, right, None, 12)
        #
        # First op C
        #
        elif pair == ("C", "D"):
            if left.source == right.path:
                # Copy followed by delete of the source is a rename.
                left.op = "R"
                return (True, left, None, None, 13)
            elif left.target == right.path:
                # This delete undoes the copy
                return (True, None, None, None, 14)
        elif pair == ("C", "R"):
            if left.source == right.source:
                # No reduction
                return (False, left, right, None, 15)
            else:
                # Copy followed by a rename of the target reduces to single copy
                if left.target == right.source:
                    left.target = right.target
                    return (True, left, None, None, 16)
        elif pair == ("C", "C"):
            # No reduction
            return (False, left, right, None, 17)
        #
        # Case not covered
        #
        raise Fatal("can't compose op '%s' and '%s'" % (left, right))
    def canonicalize(self, ei):
        "Canonicalize the list of file operations in this commit."
        coverage = set([])
        commit = self.events[ei]
        # Handling deleteall operations is simple
        lastdeleteall = None
        for (i, a) in enumerate(commit.fileops):
            if a.op == "deleteall":
                lastdeleteall = i
        if lastdeleteall is not None:
            if verbose >= DEBUG_DELETE:
                announce("removing all before rightmost deleteall")
            commit.fileops = commit.fileops[lastdeleteall:]
        # Composition in the general case is trickier.
        while True:
            # Keep making passes until nothing mutates
            mutated = False
            for (i, a) in enumerate(commit.fileops):
                if a is None:
                    continue
                for (j, b) in enumerate(commit.fileops):
                    if b is None:
                        continue
                    if i < j and a.relevant(b):
                        (modified, newa, newb, warn, case) = self.__compose(commit, a, b)
                        if modified:
                            if verbose >= DEBUG_DELETE:
                                announce("Reduction case %d fired on %s" % (case, (i,j)))
                            mutated = True
                            commit.fileops[i] = newa
                            commit.fileops[j] = newb
                            if verbose >= DEBUG_DELETE:
                                announce("During canonicalization:")
                                commit.fileop_dump(ei)
                            if warn:
                                self.warn(warn, atline=False)
                            coverage.add(case)
            if not mutated:
                break
        commit.fileops = [x for x in commit.fileops if x is not None]
        return coverage
    def delete(self, selected, policy, quiet=False):
        "Delete commits, handling multiple Ms on a file with specified policy"
        # Make sure we do deletions from greatest commit number to least
        selected = copy.copy(selected)
        selected.sort(reverse=True)
        if verbose >= DEBUG_DELETE:
            announce("Deletion list is %s" % [x+1 for x in selected])
        # Sanity checks
        for ei in selected:
            event = self.events[ei]
            if isinstance(event, Blob):
                raise Recoverable("attempt to directly delete blob %d" % (ei+1))
            elif  isinstance(event, Commit):
                if "obliterate" in policy and not quiet:
                    speak = "warning: commit %s to be obliterated has " % event.mark 
                    if '/' in event.branch and not '/heads/' in event.branch:
                        complain(speak + "non-head branch attribute %s" % event.branch)
                    for fileop in event.fileops:
                        if fileop.op != 'D':
                            announce(speak + "non-delete fileops.")
                            break
        # Here are the deletions
        for ei in selected:
            event = self.events[ei]
            if event.__class__ in (Reset, Tag, Passthrough):
                self.events.pop(ei)
            elif isinstance(event, Commit):
                if "/tags/" in event.branch:
                    identical = False
                    if "tagback" in policy:
                        if event.parents():
                            identical = event.parents()[0].branch == event.branch
                            if not identical:
                                event.parents()[0].branch = event.branch
                    elif "tagforward" in policy:
                        if event.children():
                            identical = event.children()[0].branch == event.branch
                            if not identical:
                                event.children()[0].branch = event.branch
                    else:
                        if "pushback" in policy:
                            if event.parents():
                                identical = event.parents()[0].branch == event.branch
                        else:
                            if event.children():
                                identical = event.children()[0].branch == event.branch        
                        if not identical:
                            complain("tag %s on event %s will be lost" % (event.branch, event.mark))
                # Reparent each child
                for child in event.children():
                    child.parent_marks.remove(event.mark)
                    child.parent_marks += event.parent_marks
                    if "obliterate" not in policy and "pushback" not in policy:
                        # Prepend a copy of this event's file ops to
                        # each child's list and mark the child as
                        # needing resolution.
                        for fileop in event.fileops:
                            fileop.commit = child
                        child.fileops = copy.copy(event.fileops) + child.fileops
                        child.pushed_to = True
                # We might be trying to hand the event's fileops to parents.
                if "pushback" in policy:
                    for parent in event.parents():
                        # Append a copy of this event's file ops to
                        # each parent's list and mark the parent as needing
                        # resolution.
                        for fileop in event.fileops:
                            fileop.commit = parent
                            # On a pushback (but not a push forward)
                            # we might have moved the fileop so it's
                            # now referred to before its actual
                            # definition.  This will cause a fatal
                            # error "mark not defined" on import.
                            if fileop.op == 'M':
                                swapblob = self.find(fileop.ref)
                                swapcommit = self.find(parent.mark)
                                if swapblob > swapcommit:
                                    #print "Uh oh!", swapcommit, swapblob
                                    saveblob = self.events[swapblob]
                                    for i in range(swapblob-swapcommit):
                                        countdown = swapblob-i
                                        #print "Moving %d to %d" % (countdown-1, countdown)
                                        self.events[countdown] = self.events[countdown-1]
                                    #print "Moving %d to %d" % (swapblob, swapcommit)
                                    self.events[swapcommit] = saveblob
                        parent.fileops += copy.copy(event.fileops)
                        parent.pushed_to = True
                if "tagback" not in policy and "tagforward" not in policy:
                    self.events = [t for t in self.events if not (isinstance(t, Tag)
                                                        and t.committish == event.mark)]
                elif "tagforward" in policy:
                    for t in self.events:
                        if isinstance(t, Tag) and t.committish == event.mark:
                            t.committish = event.children()[0].mark
                elif "tagback" in policy:
                    for t in self.events:
                        if isinstance(t, Tag) and t.committish == event.mark:
                            t.committish = event.parents()[0].mark
                # And remove the deleted event
                self.events.pop(ei)
            else:
                raise Fatal("unexpected object in event array")
        # Canonicalize all the commits that got ops pushed to them
        if "obliterate" not in policy:
            for (ei, event) in enumerate(self.events):
                if not isinstance(event, Commit):
                    continue
                elif event.pushed_to:
                    if verbose >= DEBUG_DELETE:
                        announce("Before canonicalization:")
                        event.fileop_dump(ei)
                    self.case_coverage |= self.canonicalize(ei)
                    if verbose >= DEBUG_DELETE:
                        announce("After canonicalization:")
                        event.fileop_dump(ei)
                    # Now apply policy in the mutiple-M case
                    for (path, oplist) in list(event.cliques().items()):
                        if len(oplist) == 1:
                            continue
                        if ("coalesce" not in policy and "obliterate" not in policy) or verbose >= DEBUG_DELETE:
                            complain("commit %s has multiple Ms for %s" % (event.mark, path))
                        if "coalesce" in policy:
                            # Remove all but the last M.
                            while len(oplist) > 1:
                                event.fileops.pop(oplist.pop(0))
                        if verbose >= DEBUG_DELETE:
                            print("Commit %d, after applying policy:" % (ei +1,))
                            for op in event.fileops:
                                print(str(op))
        # Clear everybody's problem flag
        for commit in self.commits():
            commit.pushed_to = False
    #
    # Delete machinery ends here
    #
    def front_events(self):
        "Return options, features."
        return [e for e in self.events \
                if isinstance(e, Passthrough) \
                and (e.text.startswith("option") or e.text.startswith("feature"))]
    def renumber(self):
        "Renumber the marks in a repo."
        marklist = []
        def remark(m):
            try:
                return ":" + repr(1 + marklist.index(m))
            except ValueError:
                raise Fatal("unknown mark %s cannot be renumbered!" % m)
        for event in self.events:
            if hasattr(event, "mark"):
                if event.mark is None:
                    continue
                elif not event.mark.startswith(":"):
                    raise Fatal("field not in mark format")
                else:
                    marklist.append(event.mark)
        for event in self.events:
            for fld in ("mark", "committish"):
                if hasattr(event, fld) and getattr(event, fld):
                    old = getattr(event, fld)
                    new = remark(old)
                    if verbose >= DEBUG_MERGE:
                        announce("renumbering %s -> %s in %s.%s" % (old, new,
                                                                    event.__class__.__name__,
                                                                    fld))
                    setattr(event, fld, new)
            if isinstance(event, Commit):
                for (i, old) in enumerate(event.parent_marks):
                    new = remark(old)
                    if verbose >= DEBUG_MERGE:
                        announce("renumbering %s -> %s in parents" % (old, new))
                    event.parent_marks[i] = new
                for fileop in event.fileops:
                    if fileop.op == "M" and fileop.ref.startswith(":"):
                        new = remark(fileop.ref)
                        if verbose >= DEBUG_MERGE:
                            announce("renumbering %s -> %s in fileop" % (fileop.ref, new))
                        fileop.ref = new
    def uniquify(self, color):
        "Disambiguate branches, tags, and marks using the specified label."
        for event in self.events:
            # Disambiguate all tags.
            for (objtype, attr) in ((Tag, "name"),):
                if isinstance(event, objtype):
                    setattr(event, attr, color + "-" + getattr(event, attr))
            # Disambiguate all branches and refs.
            for (objtype, attr) in ((Commit, "branch"),
                                 (Reset, "ref")):
                if isinstance(event, objtype):
                    old = getattr(event, attr)
                    new = old + "-" + color
                    if verbose >= DEBUG_MERGE:
                        announce("moving %s -> %s in %s.%s"
                                 % (old, new,
                                    objtype.__name__,
                                    attr))
                    setattr(event, attr, new)
            # Disambiguate defining marks.
            for fld in ("mark", "committish"):
                if hasattr(event, fld):
                    old = getattr(event, fld)
                    if old is None:
                        continue
                    elif not old.startswith(":"):
                        raise Fatal("field not in mark format")
                    else:
                        new = old + "-" + color
                        if verbose >= DEBUG_MERGE:
                            announce("moving %s -> %s in %s.%s"
                                     % (old, new,
                                        event.__class__.__name__,
                                        fld))
                        setattr(event, fld, new)
            # Now marks in fileops
            if isinstance(event, Commit):
                for (j, old) in enumerate(event.parent_marks):
                    new = old + "-" + color
                    if verbose >= DEBUG_MERGE:
                        announce("moving %s -> %s in parents" % (old, new))
                    event.parent_marks[j] = new
                for fileop in event.fileops:
                    if fileop.op == "M" and fileop.ref.startswith(":"):
                        new = fileop.ref + "-" + color
                        if verbose >= DEBUG_MERGE:
                            announce("moving %s -> %s in fileop"
                                     % (fileop.ref, new))
                        fileop.ref = new
        return
    def absorb(self, other):
        # Only vcstype, sourcedir, and basedir are not copied here
        self.preserve_set |= other.preserve_set
        self.case_coverage |= other.case_coverage
        # Strip feature events off the front, they have to stay in front.
        while isinstance(other[0], Passthrough):
            front = [x for x in self.events if isinstance(x, Passthrough)]
            self.events.insert(len(front), other.events.pop(0))
        # Merge in the non-feature events and blobs
        self.events += other.events
        # Transplant in fileops, blobs, and other impedimenta
        for event in other:
            if hasattr(event, "moveto"):
                event.moveto(self)
        other.events = []
        other.cleanup()
        #del other
    def graft(self, graft_repo, graft_point):
        "Graft a repo on to this one at a specified point."
        where = self.events[graft_point]
        if not isinstance(where, Commit):
            raise Recoverable("%s in %s is not a commit." % \
                              (where.mark, self.name))
        # Errors aren't recoverable after this
        graft_repo.uniquify(graft_repo.name)
        graft_repo.commits()[0].parent_marks.append(where.mark)
        self.absorb(graft_repo)
        self.renumber()
    def __last_modification(self, commit, path):
        "Locate the last modification of the specified path before this commit."
        ancestors = commit.parents()
        while ancestors:
            backto = []
            for ancestor in ancestors:
                # This is potential trouble if the file was renamed
                # down one side of a merge bubble but not the other.
                # Might cause an internal-error message, but no real
                # harm will be done.
                for (i, fileop) in enumerate(ancestor.fileops):
                    if fileop.op == 'R' and fileop.target == path:
                        path = fileop.source
                    elif fileop.op == 'M' and fileop.path == path:
                        return (ancestor, i)
                else:
                    backto += ancestor.parents()
            ancestors = backto
        return None
    def move_to_rename(self):
        "Make rename sequenced from matched delete-modify pairs."
        rename_count = 0
        for commit in self.commits():
            renames = []
            for (d, op) in enumerate(commit.fileops):
                if op.op == 'D':
                    previous = self.__last_modification(commit, op.path)
                    if not previous:
                        raise Recoverable("internal error looking for renames of %s" % op.path)
                    else:
                        (ancestor, i) = previous
                        for (m, op2) in enumerate(commit.fileops):
                            if op2.op == 'M' and \
                               ancestor.fileops[i].mode == op2.mode and \
                               ancestor.fileops[i].ref == op2.ref:
                                renames.append((d, m))
                                rename_count += 1
                                break
            for (d, m) in renames:
                commit.fileops[d].source = commit.fileops[d].path
                commit.fileops[d].target = commit.fileops[m].path
                del commit.fileops[d].path
                commit.fileops[d].op = 'R'
                commit.fileops.pop(m)
        return rename_count
    def path_walk(self, selection, hook=lambda path: path):
        "Apply a hook to all paths, returning the set of modified paths."
        modified = set([])
        for ei in selection:
            event = self.events[ei]
            if isinstance(event, Commit):
                for fileop in event.fileops:
                    if fileop.op in ("M", "D"):
                        newpath = hook(fileop.path)
                        if newpath != fileop.path:
                            modified.add(newpath)
                        fileop.path = newpath
                    elif fileop.op in ("R", "C"):
                        newpath = hook(fileop.source)
                        if newpath != fileop.source:
                            modified.add(newpath)
                        fileop.source = newpath
                        newpath = hook(fileop.target)
                        if newpath != fileop.target:
                            modified.add(newpath)
                        fileop.target = newpath
        modified = list(modified)
        modified.sort()
        return modified

    # Container emulation methods
    def __len__(self):
        return len(self.events)
    def __getitem__(self, i):
        return self.events[i]
    def __setitem__(self, i, v):
        self.events[i] = v

def read_repo(source, preferred):
    "Read a repository using fast-import."
    if source == '-':
        repo = Repository()
        repo.fast_import(sys.stdin, progress=verbose==1)
    elif not os.path.exists(source):
        raise Recoverable("%s does not exist" % source)
    elif not os.path.isdir(source):
        repo = Repository()
        repo.fast_import(open(source), progress=verbose==1)
    else:
        if verbose >= DEBUG_SHUFFLE:
            if preferred:
                announce("looking for a %s repo..." % preferred.name)
            else:
                announce("reposurgeon: looking for any repo at %s..." % \
                         os.path.abspath(source))
        hitcount = 0
        extractor = vcs = None
        for possible in vcstypes:
            if preferred and possible.name != preferred.name:
                continue
            subdir = os.path.join(source, possible.subdirectory)
            if os.path.exists(subdir) and os.path.isdir(subdir):
                vcs = possible
                hitcount += 1
        for possible in extractors:
            if preferred and possible.name != preferred.name:
                continue
            subdir = os.path.join(source, possible.subdirectory)
            if os.path.exists(subdir) and os.path.isdir(subdir):
                extractor = possible
                hitcount += 1
        if hitcount == 0:
            raise Recoverable("couldn't find a repo under %s" % os.path.relpath(source))
        elif hitcount > 1:
            raise Recoverable("too many repos under %s" % os.path.relpath(source))
        elif verbose > 0:
            announce("found %s repository" % getattr(vcs or extractor, "name"))
        repo = Repository()
        repo.sourcedir = source
        if vcs:
            repo.vcs = vcs
            repo.preserve_set = vcs.preserve
            showprogress = (verbose > 0) and not "export-progress" in repo.export_style()
            context = {"basename": os.path.basename(repo.sourcedir)}
        try:
            here = os.getcwd()
            os.chdir(repo.sourcedir)
            # We found a matching VCS type
            if vcs:
                if "%(tempfile)s" in repo.vcs.exporter:
                    try:
                        (tfdesc, tfname) = tempfile.mkstemp()
                        assert tfdesc > -1    # pacify pylint
                        context["tempfile"] = tfname
                        do_or_die(repo.vcs.exporter % context, "repository export")
                        with open(tfname) as tp:
                            repo.fast_import(tp, progress=showprogress)
                    finally:
                        os.remove(tfname)
                else:
                    with popen_or_die(repo.vcs.exporter % context, "repository export") as tp:
                        repo.fast_import(tp, progress=showprogress)
                if repo.vcs.authormap and os.path.exists(repo.vcs.authormap):
                    announce("reading author map.")
                    with open(repo.vcs.authormap) as fp:
                        repo.read_authormap(range(len(repo.events)),fp)
                fossils = os.path.join(vcs.subdirectory, "fossils")
                if os.path.exists(fossils):
                    with open(fossils) as rfp:
                        repo.read_fossilmap(rfp)
                # kluge: git-specific hook
                if repo.vcs.name == "git":
                    if os.path.exists(".git/cvs-revisions"):
                        announce("reading cvs-revisions map.")
                        pathrev_to_hash = {}
                        # Pass 1: Get git's path/revision to hash mapping
                        for line in open(".git/cvs-revisions"):
                            (path, rev, hashv) = line.split()
                            pathrev_to_hash[(path, rev)] = hashv
                        # Pass 2: get git's hash to (time,person) mapping 
                        hash_to_action = {}
                        stamp_set = set({})
                        with popen_or_die("git log --all --format='%H %ct %ce'", "r") as fp:
                            for line in fp:
                                (hashv, ctime, cperson) = line.split()
                                stamp = (int(ctime), cperson)
                                if stamp in stamp_set:
                                    complain("more than one commit matches %s!%s (%s)" \
                                             % (rfc3339(int(ctime)), cperson, hashv))
                                    if stamp in hash_to_action:
                                        del hash_to_action[hashv]
                                else:
                                    hash_to_action[hashv] = stamp
                                    stamp_set.add(stamp)
                            # Pass 3: build a (time,person) to commit mapping 
                            action_to_mark = {}
                            for commit in repo.commits():
                                action_to_mark[(commit.committer.date.timestamp, commit.committer.email)] = commit
                            # Pass 4: use it to set commit properties
                            for ((path, rev), value) in pathrev_to_hash.items():
                                if value in hash_to_action:
                                    (ctime, cperson) = hash_to_action[value]
                                    action_to_mark[(ctime, cperson)].set_fossil("CVS:%s:%s" % (path, rev))
                            del pathrev_to_hash
                            del hash_to_action
                            del stamp_set
            # We found a matching custom extractor
            if extractor:
                streamer = RepoStreamer(extractor)
                streamer.extract(repo, progress=verbose>0)
        finally:
            os.chdir(here)
    return repo

class CriticalRegion:
    "Encapsulate operations to try and make us un-interruptible."
    # This number is magic. Python sets a much higher signal.NSIG
    # value, but under Linux the signal calls start to trigger
    # runtime errors at this value and above.
    NSIG = 32
    def __init__(self):
        self.handlers = None	# Pacifies pylint
    def __enter__(self):
        "Begin critical region."
        if verbose >= DEBUG_COMMANDS:
            complain("critical region begins...")
        # Alas that we lack sigblock support
        self.handlers = [None]*(CriticalRegion.NSIG+1)
        for sig in range(1, CriticalRegion.NSIG):
            if not sig in (signal.SIGKILL, signal.SIGSTOP):
                self.handlers[sig] = signal.signal(sig, signal.SIG_IGN)
    def __exit__(self, extype_unused, value_unused, traceback_unused):
        "End critical region."
        for sig in range(1, CriticalRegion.NSIG):
            if not sig in (signal.SIGKILL, signal.SIGSTOP):
                signal.signal(sig, self.handlers[sig])
        if verbose >= DEBUG_COMMANDS:
            complain("critical region ends.")
        return False

def rebuild_repo(repo, target, preferred):
    "Rebuild a repository from the captured state."
    if not target and repo.sourcedir:
        target = repo.sourcedir
    if target:
        target = os.path.abspath(target)
    else:
        raise Recoverable("no default destination for rebuild")
    vcs = preferred or repo.vcs
    if not vcs:
        raise Recoverable("please prefer a repo type first")
    if not hasattr(vcs, "exporter") or vcs.importer is None:
        raise Recoverable("%s repositories are supported for read only." \
                          % preferred.name)

    # Create a new empty directory to do the rebuild in
    if not os.path.exists(target):
        staging = target
        try:
            os.mkdir(target)
        except OSError:
            raise Recoverable("target directory creation failed")
    else:
        staging = target + "-stage" + str(os.getpid())
        assert(os.path.isabs(target) and os.path.isabs(staging))
        try:
            os.mkdir(staging)
        except OSError:
            raise Recoverable("staging directory creation failed")

    # Try the rebuild in the empty staging directory 
    here = os.getcwd()
    try:
        os.chdir(staging)
        if vcs.initializer:
            do_or_die(vcs.initializer, "repository initialization")
        parameters = {"basename": os.path.basename(target)}
        if "%(tempfile)s" in vcs.importer:
            try:
                (tfdesc, tfname) = tempfile.mkstemp()
                assert tfdesc > -1    # pacify pylint
                with open(tfname, "w") as tp:
                    repo.fast_export(list(range(len(repo))), tp, progress=verbose>0)
                do_or_die(vcs.exporter % parameters, "import")
            finally:
                os.remove(tfname)
        else:
            with popen_or_die(vcs.importer % parameters, "import", mode="w") as tp:
                repo.fast_export(list(range(len(repo))), tp, progress=verbose>0)
        if repo.write_fossils:
            with open(os.path.join(vcs.subdirectory, "fossils"), "w") as wfp:
                repo.write_fossilmap(wfp)
        do_or_die(vcs.checkout, "repository_checkout")
        if verbose:
            announce("rebuild is complete.")

        os.chdir(here)
        # Rebuild succeeded - make an empty backup directory
        backupcount = 1
        while True:
            savedir = target + (".~%d~" % backupcount)
            if os.path.exists(savedir):
                backupcount += 1
            else:
                break
        os.mkdir(savedir)
        assert(os.path.abspath(savedir))

        if staging != target:
            if not repo.preserve_set:
                if verbose:
                    announce("no preservations.")
            else:
                preserve = repo.preserve_set.copy()
                if vcs and vcs.lister:
                    def fileset(exclude):
                        allfiles = []
                        for root, dirs, files in os.walk("."):
                            allfiles += [os.path.join(root, name)[2:] for name in files]
                            for exdir in exclude:
                                if exdir in dirs:
                                    dirs.remove(exdir)
                        return set(allfiles)
                    with popen_or_die(vcs.lister) as fp:
                        repofiles = set(fp.read().split())
                    allfiles = fileset(exclude=[vcs.subdirectory]+glob.glob(".rs*"))
                    preserve = allfiles - repofiles
            # This is a critical region.  Ignore all signals until we're done.
            with CriticalRegion():
                # Move the unmodified repo contents in target to the
                # backup directory.  Then move the staging contents to the
                # target directory.  Finally, restore designated files
                # from backup to target.
                for sub in os.listdir(target):
                    os.rename(os.path.join(target, sub),
                              os.path.join(savedir, sub))
                if verbose:
                    announce("repo backed up to %s." % os.path.relpath(savedir))
                for sub in os.listdir(staging):
                    os.rename(os.path.join(staging, sub),
                              os.path.join(target, sub))
                if verbose:
                    announce("modified repo moved to %s." % os.path.relpath(target))
                    for sub in preserve:
                        src = os.path.join(savedir, sub)
                        dst = os.path.join(target, sub)
                        if os.path.exists(src):
                            if os.path.isdir(src):
                                shutil.copytree(src, dst)
                            else:
                                shutil.copy2(src, dst)
                    if verbose:
                        announce("preserved files restored.")
    finally:
        os.chdir(here)
        if staging != target:
            nuke(staging, "reposurgeon: removing staging directory")

def do_or_die(dcmd, legend=""):
    "Either execute a command or raise a fatal exception."
    if legend:
        legend = " "  + legend
    if verbose >= DEBUG_COMMANDS:
        announce("executing '%s'%s" % (dcmd, legend))
    try:
        retcode = subprocess.call(dcmd, shell=True)
        if retcode < 0:
            raise Fatal("child was terminated by signal %d." % -retcode)
        elif retcode != 0:
            raise Fatal("child returned %d." % retcode)
    except (OSError, IOError) as e:
        raise Fatal("execution of %s%s failed: %s" % (dcmd, legend, e))

class popen_or_die:
    "Read or write from a subordinate process."
    def __init__(self, command, legend="", mode="r"):
        assert mode in ("r", "w")
        self.command = command
        self.legend = legend
        self.mode = mode
        if self.legend:
            self.legend = " "  + self.legend
        self.fp = None
    def __enter__(self):
        if verbose >= DEBUG_COMMANDS:
            if self.mode == "r":
                announce("%s: reading from '%s'%s" % (rfc3339(time.time()), self.command, self.legend))
            else:
                announce("%s: writing to '%s'%s" % (rfc3339(time.time()), self.command, self.legend))
        try:
            self.fp = os.popen(self.command, self.mode)
            return self.fp
        except (OSError, IOError) as oe:
            raise Fatal("execution of %s%s failed: %s" \
                                 % (self.command, self.legend, oe))
    def __exit__(self, extype, value, traceback):
        if extype:
            if verbose:
                complain("fatal exception in popen_or_die.")
            # This is what we want, but it's only in true Python 3.x
            if sys.version_info.major >= 3:
                raise extype(value).with_traceback(traceback)
            else:
                try:
                    # Python 3.2 chokes on this syntax.
                    raise extype, value, traceback
                except SyntaxError:
                    pass
        if self.fp.close() is not None:
            raise Fatal("%s%s returned error." % (self.command, self.legend))
        return False

def iso8601_from_unixtime(secs_since_epoch):
    "Make an ISO8601 timestamp frm Unix time."
    return time.strftime("%Y-%m-%dT%H:%M:%S", time.localtime(secs_since_epoch))

class Recoverable(Exception):
    def __init__(self, msg):
        Exception.__init__(self)
        self.msg = msg

class RepositoryList:
    "A repository list with selection and access by name."
    def __init__(self):
        self.repo = None
        self.repolist = []
    def chosen(self):
        return self.repo
    def choose(self, repo):
        self.repo = repo
    def unchoose(self):
        self.repo = None
    def reponames(self):
        "Return a list of the names of all repositories."
        return [r.name for r in self.repolist]
    def uniquify(self, name):
        "Uniquify a repo name in the repo list."
        if name.endswith(".fi"):
            name = name[:-3]
        if name not in self.reponames():
            return name
        else:
            # repo "foo" is #1
            seq = 2
            while name + str(seq) in self.reponames():
                seq += 1
            return name + str(seq)
    def repo_by_name(self, name):
        "Retrieve a repo by name."
        return self.repolist[self.reponames().index(name)]
    def remove_by_name(self, name):
        "Remove a repo by name."
        if self.repo and self.repo.name == name:
            self.unchoose()
        self.repolist.pop(self.reponames().index(name))        
    def cut(self, early, late):
        "Attempt to topologically cut the selected repo."
        # Apply a graph-coloring algorithm to determine if the repo can be split
        ri = late.parent_marks.index(early.mark)
        late.parent_marks.remove(early.mark)
        def do_color(commit, color):
            commit.color = color
            for fileop in commit.fileops:
                if fileop.op == "M" and fileop.ref != "inline":
                    blob = self.repo.find(fileop.ref)
                    assert isinstance(self.repo[blob], Blob)
                    self.repo[blob].colors.append(color)
        do_color(early, "early")
        do_color(late, "late")
        conflict = False
        keepgoing = True
        while keepgoing and not conflict:
            keepgoing = False
            for event in self.repo.commits():
                if event.color:
                    for neighbor in event.parents() + event.children():
                        if neighbor.color == None:
                            do_color(neighbor, event.color)
                            keepgoing = True
                            break
                        elif neighbor.color != event.color:
                            conflict = True
                            break
        if conflict:
            late.parent_marks.insert(ri, early.mark)
            for event in self.repo:
                if hasattr(event, "color"):
                    event.color = None
            return False
        # Repo can be split, so we need to color tags
        for t in self.repo.events:
            if isinstance(t, Tag):
                for c in self.repo.events:
                    if isinstance(c, Commit):
                        if c.mark == t.committish:
                            t.color = c.color
        # Front events go with early segment, they'll be copied to late one. 
        for event in self.repo.front_events():
            event.color = "early"        
        assert all([hasattr(x, "color") or hasattr(x, "colors") or isinstance(x, Reset) for x in self.repo])
        # Resets are tricky.  One may have both colors.
        # Blobs can have both colors too, through references in
        # commits on both sides of the cut, but we took care
        # of that earlier.
        trackbranches = {"early": set([]), "late": set([])}
        for commit in self.repo.commits():
            if commit.color is None:
                complain("%s is uncolored!" % commit.mark)
            else:
                trackbranches[commit.color].add(commit.branch)
        # Now it's time to do the actual partitioning
        early = Repository(self.repo.name + "-early")
        os.mkdir(early.subdir())
        late = Repository(self.repo.name + "-late")
        os.mkdir(late.subdir())
        for event in self.repo:
            if isinstance(event, Reset):
                if event.ref in trackbranches["early"]:
                    early.events.append(copy.copy(event))
                if event.ref in trackbranches["late"]:
                    late.events.append(copy.copy(event))
            elif isinstance(event, Blob):
                if "early" in event.colors:
                    early.events.append(event.clone(early))
                if "late" in event.colors:
                    late.events.append(event.clone(late))
            else:
                if event.color == "early":
                    if hasattr(event, "moveto"):
                        event.moveto(early)
                    early.events.append(event)
                elif event.color == "late":
                    if hasattr(event, "moveto"):
                        event.moveto(late)
                    late.events.append(event)
                else:
                    # FIXME: Someday, color passthroughs that aren't fronted.
                    raise Fatal("coloring algorithm failed on %s" % event)
        for event in self.repo:
            if hasattr(event, "color"):
                event.color = None
            if hasattr(event, "colors"):
                event.colors = []
        # Options and features may need to be copied to the late fragment.
        late.events = copy.copy(early.front_events()) + late.events
        # Add the split results to the repo list. 
        self.repolist.append(early)
        self.repolist.append(late)
        self.repo.cleanup()
        self.remove_by_name(self.repo.name)
        return True
    def merge(self, factors):
        "Merge multiple repos into a union repo."
        factors.sort(key=lambda x: x.earliest())
        roots = [x.commits()[0] for x in factors]
        union = Repository("+".join([r.name for r in factors]))
        os.mkdir(union.subdir())
        for (i, factor) in enumerate(factors):
            if i != 0:
                factor.uniquify(factor.name)
            union.absorb(factor)
            self.remove_by_name(factor.name)
        # Renumber all events
        union.renumber()
        # Sort out the root grafts. The way we used to do this involved
        # sorting the union commits by timestamp, but this fails because
        # in real-world repos timestamp order may not coincide with mark
        # order - leading to "mark not defined" errors from the importer at
        # rebuild time.  This method gives less intuitive results but at
        # least means we never need to reorder.
        commits = union.commits()
        for root in roots[1:]:
            most_recent = None
            for (i, event) in enumerate(commits):
                if root.when() >= event.when():
                    continue
                elif not most_recent or event.when() < most_recent.when():
                    most_recent = commits[i-1]
                    break
            if most_recent is None:
                # Should never fire bacause we sorted the factors array
                # so root[0] is the oldest commit and the first in union.
                raise Fatal("should never happen!")
            elif most_recent.mark is None:
                # This should never happen either.
                raise Fatal("can't link to commit with no mark")
            else:
                root.parent_marks.append(most_recent.mark)
        # Put the result on the load list
        self.repolist.append(union)
        self.choose(union)
    def expunge(self, selection, matchers):
        "Expunge a set of files from the commits in the selection set."
        def digest(toklist):
            return re.compile("|".join(["(?:" + s + ")" for s in toklist]))
        try:
            # First pass: compute fileop deletions
            alterations = []
            expunge = digest(matchers)
            for ei in selection:
                event = self.repo[ei]
                deletia = []
                if hasattr(event, "fileops"):
                    for (i, fileop) in enumerate(event.fileops):
                        if verbose >= DEBUG_DELETE:
                            print(str(fileop))
                        if fileop.op in "DM":
                            if expunge.search(fileop.path):
                                deletia.append(i)
                        elif fileop.op in "RC":
                            fileop.sourcedelete = expunge.search(fileop.source)
                            fileop.targetdelete = expunge.search(fileop.target)
                            if fileop.sourcedelete:
                                deletia.append(i)
                                announce("following %s of %s to %s" %
                                         (fileop.op,
                                          fileop.source,
                                          fileop.target))
                                if fileop.op == "R" and fileop.source in matchers:
                                    matchers.remove(fileop.source)
                                matchers.append("^" + fileop.target + "$")
                                expunge = digest(matchers)
                            elif fileop.targetdelete:
                                if fileop.op == "R":
                                    fileop.op = "D"
                                elif fileop.op == "C":
                                    deletia.append(i)
                                matchers.append("^" + fileop.target + "$")
                                expunge = digest(matchers)
                alterations.append(deletia)
            # Second pass: perform actual fileop expunges
            expunged = Repository(self.repo.name + "-expunges")
            os.mkdir(expunged.subdir())
            for event in self.repo:
                event.deletehook = None
            for (ei, deletia) in zip(selection, alterations):
                event = self.repo[ei]
                keepers = []
                blobs = []
                deletia.reverse()
                for i in deletia:
                    fileop = event.fileops[i]
                    # For D, we're done; removing it has no side effects,
                    # and we don't want to keep it in the expunged repo.
                    if fileop.op == 'M':
                        keepers.append(fileop)
                        if fileop.ref != 'inline':
                            bi = self.repo.find(fileop.ref)
                            blob = self.repo[bi]
                            assert(isinstance(blob, Blob))
                            blobs.append(blob)
                        if verbose:
                            announce("at %d, expunging %s" % (ei+1,fileop.path))
                    elif fileop.op in ("R", "C"):
                        assert(fileop.sourcedelete or fileop.targetdelete)
                        if fileop.sourcedelete and fileop.targetdelete:
                            keepers.append(fileop)
                    event.fileops.pop(i)
                # If there are any keeper fileops, hang them them and
                # their blobs on deletehooks, cloning the commit() for them.
                if keepers:
                    keepers.reverse()
                    blobs.reverse()
                    for blob in blobs:
                        blob.deletehook = blob.clone(expunged)
                        newevent = event.clone(expunged)
                        newevent.fileops = keepers
                        event.deletehook = newevent
        except re.error:
            raise Fatal("you confused the regexp processor!")
        # Build the new repo and hook it into the load list
        expunged.events = copy.copy(self.repo.front_events())
        expunged_branches = expunged.branchlist()
        for event in self.repo:
            if event.deletehook:
                expunged.events.append(event.deletehook)
                event.deletehook = None
            elif isinstance(event, Reset) or isinstance(event, Tag):
                target = self.repo.find(event.committish)
                if target is not None:
                    if self.repo[target].deletehook:
                        expunged.events.append(copy.deepcopy(event))
                    continue
                if isinstance(event, Reset) and event.ref in expunged_branches:
                    expunged.events.append(copy.copy(event))
        for event in self.repo.events + expunged.events:
            if hasattr(event, "deletehook"):
                delattr(event, "deletehook")
        expunged_marks = set([event.mark for event in expunged.events if hasattr(event, "mark")])
        for event in expunged.events:
            if hasattr(event, "parent_marks"):
                event.parent_marks = [e for e in event.parent_marks if e in expunged_marks]
        keeper_marks = set([event.mark for event in self.repo.events if hasattr(event, "mark")])
        for event in self.repo.events:
            if hasattr(event, "parent_marks"):
                event.parent_marks = [e for e in event.parent_marks if e in keeper_marks]
        self.repolist.append(expunged)
        backreferences = collections.Counter()
        for event in self.repo.events:
            if isinstance(event, Commit):
                for fileop in event.fileops:
                    if fileop.op == 'M':
                        backreferences[fileop.ref] += 1
        # Now remove commits that no longer have fileops, and released blobs.
        deletia = [not ((isinstance(e, Commit) and len(e.fileops)==0) or (isinstance(e, Blob) and not backreferences[e.mark])) for e in self.repo.events]
        deletia = [x[0] for x in [i_e for i_e in enumerate(deletia) if not i_e[1]]]
        if verbose:
            announce("deleting blobs and empty commits %s" % [x+1 for x in deletia])
            deletia.reverse()
            for i in deletia:
                self.repo.events.pop(i)

class RepoSurgeon(cmd.Cmd, RepositoryList):
    "Repository surgeon command interpreter."
    Options = (
        ("use_uuid", """\
    If set, use Subversion UUID when faking up email addresses, a la git-svn.
Otherwise, fake up addresses the way git cvs-import does it.
"""),
        )
    def __init__(self):
        cmd.Cmd.__init__(self)
        RepositoryList.__init__(self)
        self.use_rawinput = True
        self.echo = 0
        self.prompt = "reposurgeon% "
        self.preferred = None
        self.selection = []
        self.line = ""
        self.history = []
        self.profile_log = None
        for option in dict(RepoSurgeon.Options):
            global_options[option] = False
    #
    # Housekeeping hooks.
    #
    def onecmd(self, line):
        "Execute one command, fielding interrupts for recoverable exceptions."
        try:
            cmd.Cmd.onecmd(self, line)
        except Recoverable as e:
            complain(e.msg)
    def postcmd(self, unused, line):
        assert unused is not []   # pacify pylint
        if line == "EOF":
            return True
    def emptyline(self):
        pass
    def precmd(self, line):
        "Pre-command hook."
        self.history.append(line.rstrip())
        if self.echo:
            sys.stdout.write(line.rstrip()+"\n")
        if "#" in line:
            line = line[:line.index("#")].rstrip()
        return line
    def do_shell(self, line):
        "Execute a shell command."
        sys.stdout.flush()
        sys.stderr.flush()
        if os.system(line):
            raise Recoverable("'shell %s' returned error." % line)
    def do_EOF(self, unused):
        "Terminate reposurgeon."
        assert unused is not None   # pacify pylint
        print("")
        return True
    def cleanup(self):
        "Tell all the repos we're holding to clean up."
        if verbose >= DEBUG_SHUFFLE:
            announce("interpreter cleanup called.")
        for repo in self.repolist:
            repo.cleanup()
    #
    # The selection-language parsing code starts here.
    #
    def set_selection_set(self, line, default=None):
        "Implement object-selection syntax."
        # Returns the line with the selection removed
        self.selection = []
        if not self.chosen():
            return line
        self.line = line
        self.selection = list(self.eval_expression())
        if self.line == line:
            self.selection = default
        else:
            # FIXME: We probably want to stop doing this
            self.selection.sort()
        return self.line.lstrip()
    def peek(self):
        return self.line and self.line[0]
    def pop(self):
        if not self.line:
            return ''
        else:
            c = self.line[0]
            self.line = self.line[1:]
            return c
    def eval_expression(self):
        if verbose >= DEBUG_LEXER:
            announce("eval_expression(%s)" % self.line)
        self.line = self.line.lstrip()
        value = self.eval_disjunct()
        c = self.peek()
        if c == '?':
            self.pop()
            add_list = []
            remove_list = []
            for ei in value:
                event = self.chosen().events[ei]
                if isinstance(event, Commit):
                    for parent in event.parents():
                        add_list.append(self.chosen().find(parent.mark))
                    for child in event.children():
                        add_list.append(self.chosen().find(child.mark))
                elif isinstance(event, Blob):
                    remove_list.append(ei) # Don't select the blob itself
                    for (i, event2) in enumerate(self.chosen().events):
                        if isinstance(event2, Commit):
                            for fileop in event2.fileops:
                                if fileop.op == 'M' and fileop.ref==event.mark:
                                    add_list.append(i)
                elif isinstance(event, Tag) or isinstance(event, Reset):
                    add_list.append(self.chosen().find(event.committish))
            value |= set(add_list)
            value -= set(remove_list)
        self.line = self.line.lstrip()
        if verbose >= DEBUG_LEXER:
            announce("%s <- eval_expression(), left = %s" % (value, repr(self.line)))
        return value
    def eval_disjunct(self):
        "Evaluate a disjunctive expression (| has lowest precedence)" 
        if verbose >= DEBUG_LEXER:
            announce("eval_disjunct(%s)" % self.line)
        self.line = self.line.lstrip()
        disjunct = set([])
        while True:
            conjunct = self.eval_conjunct()
            if conjunct is None:
                break
            else:
                disjunct |= conjunct
            self.line = self.line.lstrip()
            if self.peek() == '|':
                self.pop()
            else:
                break
        if verbose >= DEBUG_LEXER:
            announce("%s <- eval_disjunct(), left = %s" % (disjunct, repr(self.line)))
        return disjunct
    def eval_conjunct(self):
        "Evaluate a conjunctive expression (& has higher precedence)" 
        if verbose >= DEBUG_LEXER:
            announce("eval_conjunct(%s)" % self.line)
        self.line = self.line.lstrip()
        conjunct = set(range(0, len(self.chosen())))
        while True:
            term = self.eval_term()
            if term is None:
                break
            else:
                conjunct = conjunct & term
            self.line = self.line.lstrip()
            if self.peek() == '&':
                self.pop()
            else:
                break
        if verbose >= DEBUG_LEXER:
            announce("%s <- eval_conjunct(), left = %s" % (conjunct, repr(self.line)))
        return conjunct
    def eval_term(self):
        if verbose >= DEBUG_LEXER:
            announce("eval_term(%s)" % self.line)
        self.line = self.line.lstrip()
        if self.peek() == '{':
            self.pop()
            term = self.eval_disjunct()
            self.line = self.line.lstrip()
            if self.peek() != '}':
                raise Recoverable("trailing junk on inner expression")
            else:
                self.pop()
        else:
            term = self.eval_visibility()
            if term is None:
                term = self.eval_polyrange()
                if term is None:
                    term = self.eval_textsearch()
                    if term == None:
                        term = self.eval_branchset()
                        if term == None:
                            term = self.eval_pathset()
        if verbose >= DEBUG_LEXER:
            announce("%s <- eval_term(), left = %s" % (term, repr(self.line)))
        return term
    def eval_visibility(self):
        "Parse a visibility spec."
        if verbose >= DEBUG_LEXER:
            announce("eval_visibility(%s)" % self.line)
        self.line = self.line.lstrip()
        if not self.peek() == "=":
            visibility = None
        else:
            typeletters = {
                "B" : lambda e: isinstance(e, Blob),
                "C" : lambda e: isinstance(e, Commit),
                "T" : lambda e: isinstance(e, Tag),
                "R" : lambda e: isinstance(e, Reset),
                "P" : lambda e: isinstance(e, Passthrough),
                "H" : lambda e: isinstance(e, Commit) and not e.children(),
                }
            visible = set([])
            self.pop()
            while self.peek() in typeletters:
                c = self.pop()
                if c in typeletters:
                    visible.add(typeletters[c])
            # We need a special check here because these expressions
            # could otherwise run onto the text part of the command.
            if self.peek() not in "()|& ":
                raise Recoverable("garbled type mask at %s" % repr(self.line))
            if verbose >= DEBUG_LEXER:
                announce("visibility set is %s with %s left" % ([x.__name__ for x in visible], repr(self.line)))
            selected = []
            for (i, event) in enumerate(self.chosen()):
                for predicate in visible:
                    if predicate(event):
                        selected.append(i)
                        break
            visibility = set(selected)
        if verbose >= DEBUG_LEXER:
            announce("%s <- eval_visibility(), left = %s" % (visibility, repr(self.line)))
        return visibility
    def eval_polyrange(self):
        "Parse a polyrange specification (list of intervals)."
        if verbose >= DEBUG_LEXER:
            announce("eval_polyrange(%s)" % self.line)
        self.line = self.line.lstrip()
        polyrange_initials = (":","0","1","2","3","4","5","6","7","8","9","$", "<")
        if not self.peek() in polyrange_initials:
            polyrange = None
        else:
            selection = []
            while self.peek() in polyrange_initials + (".", ","):
                # First, literal command numbers (1-origin)
                match = re.match("[0-9]+", self.line)
                if match:
                    number = match.group()
                    selection.append(int(number)-1)
                    self.line = self.line[len(number):]
                    continue
                # Next, mark references
                match = re.match(":[0-9]+", self.line)
                if match:
                    markref = match.group()
                    self.line = self.line[len(markref):]
                    for (i, event) in enumerate(self.chosen()):
                        if hasattr(event, "mark") and event.mark == markref:
                            selection.append(i)
                            break
                        elif hasattr(event, "committish") and event.committish == markref:
                            selection.append(i)
                            break
                    else:
                        raise Recoverable("mark %s not found." % markref)
                    continue
                elif self.peek() == ':':
                    raise Recoverable("malformed mark")
                # $ means last commit, a la ed(1).
                if self.peek() == "$":
                    selection.append(len(self.chosen())-1)
                    self.pop()
                    continue
                # Comma just delimits a location spec
                if self.peek() == ",":
                    self.pop()
                    continue
                # Following ".." means a span
                if self.line[:2] == "..":
                    if selection:
                        selection.append("..")
                        self.line = self.line[2:]
                        continue
                    else:
                        raise Recoverable("start of span is missing")
                if self.peek() == "<":
                    self.pop()
                    matched = False
                    # First, search branches
                    branchlist = self.chosen().branchlist()
                    branchlist.sort(key=len, reverse=True) # longest name first
                    for symbol in branchlist:
                        if self.line.startswith(os.path.basename(symbol)):
                            self.line = self.line[len(os.path.basename(symbol)):]
                            loc = None
                            # Find the last commit with this branchname
                            for (i, event) in enumerate(self.chosen()):
                                if isinstance(event, Commit):
                                    if event.branch == symbol:
                                        loc = i
                            if loc is None:
                                raise Recoverable("branch name %s points to hyperspace" % symbol)
                            else:
                                matched = True
                                selection.append(loc)
                    # Next, search tags
                    taglist = [e for e in self.chosen().events if isinstance(e, Tag)]
                    taglist.sort(key=lambda x: len(x.name), reverse=True)
                    for tag in taglist:
                        if self.line.startswith(tag.name):
                            self.line = self.line[len(tag.name):]
                            for (i, event) in enumerate(self.chosen()):
                                if isinstance(event, Commit) and event.mark == tag.mark:
                                    matched = True
                                    selection.append(i)
                            else:
                                raise Recoverable("tag name %s points to hyperspace" % tag.name)
                    if self.peek() == '>':
                        self.pop()
                    else:
                        raise Recoverable("branch ref improperly terminated. '%s'" % self.line)
                    if not matched:
                        raise Recoverable("couldn't match a name at <%s>" % self.line)
            if verbose >= DEBUG_LEXER:
                announce("location list is %s with %s left" % (selection, repr(self.line)))
            # Resolve spans
            resolved = []
            spanning = last = 0
            for elt in selection:
                if elt == '..':
                    spanning = True
                else:
                    if spanning:
                        resolved += list(range(last+1, elt+1))
                        spanning = False
                    else:
                        resolved.append(elt)
                    last = elt
            selection = resolved
            if verbose >= DEBUG_LEXER:
                announce("resolved list is %s with %s left" % (selection, repr(self.line)))
            # Sanity checks
            if spanning:
                raise Recoverable("incomplete range expression.")
            for elt in selection:
                if elt < 0 or elt > len(self.chosen())-1:
                    raise Recoverable("event number %s out of range" % (elt+1))
            polyrange = set(selection)
        if verbose >= DEBUG_LEXER:
            announce("%s <- eval_polyrange(), left = %s" % (polyrange, repr(self.line)))
        return polyrange
    def eval_textsearch(self):
        "Parse a text search specification."
        if verbose >= DEBUG_LEXER:
            announce("eval_textsearch(%s)" % self.line)
        self.line = self.line.lstrip()
        if not self.peek() == '/':
            return None
        elif '/' not in self.line[1:]:
            raise Recoverable("malformed text search specifier")
        else:
            assert(self.pop() == '/')
            endat = self.line.index('/')
            try:
                regex = re.compile(self.line[:endat])
            except sre_constants.error:
                raise Recoverable("invalid regular expression")
            self.line = self.line[endat+1:]
            matchers = []
            for (i, e) in enumerate(self.chosen()):
                for searchable in ("author", "branch", "comment",
                                   "committer", "committish", "text",
                                   "tagger", "name"):
                    if hasattr(e, searchable) and regex.search(str(getattr(e, searchable))):
                        matchers.append(i)
                # We don't do blobs because it would be too slow
                # and not very useful.
            if verbose >= DEBUG_LEXER:
                announce("%s <- eval_textsearch(), left = %s" % (matchers, repr(self.line)))
            return set(matchers)
    def eval_pathset(self):
        "Resolve a path name to the set of commits that refer to it."
        if self.peek() != "[":
            return None
        elif self.line.find("]") == -1:
            raise Recoverable("malformed path wildcard")
        else:
            self.pop()
            i = self.line.find("]")
            path = self.line[:i]
            self.line = self.line[i+1:]
            selection = []
            for (ei, event) in enumerate(self.chosen().events):
                if isinstance(event, Commit):
                    for fileop in event.fileops:
                        if path in fileop.paths():
                            selection.append(ei)
            return set(selection)
    def eval_branchset(self):
        "Resolve a branch name to its set of associated events."
        if self.peek() != "(":
            return None
        else:
            self.pop()
            branchlist = self.chosen().branchlist()
            branchlist.sort(key=len, reverse=True) # longest name first
            selection = []
            for symbol in branchlist:
                if self.line.startswith(os.path.basename(symbol)):
                    for (i, event) in enumerate(self.chosen()):
                        if isinstance(event, Reset):
                            if event.ref == symbol:
                                selection.append(i)
                        elif isinstance(event, Commit):
                            if event.branch == symbol:
                                selection.append(i)
                        elif isinstance(event, Tag):
                            ti = self.chosen().find(event.committish)
                            assert(ti is not None)
                            assert(isinstance(self.chosen()[ti], Commit))
                            if self.chosen()[ti].branch == symbol:
                                selection.append(i)
                    self.line = self.line[len(os.path.basename(symbol)):]
                    if self.pop() != ')':
                        raise Recoverable("branch set improperly terminated.")
                    break
            else:
                raise Recoverable("unknown branch name %s" % self.line)
            return set(selection)
    #
    # Helpers
    #
    def report_select(self, line, method, optargs=()):
        "Generate a repository report on all objects with a specified method."
        if not self.chosen():
            complain("no repo has been chosen.")
            return
        default = [x[0] for x in [n_o for n_o in enumerate(self.chosen()) if hasattr(n_o[1], method)]]
        line = self.set_selection_set(line, default)
        outfile = line.strip()
        if outfile and outfile != '-':
            fp = open(outfile, "w")
        else:
            fp = sys.stdout
        for i in self.selection:
            event = self.chosen().events[i]
            if hasattr(event, method):
                summary = getattr(event, method)(*((i,)+optargs))
                if summary:
                    fp.write(summary + "\n")
        if outfile:
            fp.close()
    def edit(self, selection, line):
        # Mailboxize and edit the non-blobs in the selection
        editor = line.strip() or os.getenv("EDITOR")
        if not editor:
            complain("you have not specified an editor and $EDITOR is not set")
            return
        # Special case: user selected a single blob
        if len(self.selection) == 1:
            singleton = self.chosen()[self.selection[0]]
            if isinstance(singleton, Blob):
                def find_successor(event, path):
                    here = []
                    for child in event.children():
                        for fileop in child.fileops:
                            if fileop.op == "M" and fileop.path == path:
                                here.append(child.mark)
                        here += find_successor(child, path)
                    return here 
                for event in self.chosen().commits():
                    for fileop in event.fileops:
                        if fileop.op == 'M' and fileop.ref == singleton.mark:
                            if len(find_successor(event, fileop.path)) > 0:
                                complain("beware: not the last 'M %s' on its branch" % fileop.path)
                            break
                os.system(editor + " " + singleton.blobfile())
                return
            # Fall through
        (tfdesc, tfname) = tempfile.mkstemp()
        assert tfdesc > -1    # pacify pylint
        with open(tfname, "w") as tfp:
            for i in selection:
                event = self.chosen()[i]
                if hasattr(event, "email_out"):
                    tfp.write(event.email_out(i))
        if os.system(editor + " " + tfname):
            raise Recoverable("%s returned a failure status" % editor)
        else:
            self.do_mailbox_in(tfname)
        # No try/finally here - we want the tempfile to survive on fatal error
        # because it might have megabytes of metadata edits in it.
        os.remove(tfname)
    #
    # On-line help and instrumentation
    #
    def help_help(self):
        print("Show help for a command. Follow with space and the command name.")
    def help_verbose(self):
        print("""
Without an argument, this command requists a report of the verbosity
kevel.  'verbose 1' enables progress messages, 'verbose 0' disables
them. Higher levels of verbosity are available but intended for
developers only.
""")
    def do_verbose(self, line):
        global verbose
        if line:
            try:
                verbose = int(line)
            except ValueError:
                complain("verbosity value must be an integer")
        if not line or verbose:
            announce("verbose %d" % verbose)

    def do_echo(self, line):
        "Set or clear echoing commands before processing (for regression tests)"
        try:
            self.echo = int(line)
        except ValueError:
            announce("echo value must be an integer")
        if verbose:
            announce("echo %d" % self.echo)

    def do_version(self, unused):
        "Report the program version and supported version-control systems."
        assert unused is not None   # pacify pylint
        print("reposurgeon " + version + " supporting " + " ".join([x.name for x in (vcstypes+extractors)]))

    def help_resolve(self):
        print("""
Does nothing but resolve a selection-set expression
and report the resulting event-number set to standard
output. Implemented mainly for recression testing, but may be useful
for exploring the selection-set language.
""")
    def do_resolve(self, line):
        "Display the set of event numbers generated by a section set."
        self.set_selection_set(line)
        if self.selection is None:
            print("No selection")
        elif type(self.selection) == type([]):
            print([x+1 for x in self.selection])
        else:
            complain("resolve didn't expect a selection of %s" % self.selection)

    def do_names(self, unused):
        "List all known symbolic names of branches and tags."
        assert unused is not None   # pacify pylint
        for branch in self.chosen().branchlist():
            print("branch %s" % branch)
        for event in self.chosen():
            if isinstance(event, Tag):
                print("tag    %s" % event.name)

    def do_script(self, line):
        "Read and execute commands from a named file."
        if not line:
            complain("script requires a file argument")
            return
        try:
            for cmdline in open(line):
                interpreter.onecmd(interpreter.precmd(cmdline))
        except IOError as e:
            complain("script failure on '%s': %s" % (line, e))

    def do_history(self, line):
        "Dump your command list from this session so far."
        for line in self.history:
            print(line)

    def do_coverage(self, unused):
        "Display the coverage-case set (developer instrumentation)."
        assert unused is not None   # pacify pylint
        if not self.chosen():
            complain("no repo has been chosen.")
            return
        for (i, e) in enumerate(self.chosen().events):
            if isinstance(e, Commit):
                e.fileop_dump(i)
        coverage = list(self.chosen().case_coverage)
        coverage.sort()
        sys.stdout.write("Case coverage: %s\n" % coverage)

    def help_index(self):
        print("""
Display four columns of info on selected objects: their number, their
type, the associate mark (or '-' if no mark) and a summary field
varying by type.  For a branch or tag it's the reference; for a commit
it's the commit branch; for a blob it's the repository path of the
file in the blob.
""")
    def do_index(self, line):
        "Generate a summary listing of objects."
        if not self.chosen():
            complain("no repo has been chosen.")
            return
        # We could do all this logic using report_select() and index() methods
        # in the objects, but that would have two disavadntages.  First, we'd
        # get a default-set computation we don't want.  Second, for this
        # function it's helpful to have the method strings close together so
        # we can maintain columnation.
        default = [x[0] for x in [n_o1 for n_o1 in enumerate(self.chosen()) if not isinstance(n_o1[1], Blob)]]
        self.set_selection_set(line, default)
        for i in self.selection:
            event = self.chosen().events[i]
            if isinstance(event, Blob):
                print("%6d blob   %6s    %s" % (i+1, event.mark,event.path))
                continue
            if isinstance(event, Commit):
                print("%6d commit %6s    %s" % (i+1, event.mark or '-', event.branch)) 
                continue
            if isinstance(event, Tag):
                print("%6d tag    %6s    %4s" % (i+1, event.committish, repr(event.name),)) 
                continue
            if isinstance(event, Reset):
                print("%6d branch %6s    %s" % (i+1, event.committish or '-', event.ref)) 
                continue
            else:
                print("?      -      %s" % (event,)) 
    def help_profile(self):
        print("""
Enable profiling. Must be one of the initial command-line arguments, and
gathers statistics only on code executed via '-'.
""")
    def do_profile(self, line):
        "Enable profiling."
        assert line is not None # Pacify pylint
        self.profile_log = line
        announce("profiling enabled.")

    #
    # Information-gathering
    #
    def help_stats(self):
        print("""
Report size statistics and import/export method information of the
currently chosen repository.
""")
    def do_stats(self, line):
        "Report information on repositories."
        if not line:
            line = self.chosen().name
            if line is None:
                complain("no repo has been chosen.")
                return                
        for name in line.split():
            repo = self.repo_by_name(name)
            if not repo:
                raise Recoverable("no such repo as %s" % name)
        if repo:
            def count(otype):
                return len([x for x in repo.events if isinstance(x,otype)])
            print("%s: %.0fK, %d events, %d blobs, %d commits, %d tags, %d resets, %s." % \
                  (repo.name, repo.size() / 1000.0, len(repo),
                   count(Blob), count(Commit), count(Tag), count(Reset),
                   iso8601_from_unixtime(repo.readtime)))
            if repo.sourcedir:
                print("  Loaded from " + repo.sourcedir)
            if repo.vcs:
                print(repr(repo.vcs))
        else:
            complain("no repository has been chosen.")

    def help_list(self):
        print("""
Display commits in a human-friendly format; the first column is raw
event numbers, the second a timestamp, and the third the leading text
of the comment.  If there is a second argument, or the first is not
recognized as a selection set, it will be taken as the name of the
file to report to; no argument, or one of '-'; writes to standard
output.
""")
    def do_list(self, line):
        "Generate a human-friendly listing of objects."
        self.report_select(line, "lister", (screenwidth(),))

    def help_tags(self):
        print("""
Display lightweight tags: two fields, an event number and a tag name.  If
there is a second argument, or the first is not recognized as a
selection set, it will be taken as the name of the file to report to;
no argument, or one of '-'; writes to standard output.
""")
    def do_tags(self, line):
        "Generate a human-friendly listing of objects."
        self.report_select(line, "tags", (screenwidth(),))

    #
    # Housekeeping
    #
    def help_prefer(self):
        print("""
Report or set (with argument) the preferred type of repository. With
no arguments, describe capabilities of all supported systems. With
an argument (which must be the name of a supported system) this has
two effects:

First, if there are multiple repositories in a directory you do a read
on, reposurgeon will read the preferred one (otherwise it will
complain that it can't choose among them).

Secondly, if there is a selected repo, this will change its type.
This means that you do a write to a directory, it will build a repo of
the preferred type rather than its original type (if it had one).

If no preferred type has been explicitly selected, reading in a
repository (but not a fast-import stream) will implicitly set it
to the type of that repository.
""")
    def do_prefer(self, line):
        "Report or select the preferred repository type."
        if not line:
            for vcs in vcstypes:
                print(vcs)
            if [ext for ext in extractors if ext.visible]:
                print("Other systems supported for read only: %s\n" \
                      % " ".join(ext.name for ext in extractors if ext.visible))
        else:
            for repotype in vcstypes + extractors:
                if line.lower() == repotype.name:
                    self.preferred = repotype
                    if self.chosen():
                        self.chosen().vcs = self.preferred
                    break
            else:
                complain("known types are %s." % " ".join([x.name for x in vcstypes] + [x.name for x in extractors if x.visible]))
        if verbose:
            if not self.preferred:
                print("No preferred type has been set.")
            else:
                print("%s is the preferred type." % self.preferred.name)

    def help_choose(self):
        print("""
Choose a named repo on which to operate.  The name of a repo is
normally the basename of the directory or file it was loaded from, but
repos loaded from standard input are 'unnamed'. The program will add
a disambiguating suffix if there have been multiple reads from the
same source.

With no argument, lists the names of the currently stored repositories
and their load times.  The second column is '*' for the currently selected
repository, '-' for others.
""")
    def do_choose(self, line):
        "Choose a named repo on which to operate."
        if not self.repolist:
            if verbose > 0:
                complain("no repositories are loaded.")
                return
        self.repolist.sort(key=lambda x: x[1])
        if not line:
            for repo in self.repolist:
                status =  '-'
                if self.chosen() and repo == self.chosen():
                    status = '*'
                announce("%s %s %s" % (iso8601_from_unixtime(repo.readtime), status, repo.name))
        else:
            if line in self.reponames():
                self.choose(self.repo_by_name(line))
                if verbose:
                    self.do_stats(line)
            else:
                complain("no such repo as %s" % line)

    def help_drop(self):
        print("""
Drop a repo named by the argument from reposurgeon's list, freeing the memory
used for its metadata and deleting on-disk blobs. With no argument, drops the
currently chosen repo.
""")
    def do_drop(self, line):
        "Drop a repo from reposurgeon's list."
        if not self.reponames():
            if verbose:
                complain("no repositories are loaded.")
                return
        if not line:
            line = self.chosen().name
        if line in self.reponames():
            holdrepo = self.repo_by_name(line)
            holdrepo.cleanup()
            self.remove_by_name(line)
            if line == self.chosen().name:
                self.unchoose()
            del holdrepo
        else:
            complain("no such repo as %s" % line)
        if verbose:
            # Emit listing of remaining repos
            self.do_choose('')

    def help_rename(self):
        print("""
Rename the currently chosen repo; requires an argument.  Won't do it
if there is already one by the new name.
""")
    def do_rename(self, line):
        "Rename a repository."
        if line in self.reponames():
            complain("there is already a repo named %s" % line)
        else:
            self.chosen().rename(line)

    def help_preserve(self):
        print("""
Add (presumably untracked) files or directories to the repo's list of
paths to be restored from the backup directory after a rebuild. Each
argument, if any, is interpreted as a pathname.  The current preserve
list is displayed afterwards.
""")
    def do_preserve(self, line):
        "Add files and subdirectories to the preserve set."
        for filename in line.split():
            self.chosen().preserve(filename)
        announce("preserving %s." % list(self.chosen().preservable()))

    def help_unpreserve(self):
        print("""
Remove (presumably untracked) files or directories to the repo's list
of paths to be restored from the backup directory after a
rebuild. Each argument, if any, is interpreted as a pathname.  The
current preserve list is displayed afterwards.
""")
    def do_unpreserve(self, line):
        "Remove files and subdirectories from the preserve set."
        for filename in line.split():
            self.chosen().unpreserve(filename)
        announce("preserving %s." % list(self.chosen().preservable()))

    #
    # Serialization and de-serialization.
    #
    def help_read(self):
        print("""
A read command with no arguments is treated as 'read .', operating on the
current directory.
 
With a directory-name argument, this command attempts to read in the
contents of a repository in any supported version-control system under
that directory.

If the argument is the name of a plain file, it will be read in as a
fast-import stream.

With an argument of <quote>-</quote>, this command reads a fast-import
stream from standard input (this will be useful in filters constructed
with command-line arguments).
""")
    def do_read(self, line):
        "Read in a repository for surgery."
        if line:
            line = os.path.expanduser(line)
        if not line or line == '.':
            line = os.getcwd()
        repo = read_repo(line, self.preferred)
        self.repolist.append(repo)
        self.choose(repo)
        if self.chosen():
            if self.chosen().vcs:
                self.preferred = self.chosen().vcs
            name = self.uniquify(os.path.basename(self.chosen().sourcedir or line or "unnamed"))
            self.chosen().rename(name)
        if verbose:
            self.do_choose('')

    def help_write(self):
        print("""
Dump a fast-import stream representing selected events to standard
output (if second argument is empty or '-') or a file. Property
extensions will be omitted if the importer for the selected repo cannot
digest them. Fails if the argument exists and is a directory or
anything other than a plain file. The default selection is all events.
""")
    def do_write(self, line):
        "Stream out the results of repo surgery."
        if not self.chosen():
            complain("no repo has been chosen.")
            return
        line = self.set_selection_set(line, list(range(len(self.chosen()))))
        if not line:
            line = '-'
        if line == '-':
            self.chosen().fast_export(self.selection, sys.stdout, progress=verbose==1)
        else:
            try:
                with open(line, "w") as out:
                    self.chosen().fast_export(self.selection, out)
            except OSError:
                complain("open of %s for write failed.\n" % line)

    def help_inspect(self):
        print("""
Dump a fast-import stream representing selected events to standard output.
Just like a write, except (1) the progress meter is disabled, (2) properties
are always dumped, and (3) there is an identifying header before each event
dump.
""")
    def do_inspect(self, line):
        "Dump raw events."
        if not self.chosen():
            complain("no repo has been chosen.")
            return
        self.set_selection_set(line, list(range(len(self.chosen()))))
        for ei in self.selection:
            event = self.chosen().events[ei]
            sys.stdout.write(repr(ei+1) + ": " + (72 * "=") + "\n")
            if isinstance(event, Commit):
                sys.stdout.write(event.dump(write_properties=True))
            else:
                sys.stdout.write(str(event))

    def help_rebuild(self):
        print("""
Rebuild a repository from the state held by reposurgeon.  The argument
specifies the target directory in which to do the rebuild; if the
repository read was from a repo directory (and not a git-import stream), it
defaults to that directory.  If the target directory is nonempty
its contents are backed up to a save directory.
""")
    def do_rebuild(self, line):
        "Rebuild a repository from the edited state."
        if not self.chosen():
            complain("no repo has been chosen.")
            return
        rebuild_repo(self.chosen(), line, self.preferred)

    #
    # Editing commands
    #
    def help_mailbox_out(self):
        print("""
Emit a mailbox file of messages in RFC822 format representing the
contents of repository metadata. Takes a selection set; members of the set
other than commits, annotated tags, and passthroughs are ignored (that
is, presently, blobs and resets).  If there is a second argument, or
the first is not recognized as a selection set, it will be taken as
the name of the file to report to; no argument, or one of '-'; writes
to standard output.
""")
    def do_mailbox_out(self, line):
        "Generate a mailbox file representing object metadata."
        self.report_select(line, "email_out")

    def help_mailbox_in(self):
        print("""
Accept a mailbox file of messages in RFC822 format representing the
contents of the metadata in selected commits and annotated tags. Takes
no selection set.  If there is an argument it will be taken as the
name of a mailbox file to read from; no argument, or one of '-'; reads
from standard output.

Users should be aware that modifying an Event-Number field will change
which event the update from that message is applied to.  This is
unlikely to have good results.

Event updates are atomic; if reposurgeon throws an error while in this
command events updated before the error will keep their changed state,
but no event will be partially modified. Event numbers are validated
before any individual event is updated.
""")
    def do_mailbox_in(self, line):
        "Accept a mailbox file representing object metadata and update from it."
        infile = line.strip()
        if infile and infile != '-':
            fp = open(infile)
        else:
            fp = sys.stdin
        update_list = []
        while True:
            msg = RepoSurgeonEmail.readmsg(fp)
            if not msg:
                break
            update_list.append(email.message_from_string(msg))
        if infile:
            fp.close()
        # First, a validation pass
        for (i, message) in enumerate(update_list):
            if "Event-Number" not in message:
                raise Recoverable("missing event number in update %d" % i)
            eventnum = int(message["Event-Number"]) - 1
            if eventnum < 0 or eventnum >= len(self.chosen()):
                raise Fatal("event number %d out of range in update %d" % (eventnum, i+1))
            event = self.chosen()[eventnum]
            if not hasattr(event, "email_in"):
                raise Fatal("event %d cannot be modified" % (eventnum+1,))
        # Now apply the updates
        modified = []
        for update in update_list:
            eventnum = int(update["Event-Number"]) - 1
            event = self.chosen()[eventnum]
            if event.email_in(update):
                modified.append(eventnum+1)
        if verbose:
            if not modified:
                announce("no events modified.")
            else:
                announce("modified events are %s." % (modified,))

    def help_edit(self):
        print("""
Report the selection set of events to a tempfile as mailbox_out does,
call an editor on it, and update from the result as mailbox_in does.
If you do not specify an editor name as second argument, it will be
taken from the $EDITOR variable in your environment.

Normally this command ignores blobs because mailbox_out does.
However, if you specify a selection set consisting of a single
blob, your editor will be called on the blob file.

The modifier 'multiline' will trim the selection set to commits that
are multiline and not in summary/blank-line/details form.
""")
    def do_edit(self, line):
        "Edit metadata interactively."
        if not self.chosen():
            complain("no repo is loaded")
            return
        default = [x[0] for x in [n_o2 for n_o2 in enumerate(self.chosen()) if hasattr(n_o2[1], "email_out")]]
        rest = self.set_selection_set(line, default)
        if "multiline" in rest:
            rest = rest.replace("multiline", "")
            mr = re.compile("[^\n]*\n[^\n]")
            filtered = []
            for ei in self.selection:
                event = self.chosen().events[ei]
                if isinstance(event, Commit) and mr.match(event.comment):
                    filtered.append(ei)
            self.selection = filtered
        self.edit(self.selection, rest)

    def help_delete(self):
        print("""
Delete a selection set of commits (and their associated blobs, if
any).  The default selection set for this command is empty.  Tags
pointing at the commits are also removed.

Note that applying this command to a commit with a modify operation
will *not* necessarily remove changes made by that commit from later
versions.  It will have the effect of retracting the modifications
only when they are the final ones on the commit's branch.
""")
    def do_delete(self, line):
        "Delete events in the specified selection set."
        if not self.chosen():
            complain("no repo is loaded")
            return
        line = self.set_selection_set(line, [])
        line = str(line)   # pacify pylint by forcing string type
        if line:
            for token in line.split():
                if token not in ["complain",
                                 "coalesce",
                                 "obliterate",
                                 "pushback",
                                 "tagback",
                                 "tagforward"]:
                    complain("no such deletion modifier as " + token)
                    return
        self.chosen().delete(self.selection, self.line)

    def help_coalesce(self):
        print("""
Scan the selection set for runs of commits with identical
comments close to each other in time (this is a common form of scar
tissues in repository up-conversions from older file-oriented
version-control systems).  Merge these cliques by deleting all but the
last commit, in order.

The optional second argument, if present, is a maximum time
separation in seconds; the default is 90 seconds.
""")
    def do_coalesce(self, line):
        "Coalesce events in the specified selection set."
        if not self.chosen():
            complain("no repo is loaded")
            return
        line = self.set_selection_set(line, [])
        if not line:
            timefuzz = 90
        else:
            try:
                timefuzz = int(line)
            except ValueError:
                raise Recoverable("time-fuzz value must be an integer")
        eligible = []
        # This is a crude search that ignores the repo graph structure;
        # properly speaking we should be chasing child links.  Screw
        # it; this operation only make sense for cleaning up
        # artifacts in linear stretches of history that have been
        # lifted from file-oriented VCSes like RCS and CVS.
        commits = [i_e3 for i_e3 in enumerate(self.chosen()) if isinstance(i_e3[1], Commit)]
        for i in range(len(commits)-1):
            cthis = self.chosen().events[commits[i][0]]
            cnext = self.chosen().events[commits[i+1][0]]
            if not (isinstance(cthis, Commit) and isinstance(cnext, Commit)):
                continue
            elif cthis.branch != cnext.branch:
                continue
            elif cthis.comment != cnext.comment:
                continue
            #elif cthis.committer.email != cnext.committer.email:
            #    continue
            elif cthis.committer.date.delta(cnext.committer.date) < timefuzz:
                eligible.append(commits[i][0])
        if verbose:
            announce("deletion set is %s" % [x+1 for x in eligible])
        self.chosen().delete(eligible, "coalesce")

    def help_renumber(self):
        print("""
Renumber the marks in a repository, from :1 up to <n> where <n> is the
count of the last mark. Just in case an importer ever cares about mark
ordering or gaps in the sequence.
""")
    def do_renumber(self, unused):
        "Renumber the marks in the selected repo."
        assert unused is not None    # pacify pylint
        if not self.chosen():
            complain("no repo has been chosen.")
            return
        self.repo.renumber()

    def help_timeoffset(self):
        print("""
Apply a time offset to all time/date stamps in the selected set.  An offset
argument is required; it may be in the form [+-]ss, [+-]mm:ss or [+-]hh:mm:ss.
The leading sign is required to distingush it from a selection expression.

Optionally you may also specify another argument in the form [+-]hhmm, a
timeone literal to apply.  To apply a timezone without an offset, use
an offset literal of +0 or -0.
""")
    def do_timeoffset(self, line):
        "Apply a time offset to all dates in selected events."
        if not self.chosen():
            complain("no repo has been chosen.")
            return
        line = self.set_selection_set(line, list(range(len(self.chosen()))))
        if not line:
            complain("a signed time offset argument is required.")
            return
        elif not line[0] in ('-', '+'):
            complain("time offset argument must begin with + or -.")
            return
        line = str(line)   # pacify pylint by forcing string type
        args = line.split()
        h = m = "0"
        if args[0].count(":") == 0:
            s = args[0]
        elif args[0].count(":") == 1:
            (m, s) = args[0].split(":")
        elif args[0].count(":") == 2:
            (h, m, s) = args[0].split(":")
        else:
            complain("too many colons")
            return
        try:
            offset = int(h)*360 + int(m)*60 + int(s)
        except ValueError:
            complain("expected numeric literals in date format")
            return
        if len(args) > 1:
            if not re.match("[+-][0-9][0-9][0-9][0-9]", args[1]):
                complain("expected timezone literal to be [+-]hhmm")
        for ei in self.selection:
            event = self.chosen()[ei]
            if isinstance(event, Tag):
                if event.tagger:
                    event.tagger.unixtime += offset
                    if len(args) > 1:
                        event.tagger.timezone = args[1]
            elif isinstance(event, Commit):
                event.committer.unixtime += offset
                if len(args) > 1:
                    event.committer.timezone = args[1]
                for author in event.authors:
                    author.unixtime += offset
                    if len(args) > 1:
                        author.timezone = args[1]

    def help_cut(self):
        print("""
Attempt to partition a repo by cutting the parent-child link
between two specified commits (they must be adjacent). Does not take a
general selection-set argument.  It is only necessary to specify the
parent commit, unless it has multiple children in which case the child
commit must follow.

This operation may fail if the commit graph remains connected
through another path; the tool will detect this. If the repo was named
'foo', the cut segments will be named 'foo-early' and 'foo-late'.
""")
    def do_cut(self, line):
        "Attempt to topologically partition the repo."
        if not self.chosen():
            complain("no repo has been chosen.")
            return
        line = self.set_selection_set(line, [])
        if len(self.selection) == 0:
            complain("one or possibly two arguments specifying a link are required")
            return
        early = self.chosen()[self.selection[0]]
        possibles = early.children() 
        if len(self.selection) == 1:    
            if len(possibles) > 1:
                complain("commit has multiple children, one must be specified")
                return
            elif len(possibles) == 1:
                possibles = early.children()
                late = possibles[0]
            else:
                complain("parent has no children")
                return
        elif len(self.selection) == 2:
            late = self.chosen()[self.selection[1]]
            if early.mark not in late.parent_marks:
                complain("not a parent-child pair")
                return
        elif len(self.selection) > 2:
            complain("too many arguments")
        assert(early and late)
        if not self.cut(early, late):
            complain("repository cannot be split here")
        if verbose:
            self.do_choose("")

    def help_expunge(self):
        print("""
Expunge files from the selected portion of the repo history; the
default is the entire history.  The arguments to this command may be
paths or Python regular expressions matching paths.

All filemodify (M) operations and delete (D) operations involving a
matched file in the selected set of events are removed.  Renames are
followed as the tool walks forward in the selection set; each triggers
a warning message. If a selected file is a copy (C) target, the copy
will be deleted and a warning message issued. If a selected file is a
copy source, the copy target will be added to the list of paths to be
deleted and a warning issued.

After file expunges have been performed, any commits with no
remaining file operations will be deleted, and any tags pointing to
them.
""")
    def do_expunge(self, line):
        "Expunge files from the chosen repository."
        if not self.chosen():
            complain("no repo has been chosen.")
            return
        line = self.set_selection_set(line, list(range(len(self.chosen()))))
        line = str(line)   # pacify pylint by forcing string type
        self.expunge(self.selection, line.split())

    def help_split(self):
        print("""
Split a specified commit in two, the opposite of coalesce.  The first argument
is required to be a commit location; the separating keyword 'at' must follow,
then an integer 1-origin index of a file operation within the commit.

The commit is copied copied and inserted into a new position in the
event sequence, immediately following itself; the duplicate becomes
the child of the original, and replaces it as parent of the original's
children. Commit metadata is duplicated; the mark of the new commit is
then changed, with 'bis' added as a suffix.

Finally, some file operations - starting at the one indexed by the split
argument - are moved forward from the original commit into the new one.
Legal indices are 2-n, where n is the number of file operations in the
original commit.
""")
    def do_split(self, line):
        "Split a commit."
        if not self.chosen():
            raise Recoverable("no repo has been chosen.")
        line = self.set_selection_set(line, [])
        if len(self.selection) != 1:
            raise Recoverable("selection of a single commit required for this command")
        where = self.selection[0]
        event = self.chosen()[where]
        if not isinstance(event, Commit):
            raise Recoverable("fileop argument doesn't point at a commit")
        line = str(line)   # pacify pylint by forcing string type
        if not line.startswith("at"):
            raise Recoverable("expected 'at'")
        else:
            line = line[2:].lstrip()
            try:
                line = str(line)   # pacify pylint by forcing string type
                splitpoint = int(line.split()[0]) - 1
            except ValueError:
                raise Recoverable("expected integer fileop index (1-origin)")
            if splitpoint not in list(range(1, len(event.fileops))):
                raise Recoverable("fileop index out of range")
            # Actual implementation starts here
            self.chosen().events.insert(where+1, event.clone())
            event2 = self.chosen().events[where+1]
            assert(event.mark == event2.mark)
            event2.mark = event.mark + "bis"
            # Fix up parent/child relationships
            event2.parent_marks = [event.mark]
            for child in event.children():
                for (j, mark) in enumerate(child.parent_marks):
                    if mark == event.mark:
                        child.parent_marks[j] = event2.mark
            # Fileop split happens here
            event2.fileops = event.fileops[splitpoint:]
            event.fileops = event.fileops[:splitpoint]
            if verbose:
                self.do_inspect(repr(where+1) + "," + repr(where+2))

    def help_merge(self):
        print("""
Merge repositories. Name any number of loaded repositories; they will
be merged into one union repo and removed from the load list.  The
union repo will be selected.

Before merging, the repos will be sorted by date of first commit.  The
oldest will keep all its branch and tag names unchanged (this rule is
followed so there will always be a defined default branch).  All others
will have their branch and tag names suffixed with their load name.
Marks will be renumbered.

The name of the new repo will be the names of all parts concatenated,
separated by '+'. It will have no source directory or preferred system
type.
""")
    def do_merge(self, line):
        "Merge repos together."
        self.unchoose()
        factors = []
        for name in line.split():
            repo = self.repo_by_name(name)
            if repo is None:
                raise Recoverable("no such repo as %s" % name)
            else:
                factors.append(repo)
        if not factors or len(factors) < 2: 
            raise Recoverable("merge requires repo name arguments")
        self.merge(factors)
        if verbose:
            self.do_choose('')

    def help_graft(self):
        print("""
For when merge doesn't give you enough control.  The selection set
must be of size 1, identifying a single commit in the currently
selected repo.  A following argument must be a repository name.
Labels and branches in the named repo are prefixed with its name; then
it is grafted to the selected one. Its root becomes a child of the
specified commit.  Finally the named repo is removed from the load
list.
""")
    def do_graft(self, line):
        "Graft a named repo onto the selected one."
        if not self.chosen():
            complain("no repo has been chosen.")
            return
        line = self.set_selection_set(line, [])
        if len(self.selection) == 1:
            graft_point = self.selection[0]
        else:
            raise Recoverable("a singleton selection set is required.")
        if not self.repolist:
            raise Recoverable("no repositories are loaded.")
        if line in self.reponames():
            graft_repo = self.repo_by_name(line)
        else:
            raise Recoverable("no such repo as %s" % line)
        # OK, we've got the two repos and the graft point.  Do it.
        self.chosen().graft(graft_repo, graft_point)
        self.remove_by_name(graft_repo.name)

    def help_sort(self):
        print("""Sort events in the selected repo by timestamp, unless
doing so would put a child before a parent. May be useful after a
graft operation; a sorted repo tends to display more nicely in tools
which exibit the graph structure. Does not take a selection set.""")
    def do_sort(self, line):
        "Sort events by timestamp."
        assert line is not None      # Pacify pylint
        if self.chosen():
            self.chosen().sort()
        else:
            complain("no repo has been chosen.")

    def help_paths(self):
        """Without a modifier, list all paths touched by fileops in
the selection set (which defaults to the entire repo). With the 'sub'
modifier, take a second argument that is a directory name and prepend
it to every path. With the 'sup' modifier, strip the first directory
component from every path.""" 
    def do_paths(self, line):
        if not self.chosen():
            complain("no repo has been chosen.")
            return
        rest = self.set_selection_set(line, list(range(len(self.chosen()))))
        if not rest:
            allpaths = set([])
            self.chosen().path_walk(self.selection,
                                  lambda f: allpaths.add(f) or f)
            allpaths = list(allpaths)
            allpaths.sort()
            print "\n".join(allpaths)
            return
        fields = rest.split()
        if fields[0] == "sub":
            prefix = fields[1]
            modified = self.chosen().path_walk(self.selection,
                                               lambda f: os.path.join(prefix,f))
            print "\n".join(modified)
        elif fields[0] == "sup":
            try:
                modified = self.chosen().path_walk(self.selection,
                                               lambda f: f[f.find(os.sep)+1:])
                print "\n".join(modified)
            except IndexError:
                raise Recoverable("no / in sup path.")
    #
    # Artifact Removal
    #
    def help_cvspurge(self):
        print("""
Purge cruft created by cvs2svn conversions.  This looks for dummy
commits created by cvs2svn with one parent that were created within 10
seconds of the parent, have stereotyped comments and either no fileops
or all delete fileops.  The branch attribute of the commit is given to
the commit's parent and the commit itself deleted.  Also,
""")
    def do_cvspurge(self, line):
        "Purge cruft created by cvs2svn and re-map CVS references."
        if not self.chosen():
            complain("no repo has been chosen.")
            return
        line = self.set_selection_set(line, list(range(len(self.chosen()))))
        manufactured = "This commit was manufactured by cvs2svn to create"
        deletion_set = []
        for ei in self.selection:
            event = self.chosen().events[ei]
            if isinstance(event, Commit):
                # Look at only manufactured commits
                if len(event.parents()) != 1 or \
                   event.when() - event.parents()[0].when() >= 10 or \
                   not manufactured in event.comment:
                    continue
                # Generate appropriate tag
                m = re.search("create tag '([^']*)'", event.comment)
                if m:
                    self.chosen().events.append(Tag(name=m.group(1),
                                                    committish=event.mark,
                                                    tagger=copy.copy(event.committer),
                                                    comment="Tag deduced from bogus cvs2svn-generated commit.\n"))
                # One kind of pathological generated commit is empty.
                if len(event.fileops) == 0:
                    announce("Nuking empty manufactured commit %s"%event.mark) 
                    deletion_set.append(ei)
                    continue
                # Another just nukes every file in sight
                for fileop in event.fileops:
                    if fileop.op != 'D':
                        break
                else:
                    announce("Nuking all-delete manufactured commit %s" % event.mark)
                    deletion_set.append(ei)
                    continue
                announce("Manufactured commit %s survived" % event.mark)
        self.chosen().delete(deletion_set, ["obliterate", "tagback"])
        if verbose >= 1:
            announce("%d commits obliterated." % len(deletion_set))
                        
    def help_gitsvnparse(self):
        print("""
Interpret final comment lines of commits beginning with 'git-svn-id:'
as git-svn magic cookies and remove them.  The Subversion commit ID is
extracted and made the value of a per-commit property named 'svn',
which becomes visible and editable. Also, change refs/remotes/svn branch
names to corresponding local ones and lift tip tag commits to actual
tag objects (replicating what svn2git does to clean up the
repository). Finally, enables writing of the fassil-reference map when
the repo is written or rebuilt.
""")
    def do_gitsvnparse(self, line):
        "Interpret and strip git-svn ID lines."
        assert line is not None   # Pacify pylint
        if not self.chosen():
            complain("no repo has been chosen.")
            return
        for event in self.chosen().events:
            if isinstance(event, Reset):
                event.ref = event.ref.replace("refs/remotes/svn/",
                                                  "refs/heads/")
            elif isinstance(event, Commit):
                m = re.search("\ngit-svn-id:.*\n", event.comment)
                if m:
                    gitsvn = event.comment[m.start():m.end()]
                    event.comment = event.comment[:m.start()]
                    (_, after) = gitsvn.split("@")
                    fields = after.split()
                    event.set_fossil("SVN:" + fields[0])
                # Things that were remote branches in the git-svn pull
                # need to become local now.
                event.branch=event.branch.replace("refs/remotes/svn/tags/",
                                                  "refs/tags/")
                event.branch=event.branch.replace("refs/remotes/svn/",
                                                  "refs/heads/")
        # Look for commits generated by git-svn to represent tags.
        # They follow a very specific pattern.
        for (ei, event) in enumerate(self.chosen().events):
            if isinstance(event, Commit) \
                     and not event.children() \
                     and len(event.parents()) == 1 \
                     and len(event.parents()[0].children()) == 2 \
                     and not event.fileops \
                     and "tags" in event.branch:
                parent = event.parents()[0]
                siblings = parent.children()
                siblings.remove(event)
                sibling = siblings[0]
                self.chosen().events[ei] = Tag(os.path.basename(event.branch),
                                               parent.mark,
                                               event.committer,
                                               event.comment)
                while parent.branch == event.branch:
                    parent.branch = sibling.branch
                    if parent.parent_marks:
                        parent = parent.parents()[0]
                    else:
                        break
        self.chosen().write_fossils = True
        self.chosen().move_to_rename()

    def help_authors(self):
        print("""
Apply or dump author-map information for the specified selection
set, defaulting to all events. 

Lifts from CVS and Subversion may have only usernames local to
the repository host in committer and author IDs. DVCSes want email
addresses (net-wide identifiers) and complete names. To supply the map
from oune to the other, an authors file is expected to consist of
lines each beginning with a local user ID, followed by a '=' (possibly
surrounded by whitespace) followed by a full name and email address.

When an authors file is applied, email addresses in committer and author
metdata for which the local ID matches between &lt; and @ are replaced
according to the mapping (this handles git-svn lifts). Alternatively,
if the local ID is the entire address, this is also considered a match
(this handles what git-cvsimport and cvs2git do) 

With the 'read' modifier, or no modifier and a filename
argument, apply author mapping data (no filename argument, read the
mapping from standard input).  May be useful if you are editing a repo
or dump created by cvs2git or by git-svn invoked without -A.

With no file argument, or with the 'write' modifier and a file
argument: write each unique committer, author, and tagger (no file
argument sends the report to standard output). This may be helpful as
a start on building an authors file.
""")
    def do_authors(self, line):
        "Apply or dump author-mapping file."
        if not self.chosen():
            complain("no repo has been chosen.")
            return
        rest = self.set_selection_set(line, list(range(len(self.chosen()))))
        fields = rest.split()
        if not fields or (fields[0] == "write" and len(fields) == 1):
            self.chosen().write_authormap(self.selection, sys.stdout)
        elif fields[0] == "write":
            with open(fields[1], "w") as wfp:
                self.chosen().write_authormap(self.selection, wfp)            
        elif fields[0] == "read" and len(fields) == 1:
            self.chosen().read_authormap(self.selection, sys.stdin)
        elif fields[0] == "read":
            with open(fields[1]) as fp:
                self.chosen().read_authormap(self.selection, fp)
        else:
            with open(fields[0]) as fp:
                self.chosen().read_authormap(self.selection, fp)

    #
    # Reference lifting
    #
    def help_fossils(self):
        print("""
Read or write the fossil-references data.  The format of each line is
three whitespace-separated fields: a reference cookie, a timestamp, and
a committer email address.  Does not take a selection set.
""")
    def do_fossils(self, line):
        "Apply a reference-mapping file."
        if not self.chosen():
            complain("no repo has been chosen.")
            return
        fields = line.split()
        if not fields or (fields[0] == "write" and len(fields) == 1):
            self.chosen().write_fossilmap(sys.stdout)
        elif fields[0] == "write":
            with open(fields[1], "w") as wfp:
                self.chosen().write_fossilmap(self.selection, wfp)            
        elif fields[0] == "read" and len(fields) == 1:
            self.chosen().read_fossilmap(sys.stdin)
        elif fields[0] == "read":
            with open(fields[1]) as fp:
                self.chosen().read_fossilmap(fp)
        else:
            with open(fields[0]) as fp:
                self.chosen().read_fossilmap(fp)

    def help_references(self):
        print("""
Produces a listing of events that may have Subversion or CVS commit
references in them. With the modifier 'edit', edit this set.

With the modifier 'lift', transform commit-reference cookies from CVS
and Subversion into action stamps.  This command expects cookies
consisting of the leading string '[[', followed by a VCS identifier
(currently SVN or CVS) followed by VCS-dependent information, followed
by ']]'. An action stamp pointing at the corresponding commit is
substituted when possible.

Enables writing of the fassil-reference map when the repo is written or rebuilt.
""")
    def do_references(self, line):
        "Look for things that might be CVS or Subversion revision references."
        if not self.chosen():
            complain("no repo has been chosen.")
            return
        repo = self.chosen()
        repo.parse_dollar_cookies()
        rest = self.set_selection_set(line, list(range(len(self.chosen()))))
        if "lift" in rest:
            fossilmap = {}
            for commit in repo.commits():
                if "fossil" in commit.properties:
                    fossilmap[commit.properties["fossil"]] = commit
            hits = 0
            for (regexp, getter) in \
                    ((r"CVS:[^:\]]+:[0-9.]+",
                      lambda p: fossilmap.get(p) or repo.dollar_map.get(p)),
                     ("SVN:[0-9]+",
                      lambda p: fossilmap.get(p) or repo.dollar_map.get(p)),
                     (":[0-9]+",
                      lambda p: repo.objfind(p)),
                     ):
                match_re = re.compile(re.escape("[[")+regexp+re.escape("]]"))
                for ei in self.selection:
                    event = repo.events[ei]
                    if isinstance(event, Commit):
                        earliest = 0
                        while True:
                            m = match_re.search(event.comment[earliest:])
                            if not m:
                                break
                            payload = m.group(0)[2:-2]
                            commit = getter(payload)
                            if commit is None:
                                complain("no commit matches " + repr(payload))
                                break
                            else:
                                if commit:
                                    event.comment = event.comment[:earliest+m.start()] \
                                                    + commit.committer.action_stamp() \
                                                    + event.comment[earliest+m.end():]
                                    hits += 1
                                else:
                                    complain("cannot resolve %s" % payload)
                                break
                            earliest += m.end()
            announce("%d references resolved." % hits)
            repo.write_fossils = True
        else:
            # No modifier, just list or edit
            idhits = []
            for ei in self.selection:
                event = repo.events[ei]
                if hasattr(event, "comment"):
                    text = event.comment
                elif hasattr(event, "text"):
                    text = event.text
                else:
                    continue
                refstyles = (
                    # Subversion references
                    r"\Wr([0-9]+)\W",
                    r"(?:SVN|svn|Subversion|subversion|rev|version).*\W([0-9]+)\W",
                    # CVS references
                    r"(?:CVS|cvs|rev|version).*\W([0-9][0-9.]+)\W",
                    # Possible bare CVS references
                    r"[0-9]+\.[0-9]+\.[0-9]+",
                    )
                for pattern in refstyles:
                    if re.search(pattern, text):
                        if ei not in idhits:
                            idhits.append(ei)
            if idhits:
                if rest.startswith("edit"):
                    self.edit(idhits, rest[4:].strip())
                else:
                    for ei in idhits:
                        event = repo.events[ei]
                        if hasattr(event, "lister"):
                            summary = event.lister(ei, screenwidth())
                            if summary:
                                sys.stdout.write(summary + "\n")
    def do_import_marks(self, line):
        "Import a marks file mapping marks to reference cookies."
        if not self.chosen():
            complain("no repo has been chosen.")
            return
        repo = self.chosen()
        for markline in open(line.strip()):
            (mark, ref) = markline.split()
            commit = repo.objfind(mark)
            if commit:
                commit.properties["fossil"] = ref
            else:
                raise Recoverable("unknown mark %s pointing at %s" % (mark,ref))

    #
    # Examining tree states
    #
    def help_checkout(self):
        print("""
Check out files for a specified commit into a directory.  The selection
set must resolve to a singleton commit.
""")
    def do_checkout(self, line):
        "Check out files for a specified commit into a directory."
        if not self.chosen():
            complain("no repo has been chosen.")
            return
        repo = self.chosen()
        rest = self.set_selection_set(line, list(range(len(self.chosen()))))
        if not rest:
            raise Recoverable("no target directory specified.")
        if len(self.selection) == 1:
            commit = repo.events[self.selection[0]]
            if not isinstance(commit, Commit):
                raise Recoverable("not a commit.")
        else:
            raise Recoverable("a singleton selection set is required.")
        commit.checkout(rest)

    def help_diff(self):
        print("""
Display the difference between commits. Takes a selection-set argument which
must resolve to exactly two commits.
""")
    def do_diff(self, line):
        "Display a diff between versions."
        if not self.chosen():
            complain("no repo has been chosen.")
            return
        repo = self.chosen()
        self.set_selection_set(line, list(range(len(self.chosen()))))
        bounds = list(self.selection)
        bounds.sort()
        bounds = tuple([repo.events[i] for i in bounds])
        if len(self.selection) != 2 or \
               not isinstance(bounds[0], Commit) or \
               not isinstance(bounds[1], Commit):
            raise Recoverable("a pair of commits is required.")
        dir1 = bounds[0].checkout()
        dir2 = bounds[1].checkout()
        command = "diff -r --label 'commit %s' --label 'commit %s' -u %s %s" % \
                  (bounds[0].mark, bounds[1].mark, dir1, dir2)
        os.system(command)
        #
        # Setting options
        #
    def help_set(self):
        print("""
Set a boolean option to control reposurgeon's behavior.   With no arguments,
displays the state of all flags. The following flags are defined:
""")
        for (opt, expl) in RepoSurgeon.Options:
            print(opt + ":\n" + expl)
    def do_set(self, line):
        if not line.strip():
            for opt in dict(RepoSurgeon.Options):
                print("\t%s = %s" % (opt, global_options.get(opt, False)))
        else:
            for option in line.split():
                if option not in dict(RepoSurgeon.Options):
                    complain("no such option as '%s'" % option)
                else:
                    global_options[option] = True
    def help_clear(self):
        print("""
Clear a boolean option to control reposurgeon's behavior.   With no arguments,
displays the state of all flags. The following flags are defined:
""")
        for (opt, expl) in RepoSurgeon.Options:
            print(opt + ":\n" + expl)
    def do_clear(self, line):
        if not line.strip():
            for opt in dict(RepoSurgeon.Options):
                print("\t%s = %s" % (opt, global_options.get(opt, False)))
        else:
            for option in line.split():
                if option not in dict(RepoSurgeon.Options):
                    complain("no such option as '%s'" % option)
                else:
                    global_options[option] = False

if __name__ == '__main__':
    try:
        def interactive():
            global verbose
            interpreter.use_rawinput = True
            if verbose == 0:
                verbose = 1
            interpreter.cmdloop()
            interpreter.use_rawinput = False
        interpreter = RepoSurgeon()
        interpreter.use_rawinput = False
        if not sys.argv[1:]:
            sys.argv.append("-")
        try:
            for arg in sys.argv[1:]:
                for arg in arg.split(";"):
                    if arg == '-':
                        if interpreter.profile_log is None:
                            interactive()
                        elif interpreter.profile_log:
                            cProfile.run('interactive()', \
                                         interpreter.profile_log)
                        else:
                            cProfile.run('interactive()')
                    else:
                        # Call the base method so RecoverableExceptios
                        # won't be caught; we want them to abort scripting.
                        cmd.Cmd.onecmd(interpreter, interpreter.precmd(arg))
        finally:
            interpreter.cleanup()
        raise SystemExit(0)
    except (Recoverable, Fatal) as xe:
        complain(xe.msg)
        raise SystemExit(1)
    except KeyboardInterrupt:
        print("")
# end
